REPORT DOCUMENTATION PAGE 

Form Approved 

OMB No. 0704-0188 

The public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, 
gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection 
of information, including suggestions for reducing the burden, to Department of Defense, Washington Headquarters Services, Directorate for Information Operations and Reports 
(0704-0188), 1215 Jefferson Davis Highway, Suite 1204, Arlington, VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be 
subject to any penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number. 

PLEASE DO NOT RETURN YOUR FORM TO THE ABOVE ADDRESS. 

1. REPORT DATE (DD-MM-YYYY) 
11/27/2007 

2. REPORT TYPE 

Quarterly Repoit 

3. DATES COVERED (From - To) 

08/28/2007 - 11/27/2007 


4. TITLE AND SUBTITLE 

Multi-sensor Information Integration and Automatic Understanding 

5a. CONTRACT NUMBER 

N00014-05-C-0294 


5b. GRANT NUMBER 


5c. PROGRAM ELEMENT NUMBER 

6. AUTHOR(S) 

5d. PROJECT NUMBER 

Dr. Matthew Welborn (technical) 

Ms. Samantha Venters (cost summary) 

5e. TASK NUMBER 


5f. WORK UNIT NUMBER 


7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES) I 8. PERFORMING ORGANIZATION 


^ REPORT NUMBER 

Signal Innovations Group, Inc. 

1009 Slater Rd„ Ste. 200 SIG.ONR.OPT1.Q4 

Durham, NC 27703 

9. SPONSORING/MONITORING AGENCY NAME(S) AND ADDRESS(ES) 10. SPONSOR/MONITOR’S ACRONYM(S) 

Office of Naval Research 

875 N. Randolph St. Ste. 1425 _ 

Arlington, VA 22203-1995 11 ■ SPONSOR/MONITOR’S REPORT 

NUMBER(S) 

12. DISTRIBUTION/AVAILABILITY STATEMENT 

A 

13. SUPPLEMENTARY NOTES 


14. ABSTRACT 

Over the course of the past two months, significant progress has been made towards adding the final features necessary for the video 
tracking system. The code has successfully transitioned into its final format in C for efficient implementation, allow us 
to perform significant optimizations and achieve very efficient running times, on the order of 35 fps. We have also added important 
capabilities for object tracking across multiple cameras, object classification (allowing behavior analysis to be conditioned on 
the type of object observed), and virtual pan/tilt/zoom capabilities. 


15. SUBJECT TERMS 


16. SECURITY CLASSIFICATION OF: 

17. LIMITATION OF 

18. NUMBER 

19a. NAME OF RESPONSIBLE PERSON 

a. REPORT 

b. ABSTRACT 

c. THIS PAGE 

ABSTRACT 

OF 

PAGES 

Samantha Venters 

U 

U 

U 

U 

2 

19b. TELEPHONE NUMBER (Include area code) 

919-323-3453 

Standard Form 298 (Rev. 8/98) 
Prescribed by ANSI Std. Z39.18 


























INSTRUCTIONS FOR COMPLETING SF 298 


1. REPORT DATE. Full publication date, including 
day, month, if available. Must cite at least the year 
and be Year 2000 compliant, e.g. 30-06-1998; 
xx-06-1998; xx-xx-1998. 

2. REPORT TYPE. State the type of report, such as 
final, technical, interim, memorandum, master's 
thesis, progress, quarterly, research, special, group 
study, etc. 

3. DATES COVERED. Indicate the time during 
which the work was performed and the report was 
written, e.g., Jun 1997 - Jun 1998; 1-10 Jun 1996; 
May - Nov 1998; Nov 1998. 

4. TITLE. Enter title and subtitle with volume 
number and part number, if applicable. On classified 
documents, enter the title classification in 
parentheses. 

5a. CONTRACT NUMBER. Enter all contract 
numbers as they appear in the report, e.g. 

F3361 5-86-C-5169. 

5b. GRANT NUMBER. Enter all grant numbers as 
they appear in the report, e.g. AFOSR-82-1 234. 

5c. PROGRAM ELEMENT NUMBER. Enter all 
program element numbers as they appear in the 
report, e.g. 61101 A. 

5d. PROJECT NUMBER. Enter all project numbers 
as they appear in the report, e.g. 1 F665702D1 257; 

ILIR. 

5e. TASK NUMBER. Enter all task numbers as they 
appear in the report, e.g. 05; RF0330201; T411 2. 

5f. WORK UNIT NUMBER. Enter all work unit 
numbers as they appear in the report, e.g. 001; 
AFAPL30480105. 

6. AUTHOR(S). Enter name(s) of person(s) 
responsible for writing the report, performing the 
research, or credited with the content of the report. 
The form of entry is the last name, first name, middle 
initial, and additional qualifiers separated by commas, 
e.g. Smith, Richard, J, Jr. 

7. PERFORMING ORGANIZATION NAME(S) AND 
ADDRESS(ES). Self-explanatory. 


8. PERFORMING ORGANIZATION REPORT NUMBER. 

Enter all unique alphanumeric report numbers assigned 
by the performing organization, e.g. BRL-1234; 
AFWL-TR-85-401 7-Vol-21 -PT-2. 

9. SPONSORING/MONITORING AGENCY NAME(S) 

AND ADDRESS(ES). Enter the name and address of the 
organization(s) financially responsible for and monitoring 
the work. 

10. SPONSOR/MONITOR'S ACRONYM(S). Enter, if 
available, e.g. BRL, ARDEC, NADC. 

11. SPONSOR/MONITOR S REPORT NUMBER(S). 

Enter report number as assigned by the sponsoring/ 
monitoring agency, if available, e.g. BRL-TR-829; -215. 

12. DISTRIBUTION/AVAILABILITY STATEMENT. Use 

agency-mandated availability statements to indicate the 
public availability or distribution limitations of the 
report. If additional limitations/ restrictions or special 
markings are indicated, follow agency authorization 
procedures, e.g. RD/FRD, PROPIN, ITAR, etc. Include 
copyright information. 

13. SUPPLEMENTARY NOTES. Enter information not 
included elsewhere such as: prepared in cooperation 
with; translation of; report supersedes; old edition 
number, etc. 

14. ABSTRACT. A brief (approximately 200 words) 
factual summary of the most significant information. 

15. SUBJECT TERMS. Key words or phrases 
identifying major concepts in the report. 

16. SECURITY CLASSIFICATION. Enter security 
classification in accordance with security classification 
regulations, e.g. U, C, S, etc. If this form contains 
classified information, stamp classification level on the 
top and bottom of this page. 

17. LIMITATION OF ABSTRACT. This block must be 
completed to assign a distribution limitation to the 
abstract. Enter UU (Unclassified Unlimited) or SAR 
(Same as Report). An entry in this block is necessary if 
the abstract is to be limited. 


Standard Form 298 Back (Rev. 8/98) 





Quarterly Status Report N00014-05-C-0294 


November 2007 


1 Summary 

The purpose of this program is to address the development of algorithms for adaptive 
processing of multi-sensor data, employing feedback to optimize the linkage between 
observed data and sensor control. The envisioned multi-modal adaptive system is 
applicable for intelligence, surveillance, and reconnaissance (ISR) in general 
environments, addressing base and port security, as well as urban and suburban sensing 
during wartime and peace-keeping operations. Of significant importance for current and 
anticipated DoD activities, the ISR system is designed to detect asymmetric threats, with 
the goal of recognizing unusual behavior or activities. Technologies and systems 
developed under this effort will be designed for semi-automated scene awareness, with 
the objective of recognizing behavior that appears atypical (e.g. atypical object motion, 
and dynamic characteristics of people and vehicles). Leveraging our previously 
developed technology, SIG is developing second-generation methods to adaptively leam 
the statistics of dynamic object behavior in video, while focusing on defining system 
requirements for sensor deployment by using field data (vs. highly controlled indoor 
data). SIG is also working closely with its subcontractor, Lockheed Martin, to integrate 
additional technologies, such as object classification and recognition, to provide a more 
robust and discriminative system. Additionally, SIG is cooperating with the Navy’s 
China Lake facility to collect representative data for a deployed system, and to specify 
requirements and features necessary of such a system. Finally, SIG is coordinating with 
Integrian on prototype development schedules and product integration requirements, and 
defining a joint marketing and commercialization strategy for such products. 

2 Technical Developments 

Over the course of the past two months, significant progress has been made towards 
adding the final features necessary for the video tracking system. The code has 
successfully transitioned into its final format in C for efficient implementation, allow us 
to perfonn significant optimizations and achieve very efficient running times, on the 
order of 35 fps. We have also added important capabilities for object tracking across 
multiple cameras, object classification (allowing behavior analysis to be conditioned on 
the type of object observed), and virtual pan/tilt/zoom capabilities. 

2.1 C code and Optimizations 

Finishing the work started last reporting period, SIG has completed the transition of the 
code from the research rapid development environment in Matlab into the more efficient 
and flexible C code, which will be the format for the final delivered code. The final 
development in C code allows significant benefits for the system, including efficiency, 
flexibility, and portability. It also allows direct access to a variety of different types of 
camera video streams, capable of utilizing the camera developer’s own highly efficient 
camera interface libraries for data input and camera control. 

One of the primary benefits of a C implementation is the increased memory and 
processing efficiency possible in a lower level compiled system. Significant time has 


1 



Quarterly Status Report N00014-05-C-0294 


November 2007 


been spent optimizing the tracking code, and we have achieved highly successful results. 
Intentionally testing on an older, lowered powered system, we used a 1.7 GHz Pentium 
M processor, which is more indicative of the anticipated system that would be readily 
available for a complete anomalous behavior detection system. On our benchmark 
testing sequences, including up to four objects moving simultaneously on screen, we are 
able to achieve speeds of 35 frames per second, well in excess of our targeted 15 fps. 

This provides plenty of leeway for inclusion of additional processing to handle the more 
advanced features we anticipate including over the next couple of months. 

2.2 Multi-camera Tracking 

To maximally leverage the power of a multi-camera system, it is critical to be able to 
track objects as they pass from the field of view of one camera to another. While 
tracking each object separately for each camera may be possible, the data association 
problem can be difficult, as many viewing conditions can differ between the two 
cameras, such as viewing angle, distance, lighting conditions, and occlusions. 

Our proposed solution specifically seeks to create observation models which take these 
viewing conditions into account to create a probabilistic set of associations. Specifically, 
we consider the joint probability of association, which allows all of the observable data to 
be treated in a relative fashion. Given that an observed pair of people in one camera 
could be associated with two other people in a second camera, it can be challenging to 
accurately model the probability of association for each one independently. However, 
when considering the joint probability, the factors which are most relevant become the 
relative differences. For instance, different viewing angles can make modeling the 
expected position for an object in the second camera very difficult, the joint probability is 
able to nonnalize out much of this noise, and instead focus on preserving the relative 
positions of the different objects. 

Initial work has already been completed to implement this approach, and has been tested 
on our data. The results have been very encouraging, and as this technology matures, it 
will be integrated with the behavior modeling and analysis component of the system. 

This will provide significantly greater amounts of information for the system to work 
with, and be able to analyze potentially suspicious behavior across a significantly broader 
period of time, providing a corresponding increase in the discriminative power of the 
system. 

2.3 Object Classification 

In the recent reporting period, we have implemented algorithms to perfonn classification 
of tracked foreground objects into one of multiple predefined object classes. This 
classification mechanism is based on shape characteristics as captured by the 
probabilistic shape models maintained internal to the Bayesian tracking engine. To 
reduce the complexity of the classification, we distill the shape model into a number of 
easily computed characteristics, such as height and width. The classification detennined 
from each of these individual characteristics can be combined in a principled manner to 


2 



Quarterly Status Report N00014-05-C-0294 


November 2007 


achieve accurate distributions. This concept has been applied to classification in many 
different ways before, such Ada-boost or bootstrapping methods. We have taken these 
ideas, and updated them to be included in our Bayesian framework. This provides a 
principled way of conditioning the expected behavior of the objects on their appearance. 

p(pose | models, shape) = Z c i ass p(pose \ class, models) p(class \ shape) 

The initial classifier perfonnance is encouraging and further refinements are being made 
based on additional data sets. The classifier is able to make decisions not just about 
individual shape classes (e.g. pedestrian versus vehicle), but also identifying other 
features such as presence of multiple pedestrians in a group that might otherwise be 
difficult to discern based on simple features such as velocity or position alone. 

2.4 Virtual Pan/Tilt/Zoom 

One of the objectives of the current project work is to use the information extracted from 
the video detection and tracking engine to perform not only behavior analysis, but to also 
perform active sensor management that can enhance the information gained from objects 
identified by the tracking system. Camera technology trends and our own analysis results 
indicate that a promising approach to implement a Sensor Management Agent (SMA) 
would be to use it to create a ‘virtual’ zoom and pan functions that will be used in 
conjunction with high resolution digital video cameras to achieve the same effect with 
much better control and flexibility. 

For typical video scenes, video analytic processing and accurate object tracking can be 
performed at lower resolution without loss in robustness but with a savings in 
computational load as well as internal data communications bandwidth. When objects of 
interest are detected in the video scene, it is often useful to create a ‘virtual zoom’ camera 
by processing the area of interest at higher resolution beyond the level that is needed to 
simply maintain tracking of the object. As the object moves, the system can track its 
motion and ensure that the high resolution ‘virtual zoom’ area is panned with the moving 
object of interest to support extraction of object features and/or display at higher 
resolution. 

We have created the basic framework necessary for this approach of SMA in our current 
implementation. The method is based on identifying regions of interest, as specified by 
the tracking algorithm. Since moving objects are the pertinent portions of the image, it is 
intuitive that it is these sections which are most informative to concentrate, or “zoom” in 
on. The tracking algorithm provides both a predictive and a posterior estimation of the 
objects, allowing for a consistent “pan/tilt” mechanism, to provide continuity for a single 
virtual zoom. These areas can then be extracted and sent along either for more 
processing, or along a narrow bandwidth connection for visualization purposes. This 
framework provides visualizations that demonstrate a virtual pan/zoom function based on 
tracked objects as well as necessary features to support acoustic fusion as discussed 
below. 


3 



Quarterly Status Report NOOO14-05-C-0294 


November 2007 


3 Acoustic Array 

In addition to efficient extraction of visual information, this same virtual pan/zoom 
framework will be useful as we extend the same concept to additional types of sensors or 
processing regimes that are controlled by the low level object detection and tracking 
engine to extract even more information about the objects of interest that are being 
examined in the process of asymmetric threat detection. 

In addition to our video work, SIG has moved forward with the internal development 
project (not developed on SEALS funding) to build a linear acoustic array that can collect 
acoustic data synchronized with color video data to create a real-time acoustic beam that 
can listen to a stationary or moving object under the control of the video tracking system. 
SIG has completed initial analysis of this acoustic array sensor and we are proceeding 
with data collection efforts as we work to fuse this sensor data with the ongoing video 
tracking effort. Figure 2 shows results of modeling this sensor to determine potential 
acoustic gain available to extract acoustic information in a multi-sensor tracking 
application. 



Figure 2: Initial results for antenna array gain pattern modeling showing that significant azimuth gain can 
be achieved with a linear array, as well as potential ability to null nearby acoustics sources. 

4 Future Directions 

During the next reporting period, our goal is to finish integrating more advanced methods 
into the tracking system to allow us to achieve the highest levels of discrimination in the 
most challenging situations of the our data collections. We will focus specifically on 
apparent object separation and merging instances, for example, when a person exits or 
enters a vehicle, or when they drop off or pick up a package. Another area of 
improvement will consider the challenges that can arise when objects remain on screen 
for an extended period of time, or loiter in place for a long time. This situation has 
particularly strong implications for the color models used, which are currently being 


4 














Quarterly Status Report N00014-05-C-0294 


November 2007 


examined. The more recent features of the tracking algorithm, such as object 
classification and trans-camera tracking, will be tightly integrated into the behavior 
analysis section of the system, to provide more discriminative and informative results. 

In addition to this near-term work, we are continuing to plan our efforts for the Year III 
research that ONR has indicated will be funded. One key aspect of this next year, as 
indicated previously, is to work toward transitioning the research to other efforts, such as 
the DARPA LACOSTE. In this effort and others, an important component of the sensor 
framework involves compressive-sensing and other related concepts to address the very 
high envisioned data rates. This compressive-sensing construct involves non-adaptive 
random sampling, while related constructs such as value-of-infonnation sensor 
management algorithms are adaptive and non-random. Given ONR’s interest in 
compressive sensing, within the final year of this program these two approaches will be 
investigated in the context of high-bit-rate video collections. 


5 



Signal Inovations Group, Inc. 
Quarterly Cost Report Option 1 


SCHEDULE NO. 


(08/28/2007 - 11/27/2007) 
N00014-05-C-0294 


SHEET NO. 


_|_ CONTINUA TION SHEET 

U.S. DEPARTMENT, BUREAU, OR ESTABLISHMENT 


OFFICE OF NAVAL RESEARCH 


NUMBER 
AND DATE 
OF ORDER 


DATE OF 
DELIVERY 
OR SERVICE 


ARTICLES OR SERVICES 
(Enter description, item number of contract or Federal 
supply schedule, and other information deemed necessary) 


QUAN-[ 

TITY 


UNIT PRICE 


COST 


AMOUNT 


N00014-05-C-0294 


Signal Innovations Group, Inc. 
1009 Slater Rd., Ste. 200 
Durham, NC 27703 


CONTRACT VALUE: 
Estimated Cost 
Fixed Fee 


Total Estimated Cost Plus Fixed Fee 

FUNDED AMOUNT: 

Estimated Cost 
Fixed Fee 

Total Estimated Cost Plus Fixed Fee 


$ 

723,483 

57,878 


$ 

781,361 


$ 

368,541 

29,483 

$ 

398,024 


Major Cost Elements: 
Direct Labor 
Benefits 
Subtotal 


CURRENT 

7,362.73 

2,200.86 

9,563.59 


CUMULATIVE 

88,578.20 

24,497.92 

113,076.12 


Expenses: 

Travel 

Subcontract Expense 
Other Direct Costs 
Expenses Subtotal 


0.00 

111,265.00 

0.00 

111,265.00 


3,017.00 

174,189.00 

316.41 

177,522.41 


Overhead 


5,247.43 


60,545.38 


Subtotal 


126,076.02 


351,143.91 


Fee 8% 


8,148.17 

134,224.19 


26,153.64 

377,297.54 


Total Amount Due SIG 





















