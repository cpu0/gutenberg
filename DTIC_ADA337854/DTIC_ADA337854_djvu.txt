MINUTES OF THE 

41ST COMPUTER RESOURCES INTEGRATION 

MANAGEMENT 

MEETING 

14 May 1991 


19980302 057 


PLEASE RETURN TO: 



PLEASE fir 
SDI TECHNICAL *, 


. 

BROWN ENGINEERING 

Cummings Research Park* Huntsville, Alabama 35807 


Approved for public release; 
Distribution Unlimited 









1/16/98 11:48:02 AM 


Accession Number: 2971 

Title: Minutes of the 41st Computer 

Resources Integration Management 

Meeting, 14 May 1991 

Contract Number: DASG60-87-C-0042 

Corporate Author or Publisher: Teledyne Brown Engineering, 

Huntsville, AL 35807 

Publication Date: May 14, 1991 

Pages: 00200 

Comments on Document: Contains copies of briefing 
presentations. See U2909 for 39th 
Meeting Minutes; U2908 for 40th. 

Descriptors, Keywords: SDS Computer Resource Management 
Parallel Process PPG SDINTF NTB 
NTBJPO Test NOS Network Simulation 
BM C3 EV88 EVPA RTE PCS ARCSIM Model 
Reengineer Argonne SEE 




MINUTES OF THE 

41ST COMPUTER RESOURCES INTEGRATION 
MANAGEMENT MEETING 
14 May 1991 


PREPARED FOR: 


UNITED STATES ARMY 
STRATEGIC DEFENSE COMMAND 

CONTRACT: DASG60-87C-0042 


PREPARED BY: 

TELEDYNE BROWN ENGINEERING 
CUMMINGS RESEARCH PARK 
HUNTSVILLE, ALABAMA 35807 


PREPARED ON: 14 May 1991 




Minutes of the 41st CRIM 


1. The 41st CRIM was held 14 May 1991 at 1300 hours in Room 1C1600 at USASDC in 
Huntsville, Alabama. The theme for this CRIM was the second of three meetings devoted to 
Parallel Processing (PP). Presenters were from both the government and the private sector, and 
they provided topic discussions on current research and development projects using parallel 
processing. The meeting began with Mr. Frank Poslajko presenting the agenda. Next, Mr. 
Poslajko presented a slide concerning the various committee and working group meetings in the 
near future. During the presentation of the forthcoming meetings, SEI Contractor Software 
Capabilities Workshop was discussed and Dr. Ron Green commented that contractors will be 
required to demonstrate a Level 2 on the SEI rating scale and prepare a plan for transitioning to 
Level 3. In addition, a slide was presented on the different topics of parallel processing (i.e., PP 
computers, software and tools, compilers, etc.). This slide established a high level breakdown of 
the various PP subcomponents. Finally, Mr. Poslajko presented a status report on the USASDC 
Ada Workshop which began March 25. 

2. Ms. Tina Powell of Vanguard Research presented the SDIO parallel processing activities, 
tools, etc. Ms. Powell began with an overview of the Parallel Programming Group (PPG). She 
presented the history, purpose, accomplishments, goals, and current activities of the PPG. Next, 
Ms. Powell discussed the May 1991 PPG meeting topics with the major theme being " The Use of 
Parallel Processing in SDI Simulations." Ms. Powell presented the issues from the PPG planning 
session. In addition, Ms. Powell discussed the threat tracking standard problem set which is 
planned for distribution to the PPG members in mid-July to allow common medium for 
comparison. Finally, Ms. Powell presented information about the next PPG meeting which will be 
held at Hughes Research Labs in Malibu, California, on October 29-30,1991. The theme will be 
"Parallel Programming Technology Transfer within the SDIO." 

3. Mr. John Hawk of the Strategic Defense Command presented for CPT Emily Andrews of the 
NTBJPO. He discussed the current parallel programming efforts at the National Test Facilities 
(NTF) which included hosting the Parallel Programming Group meeting on 30 April - 1 May. 
Finally, Mr. Hawk discussed the BBN TC2000 training at the NTF. 

4. Mr. John Schwacke of GRC presented applications of parallel processing in the test 
environment system. Mr. Schwacke began with a background of the Test Environment System 
(TEVS). Next, he discussed how test articles are linked to the TEVS system to allow testing. Mr. 
Schwacke presented the different applications of parallel processing in TEVS (i.e., Course-Grain 
Parallelism, Fine-Grain Parallelism, and Parallel Processing Architecture). In addition, Mr. 
Schwacke discussed the Network Operating System (NOS) and its application and benefits as well 
as the advantages of vectorization in computational intensive functions. Finally, Mr. Schwacke 
presented NWEM distributed emulation capabilities and the composition of each node. 

5. Mr. William Jarvinen of TRW presented the distributed and parallel processing in 
EV88/EVPA. He began with a discussion of the objectives of SDI BM/C 3 and how EV88/EVPA 
relates as an experimental prototype test bed for BM/C 2 processing. Next, Mr. Jarvinen discussed 
the Experimental Version (EV) system and how the BM/C 2 test articles are distributed on a 
heterogeneous network. He discussed the need for objects to follow fundamental rules during 
distributed operations, and the need for an appropriate tool set. In addition, Mr. Jarvinen 
presented an overview of the Run-Time Executive (RTE) and the interface to the process and test 
bed environment. Finally Mr. Jarvinen presented the Process Construction System (PCS), and he 
concluded with a discussion of the lessons learned in the EV project. 



6. Mr. Gordon Bate of Optimization Technology Inc. presented and demonstrated the 
USASDC-developed ARCSIM program package. Mr. Bate began with an overview of the 
architecture of the Advanced Research Center (ARC). Next, he discussed the functional flow of 
the ARCSIM package from scenario generation and translation to data reduction and results 
presentation. The translator gives Network n.5 code which can be executed on a VAX, CRAY, or 
PC. In addition, Mr. Bate presented some of the additional functions available in ARCSIM. 
Finally, Mr. Bate gave a demonstration of ARCSIM showing the ease of representing a system in 
the package and the type of data which can be extracted once the system has been run. As of this 
time, new computer systems have to be modeled manually and incorporated into the library, but 
one of the functions for the next version is the ability of ARCSIM to gather its own data from a 
target computer and develop a model using this data. Dr. Davies showed interest in having models 
made of the computers at the Simulation Center. 

7. Mr. Evan Lock of Computer Command and Control Company presented re-engineering 
existing software into distributed applications. Mr. Lock began by discussing the challenges of re¬ 
engineering to allow for the advantage of distributed architectures. He discussed the overall 
approach of re-engineering along with a summary of his company's re-engineering system. Next, 
Mr. Lock discussed in detail the distributed application workbench and the three main tools (i.e., 
Simulator, Builder, and Manager). Finally, he presented examples using the distributed application 
workbench and concluded with a brief summary of the advantages of this system. 

8. Mr. Gregory Chisholm of Argonne National Laboratory presented SDIOO-related activities at 
Argonne. Mr. Chisholm began with a discussion of the Software Engineering Environment (SEE) 
and tools. He discussed access to parallel machines at ACRF, parallel programming classes in Ada 
FORTRAN & C, and the development of portable parallel programming tools. Next Mr. 
Chisholm discussed parallel simulation. Finally, Mr. Chisholm discussed fault-tolerant, reliable, 
portable computing for the SDS. 

9. The meeting was adjourned by 1500. The 42nd CRIM is scheduled for 11 June 1991. This 
will be the third and final CRIM on parallel processing. 



41st Computer Resources Integration Management Meeting 
14 May 1991 
List of Attendees 


Name 

Organization 

Dr. Davies 

CSSD-TD 

Frank Poslajko 

CSSD-SP 

Pete Cemy 

CSSD-SP 

Ted Allen 

TBE 

Robert Ellis 

TBE 

Bill Burrows 

SFAE-SD-GBR-E 

Dr. Michael Walker 

SEIC/GE-HSV 

Dr. Ron Green 

SFAE-SD-GST-D 

Steve Risner 

CSSD-SA-BT 

James Butler 

ASAT 

Les Pierre 

SDIO/SDA 

Mike Mitrione 

DRC 

Gordon Bate 

on 

Bettie Upshaw 

CSSD-SA-BT 

John Hawk 

CSSD-NT-LO 

John Schwacke 

GRC 

Tina Powell 

Vanguard Research 

Evan Lock 

cccc 

Gregory Chisholm 

ANL 

Jeff Craver 

CSSD-SO 

TomNuttall 

CSSD-TE-P 

Dave Gazaway 

CSSD-SP 

Nancy Byrd 

CSSD-SL 

Michael P. Gately 

CSSD-SA-BE 

Susan Roberts 

PMADCCS 


Telephone 

Fax 

895-3520 

955-1995 

955-3985 

955-3069 

726-1285 

726-1033 

726-2748 

726-1033 

955-5877 

955-1867 

883-1170x1304 

722-1844 

955-3848 

722-1078 

(703)693-1826 

(703)693-1700 

(703)521-3812 

(703)521-4123 

721-1288 

837-9682 

955-3704 

955-3920 

922-1941 

(703)934-6300 

(703)273-9398 

(215)854-0555 

(215)854-0665 

(708)739-6235 

955-1695 

955-3909 

955-5209 

955-1610 

955-4945 

895-4475 

895-3178 



41st Computer Resource Integration Management (CRIM) Meeting 

Open Action Items 


1 . 

Provide a status update on the software organization 
and development at NTF. 

John Hawk - 955-3920 

2. 

Schedule status briefing on SDS committees to include 
purpose, accomplishments, plans, and schedules. 

Frank Poslajko - 955-3920 

3. 

Establish a data reduction planning committee. 

Barbara Rogers 722-1518 

4. 

DISC 4 to coordinate with Ada 9X project office 
on Ada language deficiencies. 

Bob Johnson AV227-0259 

5 

ADCCS project office to report on the number 
of Ada waiver requests submitted to DISC 4. 

Denise Jones 895-3397 

6. 

Broaden distribution of SESE Specification 

Document to CASE developers. 

Frank Poslajko - 955-3920 

7. 

Schedule a Rational & ISI tools demonstration. 

Frank Poslajko - 955-3920 

8. 

Ensure the SDI TIC is placed on the USASDC 
documentation distribution list. 

Frank Poslajko - 955-3920 

9. 

Determine requirements/procedures necessary to 
incorporate models for the Simulation Center’s 
computers in ARCSIMs model library. 

Frank Poslajko - 955-3920 

10. 

Develop process for technical data transfer of SDC 
developed products to the SDS TIC. 

Frank Poslajko - 955-3920 
Mike Mitione (703)521-3812 

11. 

What is the status of the TC2000 Ada compiler at the NTB. 

John Hawk - 955-3920 



PRESENTER: FRANK POSLAJKO 


1) Agenda 

2) Action Items 

3) Computer Resource Meetings 





Agenda 

41st Computer Resources Integration Management Meeting 

14 May 1991 

Conference Room 1C1600,0800-1500 Hours 


10 

ON 

ON 

i 

10 

ON 

o 


co 

o 

ft 

s 

3 

L. 

ft 


o 

o 

m 

NO 

■ 

m 

On 

rT 

o 


3 — 


a> 

£ 

o 

Pk 

03 

S 




2*95 



9 

5 


w 

cfl 


ID 

Tf 

CN 

Tf 

^ «D 

05 on 

- <D 
3 w 

o> 3 

.SC 

u a> 
3 3 

^ xs 

— cd 

S§ 


oo 

00 

r* 


<N 

hH 

H 

O 

i 

a> 

3 

C0 

s 

o 

*3 

u 

o 

C 


o 

L* 

3 

o 

m O 

NO 

"4 "2 

ID 5 
oo 2 
N S 

ID c 
fh 5 

o 

•- 

Si « 
o •*■» 

J 2. 

S I 

> O 

W U 


ID 

00 

NO 

a 

l> 

ON 

00 

o 

I> 


o 

.3 

.22 

IS 

c 

60 

a> 

s- 

c 


3 

o 

• PM 

CD 

3 

TO 

o 

u 


CD 

QJ 

•s 

CO 

o 

o 

H 

w 

co 

O) 


cd 

< 

OX) 

c 


i 


co 

co 

<d 

cd 

o 


jd 

13 

u 

3 

Oh 


Q 

co 


CD 




Z ft 


00 

c 

• MM 

CO 

co 

a> 

CD 

o 

u 

ft 

13 

u. 

3 

ft 


co 

3 

# o 

3 

ft 

ft 

< 

m 

U 


CO 

0) 

S 


3 

<D 

ft 


p* 

3 

CD 

L- 

CO 


>3 

a ) 

3 

£ 

'u 

co 

s 

Ip 

«2 

© 

o 

H 

3 

O 

3 


CO 

<D ^ 

s S 

O C/5 

3 u 

x & 

a> w 
tS 50 

§.£ 
f a 

U CO 


*3 

o> 

3 

pO 

»PM 

u 

*p* 

CO 

5 

c 

CO 

3 

O 

• PM 

3 

a 

. mm 

ft 

ft 

c 

00 

3 

• PM 

CO 

• M 

X 

a 

00 

3 

• M 

U 

<D 

<D 

•S CO 
00 3 
3 § 

<D X 
■ 5/3 

OC co 


3 

co 

0> 

CD 

u 

<D 

CO 


3 

U 

La 

3 

CD 

CO 

« 

co" 

0> 


3 -a 

Cid cd 
pj 

00^ 

•S "S 

% g 

CD .3 
CD ^ 
o 3 

ft ^ 


CD 


CD 


zz c 


00 


3 

Lp 

3 - : 

ft < 


"3 

3 

3 

• M 

3 

O 

U 


CD 

3 

3 


QO 

O 

i 


QO 

O 


io 

fs 

00 

o 

a 

o 

tH 

00 

o 


m 

ON 

o 


o 

i-H 

o 

fH 

I 

o 

o 

o 


m 

o 


I 

m 

m 

o 


o 


o 

o 

m 

■ 

o 

Tf 



Agenda 

41st Computer Resources Integration Management Meeting 

14 May 1991 

Conference Room 1C1600,0800-1500 Hours 


r> 


10 

Os 

OS 

i 

m 

m 

Os 

o 


Cd 

o 

Cw 

s 

3 

v 

Cv 


cd 

2 

O) 


o 

m 

o 

m 

fH 

i 

o 

o 

m 


VO 

So 

fH 

I 

m 

os 

vo 

§ 

'w' 

0) 

v 

v 

a> 

£ 

"3 

o> 


0) 


o 

o 

H 

cd 

a> 


,3D 

a 


Cd 

Cd 

a> 

a 

o 

u 

PLn 

2 

13 

u 

3 

Cu 


m 

o 

m 


in 

vo 

m 

■ 

00 

c* 

w m 

g .* 

■o I 
•< SC 
H _g 

Q 


3J 

13 

V 

3 

Pu 


JS ~ 
a 

a> 

E 
a 
o 


a> 

> 

a> 

a 

<D 

V 

3 

£ 


§ 


.2 O .2 

•<S « <3 


o 

cn 

-a 

<D 

« 

a> 

H 

13 

c 

o 


O 3 

£ z 


o m 
rq m 
m m 


i 

o 

rs 

m 


« 

? 

m 

lO 


9 


m 


2 - 

05 
0> 

H 


5 * 


9 


£ .2 


3 


9 

QJ 


Cd 3T" Vo 

2 — .*3 


u 

B 

3 

M 

3D 

# 3 

*35 

Cd 

a> 

u 

O 

v 

CU 


eft 


$ 


m 

m 

t-H 

U> 

m 

m 


TT 

Os 

fH 

i 

<N 

fS 

Os 

U 

(2 

a 


V 

p* 

u 

3 

£ 

c» 

s 

■c 

o 

| -s 


Cd 

S 

o 


a 

a 

< 


Cd 

a> 

3 

cd 

HH 9 


Cd 

cn 

OX) 

3 


cd 

3 

.2 9** 

•3 «p .3 

Tj s *■» 


03 

u 

a> ^ 
a 3 

® i 

■» c 
11 
<4! zw 


m 

® 

Tf 

?H 

1 

© 

m 

m 


m 

Tf 

os 

Tf 

^ in 
05 on 

- 0) 

§ 13 

.s o 

^ w 

V 4> 
3 3 
^ X 

— 9 

5§ 


3D 

# S 

•5 

Cd 

cj 

o 

u 

3m 


ju 

13 

v 
3 
3N 


Cd 

3 

# o 

*-3 

3 

"a- 

fip 

< 

m 

U 


S3 

a> 

2 


3 

a> 

3P 


© 

r<i 

tt 

tH 

1 

m 

© 

Tf 


00 

00 

(S 


r> 


o 

t- 


o 

m o 
© 


CH 


H 

O 

l 

a> 

3 

M 

3 

O 

L* 

O 

a 


no 

a> 

3 

X 

*2 

cd 

Q 

Vi 

a 

"o 

H 

3 

o 

1 

3 

E 

& 

§s 

3 hh 

o cn 

cd r j 

a> w 
PP 

<u w 
ts «J 

a S 

g S 

3 cd 
r 2 ^ 
U cn 


m 

m 

Tf 

fH 

1 

© 

fS 

Tf 


00 g 

/—s 3 
m c 

fH g 

fS o 

WQJ 

^ v 
o « 

J 3. 

S | 

> o 
MU 


T3 

v 

3 

© 

# v 

Cd 

5 

# s 

Cd 

3 

# o 

3 

a 

a 

< 

OX) 

3 

. ** 

Cd 

s 

3D 

G 

# v 

ai 

0) 

.£ cd 
3D 3 
3 § 
ai x 

> Cd 

fiP cn 


© 

m 

rr 

fH 

1 

in 

m 

rt 


1450-1500 Parallel Processing Facilities, Research and Services at Greg Chisholm (708) 972-6815 
Argonne National Lab 



41st Computer Resources Integration Management Meeting 

Action Items 


8 & 
£ £ 
2 < 


CO -g 

j- o 

■S a 
CO o 
CD CD 


to 

£ <D 
05 c 

o 

o -z 

- £ 
to s 

co -- 

O o 

a. 

IsT 

2 8 

LL Q. 


glL 

|z 

8 n 

Sfi 

— 4 ) 

si 

<D O 
00 to 

w 

II 

CO f- 

<0 o 
co’-g 
CD N 

2c 

> CO 

o Ui 

CL O 


E 

E u) 

8 c 
0 ) 

C/) c 

Qr 

CO W 

§t 

co £ 

O) o 

.£ 8 • 

5*8 

Jr 0 ) 3 
2 "O 

“g.® 

«5 = o 

.£■§ = 

3 3 CO 

Is? 

lit 


<]) 0) 

I ■- 

•“ 5 

E ° 

E o 
o .2. 
o o 

? * 
c X 

i 05 

a a w 

a. 3.2 

c 2c 
.2 .tr a 
o So 

■o So 

a> CO T3 
CO TJ O) 

co is co 

■8 2 3 
w ® o) 
co o g 

JZ o CO 
CO 

” ^ co 
A 7\J> 

to y < 

■H* (ft 

CO 3- c 
LU O O 


0) . 

.Q ^ 

E<J 

= 5 
o Q 

£2 

C TJ 

o a 


c 

o 2 


+«co c/5 </) .E 


O E 

CLj3 

£3 

« W 

2 CO 

a oo 

o o> 

* 4 = 3 

og" 

+-> a. 
O 

fs 

sj 

§i 

9- 

< o 


lu a o 

GO Q. O 
LU O *- 

J2 to (f) 

I« * 

o LU a 

p I 

•-CJ co 
■3 o DC 

CO 2 “ 

•—» co 
■O ts 
Z. c CD 
C <D ■= 

%s % 

§| a 

5-0 o 
OQ Q 0) 


O) 

c 


co rr 


co h» co 


data on projects briefed to SDS TIC. 955-1995 / (703) 521 -3812 



Computer Resources Meetings 

(After 9 Apr 91 CRIM) 


WO 

s 



@ DRC, Crystal City, VA 

22-24 July 91 SDS CRWG Meeting 

@ TBE, Huntsville, AL 





Parallel Processing Topics Matrix 


a 

cu 

< - 

o y 
« § 
o eo 


J s 

, Ed S 
Z_® 

z a'w 

<! .q C« 

a§5 
u ^ ea 

zw^w z 
e-t* ><<<s 

ffi S SJ^ 


§ z* 

cl5 Q£ « 
3 

MHhiO 8 

KWHHw 




fc 2 

g s 

9 

vo Q 



f z 
| <! 

H ^2 
-OS s w 

■So § ^ 

<feUJU 



las 

6S& 


u 

cn 
WPn 
PQ*“ ■ 
DU. 
.UgH 


ttaagaa 

0-« (SCL-C-JH 
HhU>* jo, 
CfaHB 


V) 

to 

t« I 

c ^ 
SCO 
•& h 
3 § ^ 

s | S> 

ca O 

« c» Q 
J2 * ■ 

u »i 

cc 2 


r—^ ^ 

^ S 

u u 2i 

W W fl 

NN HH J3 

pa u co 



Utilization Training Signal Processor 

ARGONNE NAT LAB Acousto-Optic Processing 








































2nd Ada Fundamentals Workshop 

Update 





PRESENTER: Tina Powell 


SDIO Parallel Processing Activities, Tools, etc. 



STRATEGIC DEFENSE SYSTEM 

Command & Control Element (C2E) 
Parallel Programming Briefing for the 



Tina Powell 

Vanguard Research, Inc. 





UNCLASSIFIED 


B 



CO 

0 

wmmm 






m 


5 

6 

6 

5 


c 

o 

To 

(0 

0 

CO 


U) 


(0 

c 

■ ■M 


o 

c 


Q. 

O 

c 

CO 


H 

CL 


O) 

O) 


c 

c 

■MB 


0 

0 


0 

0 


2 

2 


0 

o 


CL 

CL 


CL 

CL 




> 

o> 

o> 

CO 

E 

O) 

o 

Y“ 

T" 

> 

>* 

E 

CO 

0 

3 

2 

2 

CO 

• 

• 

• 


UNCLASSIFIED 






© 

© 

£ 

o 

o 

c 

LD 

C 

§ 

w 

T3 ^ 

8f 

Si 

I 1 

Q. o 

2° 
Q o 

© © 
£g 

SEE 

co 

< 0(0 

i-s 

o 


E 

E 

o 


(0 

a> 

© 

© 

DC 

5 

<0 

<D 


O 3 «- 


© 


o c 


o5 a> 

■go 


5 .2 


o 

(0 


i E 

LU JC 

C*“ w 
<0-0 £ ■= 


©2 
CO 3 

c £ 

CO u 

g>© 

CD O 


3 

c 

a> 

0-* 

o 

E 

2 

a. 


CO 

© 

© 

TO 

c 

© 

§ 


>. 

© 

CO 

E 

X 

o 

a. 

CL 

< 

■a 

co 


Hi:? 


CD 

© 


CO 


£ 

O 

to£ 

og 

JQ O 

2 o 

— 2 
CO 7 
C U 

o — 
o © 

CO © 
Z CO 


o> 

c 

2 
o 

2 
l- 

■o 
c 
© 

c 

© _ 

if 

.2>3 
8| 
< o 
60 

.£ © 
« = 
© © 
© »= 
o © 

G- o 

— s— 

© © 
Vft© =■? 


§§ 
£8 
^ <D 

(0 js 
T3^ 

0 ) o 

Ft 

CD 

5< 
© © 
© -C 


Q. © 
3T3 

25 

s 2 ^ 

co c z 

® 3< 

a. © 
E o 

o| 


a. 

< 

c 

2 

© 

o> 

c 

© 

© 


#1% ■% 
= © 

si 

£§> 

D < 

(0 r- 


© o 

m ° 

© i_ 

Si 
© © 

■§< 

2 c 

c « 
o ©" 

2 E 
© © 
■c © 
© >» 

5© 
© E 


© o 

g s> 

co< 


H- 

0. 

o 


© 

© 

■D 

a 

© 


© 

■o 


O 

0. 

CL 


<D 

O) 

c 

E 

E 

a 


ir> 


Hosted at USAF SSD in Los Angeles, California 

First Day Focused on the System Element Programs and Their System Operation 
and Integration Functions (SOIF) Parallel Processing Requirements 

Second Day Focused on Parallel Programming Research and Results 







UNCLASSIFIED 



co 


(0 

*-> 

c 

o 

£ 

0) 

Q. 

E 


(0 

« 

<o ir 

3 

3 ° 

< o> 
- DC 

= O 

w *55 
tr « 

§ i 2 

o c 
= O o 

Is i 

s g £ 

■o = C 

? £ ° 
T ^ T3 

O) ? $ 

» M o 

O I IL 


O 

o 

X 

o> 

2 o 

11 
— </> 
o> 
o 
o 


(0 
o 

E 

5 <» 


< = 


CB 

C 

o ° 

o> is 


o 

E 

CO 

§ 

<8 

c 

(8 

O 

Q. 

3 

O 

t_ 

O 

8) 


CO 

o 


o 


o 

n 

CB 


« * 


o 8 


8> § 
o E 

B 5 

O < 
.£ o 
2 J 

71 ■o 
D) 0) 
C 

3= S 
0> o 

® X 


CO 
0. 

•*- 
o 

C c 
Q> o 

II 

0,3 

!E LL 
o c 

<| 

(0 

E« 

J: Q 
0(f) 

t If- 

8) o 

CO 
8> C 

11 

° c 
■O 8) 

® C 

si^g, 

o Q. 2 


O C 
LL JE 


1 

X 

co 

co 

3 

O 

co 

Q 

>* 

c 

8 ) 

Q. 

O 

O 

is 

CO 1_ 
CO £ 
8) O 
0) «- 
11 O 

g> cb 
.£ a. 

M 

si 

CB 3= 

2 
8 ) 8 > 
~ CD 


CO 


I I 


i 

co 


UNCLASSIFIED 









UNCLASSIFIED 



&Q 


SutJ 

,S m 


*5 CD 

= o 
«co 

CO'S 
CL ° 
r- C 

o o 

i o 

o X 

*- Ml 

c-o 

5*5. 

X CO 
LU QC 

0) 0) 

£ £ 

*■* *4 

fl) k- 

X ° 

o **- 

q® 

X ® 

LU 3 

So 

«J8 

® TJ 

jfg 

oO 
a> M 
S 2 
a> 

a> *r OT 
0 ) 3 £ 

® Q-o 
*” o o 

o«i 

<D Q) LL 
0) £ e 

Its g 

£|& 

© o a> 


CL 

_ “* 
E CM 

§ o 

CO ® 


« CD 

m CD 

1 i 

o 8 
w c 

® < 
4- Q) 

2 £ 

C t- 

r ° 

o> 2 
o co 

o <75 

= <0 

ft 

O X 
>» 

■= «£ 

i = 

C 0 0 ) 

H "° 


1 8 

O) o 

< K 

c 2 

2 co 

g I 

i? CO 

*z ® 

C CO 
° fl) 
? DC 

2 c 

o> o 

2 E 
2 E 

C O 

E o 

& .| 
<D So 
0) £ 

f ” 

c o 
o «*- 

® E 

o E 
o o 

Li. LL 


S. 

® 8 

® £ 

| <■> 

■£ i> 

co ® 

Q) CD 

3 a 
11 
1 i 

I- CL 


3 

O 

CO CO 
±2 <D 
3 QC 

8 ® 
CC £ 

2- = 

2 5 

0) < 
cd a> 
a. co 
CD ••£ 



UNCLASSIFIED 











Showed that a Significant Increase in Throughput is 
Attainable Using Parallel Programming 


s 

» 


fl) 

3 

Q. 

£ 

Q 

fl) 


0) 

<D 

■o 

■mm 

> 

O 


O) 

c 

•MM 

(0 

(0 

fl) 

o 

o 


«) s> 

2 o 

2 s 

T* fl) 

co£ 
*o *2 

fl> i. 

£ £ 

o £ 

A o 

coo. 


3 

O 

A 

< 


E 

£ 

o 

fl> 


A) 

> 

A) 

A) 

o> 

TJ 

fl) 

g O) 

g.E 

5 co 

* CO 
m fl) 
2z O 

~ o 

£ « 
o <5 
£cl 


a) 

.q 

’co 

CO 

o 

0. 

CO 

•MM 

O) 

c 

« 

CO 

fl) 

o 

o 

k. 

Q. 

fl) 

£ 

• MM 

S co 

CO fl) 
fl) *= 
DC£ 

k. ‘ 

(0 " 
fl) fl) 

z w 
Zj2 

££ 

fl) 

2 o 

CO a> 

£ « 
O A 

E 3 

geo 

O co 


£ 

fl) 

+■» 

CO 

>» 

CO 

fl) 


o 

c 

o 


o 

fl) 

X 

LD 

fl) 

£ 


CO 

fl) 

CC 

CO 


■o 

fl) 

2 

CO 

c 

o 

£ 

fl) 

o 


Integration Functions is Within Reach 







UNCLASSIFIED 



s 

4 


Start to Acquire a Data Base for Building Confidence in 
Achieving the System Integration Function Requirements 




UNCLASSIFIED 








*J= <0 

c 5 
■■■■ 





0) 

</) 




o 

</> 

CO 

m 


3 

O 

CD 

X 

LU 





• • • • • 


o 

i 

5 

» 

o 

5 

n 


UNCLASSIFIED 































p 


•oyf 


co 

c 

o 

m 

0 

3 

E 

5) 

Q 

CO 


U) 

c 

'55 

CO 

0) 

o 

o 


a> 

2 

co 

a. 

o 

m 

co 

0 


o 

E 

a> 


o 

CO 


CD 

*5 

c 

CO 


o> 

o 

o 

c 

o 

o 


CO 

'co 

CO 

< 

o 


o 

CO 

LL 

co 

a> 


CO 

c 

o 

M 

CO 

z 

0 

CO 

2 

0 


o 

c 

a> 

CO 

«•- 
OQ 0) 

cosE 


gg 

E=> 

0.0 

O 4 - 

oO 
>0. 
CD Q. 
Q 


co 

CO 

3 

o 

CO 


0 


CO 

LU 


OO 


O 0 
CO ft 

coO 


0^ 
^co 

i«£ 

«co £ 

CL £ O 

co® (5 

TJO ® 
— o CO 

0 2 0 
I CL DC 


s 

* 




UNCLASSIFIED 







MAY 1991 PPG MEETING TOPICS 


jlli 


Ig-O'63:: 

•ouy: 


m 


SBKTi 


W QJ 

O 0 


(0 0 ) 
■=■0 


li 

0 >.g 

8 «g 

'5. 3 § 
O £ TO 
H- ” <0 

c wg 

.2 QlS 

#5 


O 2 
< <5 

« t 

3 C 

c o 
i o 
O § 

(0 = 

3 S 
O “ c 

g o O 

J> 75 « 

o S = 

■5 >* co 

I ® TJ 
« tt § 

£ S 5 

1 2 £ 

c C £ 

o o Q 

'■£ 2 ■o 
« c c 

3 O « 

,i S « 

2 3 75 
Q M <5 
CO CO O. 


(0 

0 ? 
c £ 

i is 

i 3 

o w 


o. 2 
<5 ± 

« | 2 

o t- a. 
£ • • 




Multi-processor for Data Fusion 
Parallel Assignment Algorithm 
Radar Tracking on the MasPar MP-1 


























































UNCLASSIFIED 


S 

* 


j|pg:: 

0Un3 


(0 

r 

o 

3= 

LU 

o 

D 

CO 

0 


o jo 
£ 0 

£ C 
£ CO 

> a. 

2 g> 

a. ■== 

© o 
ro 2 
O) *" 

o> 


co 

c 

0) 

E 

CO © 

S“ 

^ E 

LU CO 

H O) 
66 O 

<2 Q= 


CO 

© 


o> 


3 I 
co 5 

® o 
a c 
a. o 
< CD 

©.£ 




o 

0 


E II 
o o 

o r 

s 0 

II T3 

TJ 0} 

0 5 

3 ^ 

Sf 0 
® CO 


O) 


0 


'£ 

0 O 

0 fc- 

a. 


c *o 

| CO 
c TO 

a 11 
.£ o co 

0 

|2 1 1 


3 

0 

> 

0 

cc 


3 

o 

c 

0 

co 

0 

a 

o 

c 

0 CO 
CL E 
O £ 
2 2 
0 g; 

ill w 


c 

o 
■ ■■ 

CO 

E 


o 

c 

o 

■ MB 

CO 

c 

IBB 

E 

0 

0 

0 

■ Ml 

a 




Committee Created to Disseminate Information 
Parallel Computing Newsletter 

Parallel Computing Resources/Areas of Expertise Document 





















UNCLASSIFIED 






To £ ~ 

£ © c _ 

H g S.2 

■° : 5 8 S 

(D s: JQ 3 
8 8 =« 
g- o 5lU 


c P 
o ±5 

• ^M SOM 

4 -* k. 

c o 
™ _0) 

o < 

Q ® 
0 ) r- 


cn 

T3 

C 


Am 

3 




% 

CO 


O 

O 

CO 

CO 

0. 

O 

CO 

n 

3 

53 


O 

4jj 

c 

o 

■ MM 

c 

CO 

CO 

c5 

S 

E 

<2 

o 

< 

Q. 

■^m 

o 

CO 

c 

o 

To 

£ 

E 

CO 

CO 

CO 

o 

LLi 

1- 

3 

CO 

£ 

Q 

CO 

a> 

O 

o 

o 

■o 

o 

o 

■D 

m 

m 

c 

o 

TJ 

i_ 

<5 

T3 


CO 

W 


CO 

■o 

C 

(0 

■■ 

"D 

Q. 

f— ' 

TJ 

c 

0) 

w 

■ MM 

C 

E 

■MM 

C 

CO 

> 

75 

S 

o 

a> 

CO 

55 

m 

> 

55 

o 

Q 

CO 


Strawman of Battle Management Functions 








UNCLASSIFIED 




Vanguard Research, Inc. 
703-934-6300; 703-273-9398 (F) 
e_mail: powell@jedi.sdio.mil 












UNCLASSIFIED 




CM 




< 




© 











■ ^B 

CO 



c 

> 

■ 

O) 


3 

E 

© 

c 


E 

• 

■mi 

LU 

■Mi 

E 

E 

2 


o 

o 

CM 


© 

O 

0) 

JC. 

<D 

o> 

o 

E 

2 

o 

O 

mm 

CL 

75 

o> 

o 

© 

© 

c 

■Mi 

o> 

2 

CL 

© 

Q. 

c 

CO 

c 

X 

•mm 

£ 

E 

QL 

c 

© 

E 

o 

111 

■o 

c 

CO 

© 

O) 

UJ 

© 

U> 

o 

© 

O 

© 


© 



o 

a. 

© 

o 

O 

Q. 

3 

o 

^B 

Ou 

© 

2 

X 

3 

© 

CO 

ft 

CO 

cc 

0. 

w 

a> 

© 

© ^ 


■o 

|U 

■o 

■ iM 


■2E 

2 co 

o 

m 

> 

o 

> 

o 

0)0) 

O 

£ 

CL 

CD o 

o 



2=qz 

(0 

i 

1 


• • 


Assembling Data for Building Confidence in Meeting the 
Processing Requirements for an SDS 









UNCLASSIFIED 



UNCLASSIFIED 



































PRESENTER: John Hawk for CPT Emily Andrew 

National Test Bed Software Development Parallel 
Processing Functions & Issues 









PRESENTER: John Schwacke 

Application of Parallel Processing in the 
Test Environment System 




GRC0 


£ E 
55 ® 

® OT 
® > 
8 to 

£ c 

fl) 

© E 
= c 

(O o 

BJ ~ 

•gLU 

c « 

,2£ 

O ® 


CL C 
<" 


O 

H 

W U 

£ 

c 

o 

(0 

o 


O) 

c 

*5 

o 


c 

o 

+* 

CO 

l- 

O) 

5>i- 

c<S 

fl)T- 

3 S 

<D ^ 

DC 1 ” 

a> 


E 

o 

Vi 

5 




GRC0 Background 


c 

o 


Cl) 

0 


= <0 

.E CM 
to O 
CD "0 

f s 

pi 

*"« 0 

■o o. 

0 >> 
-*—» "tr 
13 O 

:QP 


C/3 

T3 


CL 

0 

C/3 


0*0 


C/3 

If 

0 

03 


0 
X 
0 

O 
■*—> 

^T3 

+T 03 

§ *0 
c 0 

CT3 

2 E 

C 03 
LU >, 
^ 0 
0 (f) 

l?Q 
Oco 
C/D 0 

>£ 

LU £ 

I— o 


03 

C 

'o 

03 

i 

C 

o 

c 

0 

C/3 

"c/T 

c 

0 

E 

0 

O 

c 

0 

c 

LU 

E 

0 

■ 4 —• ^ 

if) >* 

co;5 

t! 0 

o Q- 

Q-o 

Cl ° 

3C0 
C0> 
-Jr: LU 
0*” 
E © 


> 

c 

LU 

0 

0 


0 

> 

O 

qL 

E 


LU | 

co 2 
CO 03 
LU E 
I— CL 


0 


0 

■0 

C 

0 

0 
■ 4 —> 

c 

0 


0 

Q_ 

X 

0 


0 

_Q 

E 


0 

C 

"0 

0 

03 

0 

C 

0 

0 

_Q 

0 C 

0 fe 

jc 0 
C 03 

0 2 

■55 cl 

□jo 

hco 

j=co 

1-3 


2 

03 

■ 

0 


.i £ 


“0 

c 

0 


0 

03 

0 

2 

0 

O 

o 


o 

_Q 

0 
O s — 
0 

0 

0 0 
0 CL 

03 E 

0 O 

0 £ 

E § 

#< 

© o 

T3 ° 

CO E 
> .2 

H J 

0 2 

SZ CO 
1— Q. 


I tvs sottware which supports distributed simulation has utility 
in other programs (currently being used to implement a surrogate 
framework for L2SS model development) 





GRC n Experimental Systems Testing 



TEVS < I > Test Article 









Experiment Driver Configuration 


































































































































GRC n Software Layers 


9- ® 

<o co 


: = v: o 

; & w ~ 

l8*8 

i5 

i-r® 
i w ~ c 
I o 5 •*" 

; c ^ w 
■ c -2 ^ 

i o 8 x 


© M- 

> o 

« g 

•II 

c © 


© o 

8 Q- 
© © 

© c 

2 ° 

2 « 

s © 

“■ o 

,ri 

S E « 
g e i2 
c o ® 

3 O « 

E w c 
E ® © 
o S © 
o > $ 

* 2 s 

fi Q_ XI 


© • 

2 w 
2 c 
8.2 
e co 
0.0 

II 

o e 

© 8 
o 

.2 © 
— o 
© x: 

•2 o 

© 

>* © 
-C 0) 

o 

w ® 
© T3 

•o ® 
X £ 


t>2 

° « 

E © 

© o 

E'E 

T3 ® 

© « © 

1|£ 

w 5 o 

? 2 ® 
© a. in 



2 6 
Is 
Si 

*-C0 

(0 

5 


4^ ctj 
2 * 
Ss 

5 


o 

o 

o 

O) t? 

c O 



3 © 
O _C 
O — 

2ir 


| „ 
•H © 

(A X) 
£» GO 
<0 ® 

2> 

•S 

c5 

C ~o 
q> © 

a -2 

o § 

4-* CL < 

r 


© 5 3 

©5 a 
2 © E 

D O O 
O *c o 
© § _ 

£ E © 
>Efi 

^ o » 
x o ® 

® OT c 
® ® © 
OB s © 
O > 5 
O O ♦- 
= C 0 

< CL _Q 


^ © c 

2> 5 o 

(Q >. © 

^ & O 

-D >, ® 

C © o' ® 

5 S o E 

3r >D E jc E 

**• g a. © « o 

CLOSQO 
w) • • • • • 










o 

cc 

o 



All interfaces between the Test Driver and 
Test Article are Message Based 
(Implemented with NOS Buffers) 












Message Flow Across Nodes 



Messages are sent The TCP Agent picks up The NRM is interrupted by the TCP 

between nodes using messages ready for transmission Agent and moves the message to 

TCP/IP. and packs them for efficient the NOS buffer 

transfer to the destination 
node. 




GRC m Message Flow Within 






0 

T3 

O 


05 

CO 



0 

— > 
0 ^ 
TJ D 
O O 

X 

UJ 





CM 

I 

X 

LU 

“0 













Performance of Vectorized 
Trajectory Function 


O 

DC 

(D 


0 

CL 

0 
-*—• 

0) 

"D 
C 

8 

8 9 


Q. 

0 

GO 


E 

3 

E 

x 

CO 

2 


(0 

</) 

0) 

CM 

o 

in 

CO 

h* 

CM 

CM 



o 

o 

T- 


CVJ 


Q. 
0 
• 5 = E 

O '-F 


o 

CM 

r~ 

0 

E 


0 
c? 
0 
0 
2 -g 

o 
o 
00 

0 

E 


a> 

o> 

co 

v. 

> 

< 


CO 

CO 

CO 

CO 

to 

CO 

CM 


CO 

CM 


CO 

o 

o 

r- 


0 

E 

Z3 

E 

■ —■ . _ 

x 

E g 
o E 

u_ 

c E 

1 i 

« 1 

■B E 

. 92 , T 3 
JD “ 
O 

O 
O 

o 


CO 

CO 

CO 

CO 

o 

o 

CM 


CO 

CM 


CO 

o 

o 

r- 


0 

Q. 

0 

GO 

~o 

§ 8- 

8 * 

0 m 

CM Q_ 

.E 0 
o E 

O '4= 
CM 0 

T_ D) 
0 0 

E 0 

o <° 

■*- T 3 
O C 

o 

00 


x 

co 

2 


CO 

CO 

CO 


o 

in 

CO 

CO 

CO 


m 


CM 

d 



o 

O) 

CO 

k. 

0 ) 

> 

< 


CO 

CO 

CO 

CM 


r . 

CO 


o> 


CO 

CO 

CM 


CO 


0 


0 


0 
L_ 

D _ 
m D 

SS w 

0 0 

^ cr 


< 

oc 

H 

GC 

O 

u_ 


3 

0. 


c 

o 

Z ~ 

<f CO 

i. 

E 05 

H o 
oc ~ 

o - 


i o 

| 2 
< H 


0 C 

I i 

X 
0 

E 


E 

CO 

CO 

CO 

3 

r- 

o 

o 

c 

co 

r- 

CO 

c 

** 


in 

£ 

CM 

• 

M 1 

CO 


E 

o 

H— 

c 

£ 

O 


(0 

■o 

< 


3 

CL 


E 

13 

E 

W c 

O 'F 
0 C 

1 ST T3 
O 0 


o 

o 

o 


b- 0 


13 

0 

0 


O ^ 


13 

0 

0 

DC 


< 

GC 

H 

cc 

o 

u. 


3 

0. 


C 

O 

Z 3 = 
< 2 
E » 

h o> 
DC ~ 

O £ 
**" >« 
i © 

3: o 

o 

« a? 
5 2 
< l- 


co 

■o 

< 


0 ) 0 ) 

«r »- 


3 

a. 




GRC0 NWEM Distributed Emulation Capabilities 



• SW Development In C • Pre/Post Processing inC&Ada 

\ ^ • Run Time Displays \ ^ 



































































































































0 

td 

$ 0 

4 ■£ 

Zi m 

5 '§ 

.9 0 

<5 =5 

B o 

.1 • 

to 


cj 

£& 

« E 

is 

•== 0 

"is 

C _ 
0^ 
> s 

0 CO 

0 0 
•*5 D) 

g 0 

o 0 


O OT 
“8 


«_ w 

3 = 

3 C 

0 .§2 
o >* 

^■o 

+- c 
0)5 

•■So 

0*0^ 
■*-• 0 0 

"0 Itz 0 

SB 0 

TJ5I 
0^0 
■£ CO to 

E -5 0 
0 E 0 

Q.C S 

E o c 
-E o E 


0 

c 

“g-B 

Sg 

E | 
0 E 

0 § 

|o 
2E 0 
5 0 

00 

og 

§ Q. 

00 6 

><Z 

$ 0 

> 0 


Q_ -D 
. 0 


Q_ _S> 
op := 0 

X Q- a 
U-O _§ 

c*"7 o 
0 0 — 
= 0 o 

< 3 
0 >, 00 

o c o 

§:" 1 
3 E 0 
CO O > 


-o O) 

C 1= 

(Cj, 

*k 

p 

lx 

o£ 

X Q-CL 

0 ^ 

0 -2 
.>< 0 c 
c?o 

98 $ 

X 0 ^ 
u. E.C 

0 __ 

<*** ■*—»o 

1^1 

< 0 * ® 

t: cz 

o.So 

Q-mCC 

3 Q< 
0 *= 

CO ^ 

.9 2c 
Eo <S 
ScS 

Zoo 
£ ot 
0= 0 
2 CO CL 


12 node system complete and operational in GRC's facility including 
Ada and C development environment and real-time operating system 





PRESENTER: William A. Jarvinen 

Distributed and Parallel Processing in EV88/EVPA: 

An Experimental Prototype Testbed for BM/C2 Processing 







w 

= £ 

^ <o 

■ " Q) 

<H 
1 > Q 

J- CO o 

CO CO 
CL > O 

■o ULJ £ 


C - 
- cc 


co 

■o 

5>.E c 
□ to j 
o CO.I 

=2 a) ; 
o < 

P 


co 

5 ol 


c 

o 

c 

£ 

<0 

■ 

< 

■ 

5 


14 May 1991 




Distributed and Parallel Processing 

In EV88/EVPA 


! | 1 
its 


<0 «0 
O >» 

H C 0 
O m 


<0 _ 
C 12 


<0 

U § 

c o 
o C 
(0 3 

0) Li. 

§■ O 

CC C 

S I 

O 3 
■— xt 


d) <D 

O .Q 


(0 M— 

CL O 


Olu « 

§ S « f 

LL Q Q £ 


CL 

E 

■O o 
© o 
c o 

(5 < 
a> < 

-j 2 
(0 > 

§ S 

CO $ 
CO S 

a> > 











CO 


0491 - 040/01 



















LU 

Q. 

hO 


A C f) 

2cn 

Cl in 


LU 


CM 


xcc 

LUO 

-9 Li. 


Ill 

<m 


><o 

111 LU 

CO l_ 
00 
> 

LU 


ffi 

LU 5 
X c 
CO LU 
3 LU 
X Z 

<5 O 
z z 
piu 

CO o 
LU DC 

ag 

o >- 

t ® 
o o 

X -I 

x o 

11 

2 Uf 

LU *" 
(3 o 
< “ 


LU 


X — 
LU ffi 

2(0 


tzS 

< O LU 
ffi O x 


Z 

o 

i- 

3 

O 

LU 

X S 
LU LU 

10 

x« 

nj Q 

> 

Ms 

“Eg 

g^E 
s g o 
< ° < 
ZQZ 

> z < 
Q < 5 


> 

I- 


ffi 

< 

Q. 

< 

O 

_J 

<S 

X 


2 

LU 


< 

LU 

X 


O 

z 

X 

LU 

LU 

z 

o 


o 


LU 

X 

gg 

0 . o 

sg 

go 

o 




g a IS §3 ef 
2£s32 



















INTERCEPTOR 

SUCCESS 





EV BM/C3 Experimental System 

















































BM/C2 TEST ARTICLES DISTRIBUTED 
ON HETEROGENEOUS NETWORK 





































































EVPA BATTLE MANAGER IS A S/W 
PRE-PROTOTYPE 









1 

*5 


o 

! 


GO 











Distributed Operations Require that an 
Object Follow Fundamental Rules 




Support the Definition of Processes to Enable Dynamic Creation 
of Instances of that Process via Data-Driven Methodology 




Distributed Operations Require 
An Appropriate Tool Set 





DCDS Support For Distributed 
Battle Management Design 



c 

c *S 

.5>co 

8® 


0 > (0 

E.g> c § 

Sc D) CO 

£o3> 

0) 
g-D 

to w 

co 5 w 

S J5 ® 

T3 O 


c 

a> 

U) 

c 

o 


CO 

a> 


o co 2 
(OZQ. 


o 

co 

55 _ 

>> CO 
“ +■* 

c 

a> 

IS 

•-< 

O CM 



c 

o 

■ IB 

c ® 

■O S c 

® a> E ® 
3 cco E 

|SSsg- 

QC "O a) CD <d 


o 

o 

in 

o 

o 

• 

o 


a s 

CO CO 

CO m 
C CO 
CO *2 

k. (0 
1-0 


® 0.0 
® CO -77 

SgSfi 

c 5 > jg 

g c § c 

c 0 )N = 

E ‘35 iE 73 
o ait; c 
OT}3 (0 






REAL-TIME 
EXECUTIVE (RTE) 
SERVICES 





Run-Time Executive (RTE) 




CM 





Run-Time Executive's Interface to the 
Process and Testbed Environment 




in 

O 

m 

o 

o 

i 

LJJ 


CO 


Testbed Node 



Typical Process Connectivity 



2 co 

3| 

O.ti 

^ to 
= 0 
-Q 0) 

co co 

Q_ <D 
CO O 
o g 

O Q. 

©O 

CC 0 

o>_j 
CC l— 
w o 

® ° r 

7-5 1 — l— 

•goo 

> _Q 4=: 
2 £ 2 
0-3 3 
>-<= o> 

>® C 

■43 = o 
0^0 

o O ^ 

c ~ c 
c « 0 

0 o E 
o^.E 

O'? © 

!!§■ 

cc£cq 


The DDL and associated code is integrated and tested 
once. 

Allows for relationships of the test article to be dynamic 
at run time (multiple midcourse engagement planners 
each assigned to a class of controllers). 



The Process Construction System (PCS) 































































































Building A Run-Time Environment 



2 o 
<B Q. <0 
C Q-£ 
I ® 3~ 

CD CO — 1 


to 

"O 

oJ <r 
“O 

« <C 4S o> 

“D D) 

“O ’iS Q.C 
< <D ! S co 
*1 o £ ^ 2, 
eg 45 E 1 I 

(i 1 £ a D ^ 

J J J J« 

a> a> a> a> b 

CO CO CO CO £T 
3 3 3 3 

1 I IU QJ* \jJ I I J 

fc fc fc fc fc 
CC DC GC OC OC 


< 1 ) 

LU CO 
H- *tz 

oc o> 


CO 

CD 


CO 

CO 

0 


CO 

o 

CL 


cd a? 

EE 

i'l 

CO OC 
"co 00 

IE 

— CO 

aj'S 




Distributed Ada Environment Tools 


n 



Capability to Playback or Regenerate Subsystem and 
Standalone Environments for Problem Isolation 




LESSONS LEARNED ON EV 



U) 

C 

‘55 

CO 

a> 

o 

o 

cl 

a> 

E 

■ MM 

H 

■ 

CO 

a> 

cc 


■o 

>»C0 
CTJ 
<D C 

3<S 

JS3 

®TJ 

E ® 
F« 

_L O 

co *5 

4l 

•E g 

§H 

“ -h- 

to g 

4- <D 

g® 

e£ 

a>Q 

Q.> 

•i| C 

e?.2 

£ 2® 
” ffl 3 

03 E 

fgas 

iS.£ <8 

ps°? 

*7 3C 
T3 (0 <D 
C © > 
UJOCLLi 


CO 

.2 ca¬ 

ts .Eo 

(OC? 

l-m to 

« ®3 
<D Q c 

*5>T3.| 
a> c (/) 

« 2 « 

is w ^ 

55®aT 

CO CO T3 
CD CO Q) 

co — ■£ 

S9S. 

0=0 
DC a> 4 = 
> c 

5 °< 

iu 

®Bt „ 
(0= O 

I!s 

tg2 

5g.E 

o 

ge-« 

tog 

CD CO C 
(Out 

c <d a> 

— O-*- 


“ tt— d) 

CD O 3 C 

eS 

~ ® ® E 
H ®DC o 

« = s> 

0 >0 c 
cr ffl^Lu 


co 

E 

o 


(U C 4- 

■■— m CO 


CO 

■o 

0> 

3 

n 

mw^m 

to 

■ MM 

o 



Distributed Works - for Real-Time,For Ada, For Large-Scale 
Developments 

Requires The Development Of An Appropriate Tool Set 
Have To Make The Investment In Tools 




LESSONS LEARNED (Continued) 








EV88/EVPA ACCOMPLISHMENTS 



LU 

X 


o 

X 

X 


00 


LU 

h 

LU 


O 

O 

< 


00 

UJ 

> 

< 

X 

LU 

£ 


< 

o 


CO 

LU 

O 

< 

LL 

X 

LU 


LU 

> 

X 

Q 

3 

O 


o 

£ 


c 

o 


< 

UJ 

x 


■2 

3 

E 

• MM 


t 

£1 


co 

E 


Z LU 

02 


<o 

1 DC 


i> 


2 

0) 

>* 

CO 

■o 

c 

LU 

I 

s 


__ *D 

UJu £ 

go C 
O LU < 

z— *” 

Z LU O 

2^ Z 

“5 . 

<0. 


c 

o 


o 

CO 


c 

<D 

E 

© 


co 

z 

o 

h 

O 


LU 


© 

c 

a> 


CQ 

LL 

o 


UJ 

CO 

Ul 


LU 

-I 

X 


CO 


O 

O 

< 

CO 


X 

o 

LL 

X 

LU 

0. 


LL 

a 

co 


C 

o 

Q. 

Q. 

3 

CO 


E 

E 

o 

o 


CO 

o 

X 


O) © 
CO Q 


s- ~ 


O) 


_ © 
£ E 
c © 

& CO I 

» •§ © 


5 < 


o = 


a a * 


c 

© 

E 

© 

o> 

© 


4- © 

c © 
© < 
E 2 
© © 


o 
© © 


© m 
© .<0 


= < 


h- I- 


i 


P 


Z UJ 


o 

X 

H 


co 

X 

o 

CO 


o 

o 


LU 

CO 


a 

z 

< 


CO 


o 

X 

< 

UJ 

£ 


X 

H 




c 

© 

E 

c 

o> 


LU 

> 

X 

Q 


m 

C0 © D> 


co CM 
Z UJ 
O-f 

Li Q 

buj m 

S 1 . > 

u] co 
ao 

LL X 


— QQ UJ 


C 

© Q. 
© © 


a 

z{2 

oco 

ho 

<LU 
X oo 
LU 


00 


3 

o 

o> 


c * 


< -T F 


X 

Q 


< 2 


X 

o 

CO 

z 

UJ 

co 


2 

CO 

LU 

a 

x 

< 


>■ 

oo 


a 

o 


CO 


o 

< 

X 


X 

o 

C 5 


< 

LU 

X 


UJ 


I- 

< 


< 

LU 

X 


a 

UJ 


co 

< 


X 

o 


< 

> 

< 


UJ 

> 

LU 

a 


co co 
z 


I V 

O) CL 
3 O 


o 

o 

I- 


9-. O 


ui 


UJ 


< 

X 

< 

X 

Q 


< 

Q 

UJ 

H 

3 

00 

X 

H 

CO 

a 

UJ 


< 

X 

< 

X 

a 

z 

< 

a 

UJ 

i- 

3 

CQ 

X 


X CD ^ 


CO 

Q 


O 

a 


UJ 


O 

a 

UJ 

oo co 


x 

< 

UJ 


s o § 

Z o W 

i § s 





PROBLEMS SOLVED SHOULD TRANSFER AS "GRAIN SIZE" DECREASES 






PRESENTER: Gordon Bate 

Computer Resource Simulation Tool for 
Distributed Systems (ARCSIM) 



(5 a Zb 0^ 2AT£Sj orx 

ADVANCED RESEARCH CENTER 



uoujuioq 











NATIONAL TEST BED 



uoujiuoo 








JEFF(fc)025/91 *01 p 































TOOL IMPROVEMENTS OVER 
ARCSIM (V3.0) 





I 


McGRAW(fc)102/91-04a 




TOOL IMPROVEMENTS OVER 
ARCSIM (V3.0) 




> 

CO 

H 

cc 

< 

-1 

CL 

CO 

O 

CL 

LU 

X 

O 

z 

z 

o 

LU 


LU 


X 

O 

< 

N 

CO 

-1 

o 

1- 

D 

o 

X 

s 

O 

LL 

Li. 

X 

1 

Q. 

o 

< 

H 

a 

X 

LU 

0 

O 

H 

D 

< 

Q 

LU 

> 

OC 

5 

• 

• 


X 

H 

Z 

LU 

S 

LU 

> 

o 

X 

CL 


Q 

O 

X 

h 

LU 


O 

LU 


O 

O 

< 

H 

< 

O 


LU 

O 

O 


> 

X 

O 


< 

0 


X 

z 


X 

* 

LL 

J 

X 

o 

X 

< 

CO 

LU 


CO 

H 

X 

> 

D 

o 


X 

z 

< 


LU 

z 

o 

X 

< 

o 

LL 

Q 

LL 

O 

z 

O 

z 

< 

z 

o 

z 

o 

H 

< 


H 

< 

X 

o< 

X 

LU 

dQ 

LU 

z 

QO 

Z 

LU 

LU 2 

X 

0 

X — 

0 

O 

Oa 

ox 

H 

H-< 

< 

<2 

<=: 


Sx 


o 

oo 

°D 


trZ 

DLU 


< 

<x 


• 

• 

• 


McGRAW(tc)102/91 -05a 





TOOL IMPROVEMENTS OVER 
ARCSIM (V3.0) 



v'UUUPv/l o 


oc2 



0 . 0 ) 

hi |. 

CO DC fc 

fc S?h° 

£ tz z 

O tt£o 

0 - O 5 CO 

g' o> gsS. 

<0 p<z £ tiuoL< 

P _ Ssp y<2o^ 

«2 uiP< 

Lh p (— j LLl J nr . 

< saw «<£< 

h IU l “2SS«)iuzP2 

9 agS“ a 68 §Segi 

OhZil- zZZ 11 J - Hill 

og^2o 


5 iu o 
gs£2 

- m CO DC 

0> Af c/) ^ ^ 

P<Z CC 11IDl< 

s-io wMst; 

agP S<°Q 
Bj£5 

SCLUJ (0<£< 
I“PCC,„mjZPO 




£EC0DC< z 3O|-CC^Ii.C/)D 
S LLl 3 LU O CL DC -1 o o Z _1 c/) 
C/)CCHDCOOQ-DUJ^OXuj 
D.. SCChOOtC 



McGRAW<fc)102/91-06a 








ARCSIM V3.1 BASELINE 



PRINT TEXT/PRINT GRAPHICS FACILITY 





COMPUTER 
AIDED 

SIMULATION 
GENERATION 
FOR 

DISTRIBUTED 

COMPUTING 

ENVIRONMENTS 


f RUN SIMULATION \ 
(Network 11.5®) 

PC 



OVERVIEW 


Optimization Technology, Inc. (OH) has developed an advanced capability for the rapid generation of accurate 
high fidelity simulations of distributed computer environments composed of multi-network, multi-protocol and 
multi-computer configurations. This technology has been successfully applied to such diverse domains as the 
Strategic Defense Command's Advanced Research Center, the Strategic Defense Initiative National Test Facility and 
the Air Force Satellite Control Network; each of which are representative of the complex networked computing 
configurations that are commonplace in today's environment. Resulting simulations of these configurations support 
such activities as (1) optimizing connectivity and resources for specific applications, (2) identifying overloads, 
bottlenecks and inefficiencies in given network configurations, (3) optimizing computer/network expansion and 
growth and (4) optimizing resource management and configuration control. This approach to automated simulation 
generation of networked computing systems permits the user to develop extensive simulations in minutes or hours 
as opposed to weeks or months, to rapidly modify simulation functionality and to extend the domain and range of 
simulation capability by adding resource models to a Repository. 

A fundamental and successful objective in the development of this tool was to provide individuals such as 
system managers, operations personnel, software designers, resource managers and network architects (not necessarily 
simulation experts) with the capability to rapidly generate accurate simulations of networked computer 
configurations. An overview of the approach is illustrated above. 


OPTIMIZATION TECHNOLOGY, INC. • PROGRESS CENTER, SUITE 600 • 6767 MADISON PIKE, NW • HUNTSVILLE, ALABAMA 35806 

Pbons (205) 721-1288 







The heart of the system is a model Repository or library containing verified computer, network, protocol, 
router, long haul communication, algorithm, etc. models. With assistance from the tool, models can be easily added 
to the repository at any level of detail or fidelity. Currently the repository contains very high fidelity models which 
have been validated with an empirical test suite (also provided by the tool). To enhance the tool's usefulness in the 
investigation of network/computer expansion options, lower fidelity models created from vendor data can be quickly 
developed and entered into the repository. As more accurate data are made available, the fidelity of these models can 
be easily increased. 

In order to construct a simulation of the desired configuration the user first graphically describes the network and 
network connectivity, the computers, the proposed software architecture, etc. The Scenario Generator retrieves the 
desig-nated models from the Repository and appropriately binds and enhances them in a form to be passed to the 
Translator, which in turn creates a Network EL5® image of the simulation to be generated. 

Network IL5®, developed by CACI, is a powerful event driven simulation language that incorporates the ability 
to model complex and dynamic system interactions at varying degrees of fidelity. The presence of pre-defined 
building blocks such as Processing Elements, Transfer and Storage Devices, as well as some pre-defined Media 
Access layer protocols and detailed data collection capabilities, make Network II.5® an ideal medium for the 
translation stage of the tool. 

The source image of the simulation is then translated to a runtime image to be executed on one of many 
computing platforms supported by Network IL5®. Performance data is collected during runtime and stored for 
subsequent data reduction. Network II.5® has an extremely powerful data collection capability, providing data on 
virtually any aspect of simulation performance. 

The Data Reduction facility provides a data file to the Results Presentation facility, which in turn produces line 
charts, bar charts, tables, etc. as well as tabular accounts of the resulting performance. The Results Presentation 
facility supports the presentation of user designated performance measures such as throughput, utilization, task 
information, device usage and media access by isolating appropriate performance entities, developing their 
relationships and plotting them in a pre-designated format. 

OTT first developed a version of this tool, ARCSIM, under government contract for the Strategic Defense 
Command’s Advanced Research Center (ARC). The ARC contains a massive amount of computer and networking 
resources as shown below. For this application, the tool is principally utilized to optimize the distributed 
environment for large complex defense experiments. The experiment may be configured using any subset of 
available resources. An experiment planner has the ability to accurately simulate and optimize the configuration and 
its associated performance prior to committing to a given topology. 



ETHERNET 


m 


HYPERCHANNEL 


L 


ARC NETWORKS 


SILICON 

GRAPHICS 

( 15 ) 


SUN 

WORKSTATIONS 



COMPUTER 

TERMINALS 

(105) 


(7) 


ADVANCED RESEARCH CENTER 


OPTIMIZATION TECHNOLOGY. INC. • PROGRESS CENTER, SUITE 600 • 6767 MADISON PIKE. NW • HUNTSVILLE. ALABAMA 35006 

Phon* (205) 721-1208 










OTI is currently developing a derivative of the ARCSIM tool for the Strategic Defense Initiative National Test 
Facility (NTF). This tool, called NTBSIM will not only provide the capability to support National Testbed 
architectural analysis through the addition of resource models to the Repository but will also provide the 
functionality for resource planning and management. In addition to expanding the model Repository, the Scenario 
Generator is being modified and the Results Presentation capability is being extended to accommodate the new 
capabilities. An overview of NTF resources is presented below. 



NATIONAL TEST FACILITY 


OTI is developing yet another derivative of the ARCSIM tool for the Air Force Satellite Control Network to (1) 
support the Mission Support Network Service/Deficiency Reporting System by proving a capab^ty for die 
analysis of system access delays and (2) provide an AFSCN Performance and Analysis Tool (APSAT). An overview 
of these two applications is presented below. 



MISSION SUPPORT NETWORK APSAT 

SERVICE/DEFICIENCY REPORTING SYSTEM 


The capabilities for the above three applications have been hosted on an IBM PC compatible machine, (386 
with expanded memory) in Pascal. Network II.5® is available for the PC, which provides the user with a complete 
PC environment. As pointed out earlier. Network II.5® is also available for many other platforms. For extensive 
simulations, the user generates the run-time image on the PC, executes the simulation on an alternate platform, and 
ports the performance data back to the PC for Data Reduction and Results Presentation. 


OPTIMIZATION TECHNOLOGY, INC. • PROGRESS CENTER, SUITE 600 * 6767 MADISON PIKE, NW • HUNTSVILLE. ALABAMA 35006 

Phone (205) 721-1280 














PRESENTER: Evan Lock 


Re-Engineering Existing Software Into Distributed Applications 




















Computer Command and Control Company 



Computer Command and Control Company 











The Re-engineering Challenge 



Computer Command and Control Company 





Modem Applications Will Take Advantage of Distributed Architectures 



Computer Command and Control Company 



Summary of Overall Approach 





Computer Command and Control Company 


















Summary of CCCC Re-Engineering System 



Computer Command and Control Company 




Distributed Application Workbench 



Computer Command and Control Company 









Distributed Application Workbench 





Computer Command and Control Company 












Distributed Application Workbench 



Computer Command and Control Company 








SIMULATOR Functionality 



Computer Command and Control Company 




SIMULATOR Features 



Computer Command and Control Company 





SIMULATOR Benefits 



Computer Command and Control Company 










Computer Command and Control Company 
















BUILDER Features 


>tjik 



Computer Command and Control Company 






BUILDER Benefits 


O 

3 w 

T3 *0 

O <D 
Lh <d 

CU £ 

<3 bo 



£ g> 

2 1 
w> =c 

<— _ 


co <D 


<r! co 

^ <D 

<D 

co H *“ l 

03 5-1 

<U <D 



e 


.9 

<& 

*4—» 

o 

cd 

e 

cd 

C 

0) 

Vh 

<L> 

ft 

o 

C 

JD 

'3 

3 

s 

o3 

C/D 


<D 

O 


zs 

OJ 

*o 

<D 

wk 

O 


Computer Command and Control Company 


















MANAGER Functionality 



Computer Command and Control Company 











MANAGER Features 




co 

g 

o 

» 

4 —> 

G 

O 

G 

G 


O 

u 

73 

G 

G 


<D 

4-4 

C/3 

00 

b0 

G 

• ^ 

4 —> 

G 

CD 

a, 

o 


CO 

CD 


X 

<D 


O 

U 

CO 

(D 

73 


G 

O 

• *H 
4 —» 

G 

5-4 

G 

bD 

•'■H 

<+h 

G 

o 

u 

G 

O 


C/3 

4—> 

CO 

G 

G 

o 

• H 



<D 

o 

• 

04 



s 

G 

4-4 

G 

o 

< 


<D 

G 

O 

G 

<+H 


G 

$-4 

•*“4 

FH 

G 

O 


H 

> 

G 

w 

CO 

s 

s 

o 

G 

O 

• i-H 

+-> 

cd 

W) 

G 

G 

G 

P4 

G 

O 

O 

73 

0 

•tH 

4-h 

• fH 

b£> 

bD 

G 

5-h 

>*✓ 

G 

G 

T3 

O 

O 

a; 

G 

O 

H-l 

CO 

• fH 

bo 

O 

CO 

44 

s 

0 

C! 

> 

73 

*h 

<D 

CO 

G 

0 

• H 

G 

• H 

< 

4-4 

<D 

H 

s 

H 

4b 

ffi 

CO 

G 

G 

4-4 
• r-H 

G 

• r-H 

C/3 

<D 

S3 

• iH 

G 

G 

>> 

Q 

G 

Pi 

CO 

£ 

CO 

<D 

<D 

O 

•_, 

C/3 

0 

O 

4—> 

G 

4H 

45 

4-4 
• r-H 

73 

• —H 

.G 

4 —1 

$-h 

O 


> 


<D 

04 

G 

CD 

O 

4 —> 
ri 

O 

00 

P4 

Ph 

44 

M 

• 

• 

• 

• 

• 



Computer Command and Control Company 







MANAGER Benefits 



Computer Command and Control Company 













SIMULATOR, BUILDER, MANAGER Working Together 



Computer Command and Control Company 















Computer Command and Control Company 










Distributed Combat Systems Simulation (NUSC) 



Computer Command and Control Company 















Distributed Verification Calculations (DOD Contractor) 




















Distributed Avionics Simulation — LAMPS Helicopter (NADC) 



Computer Command and Control Company 



























Distributed Application Workbench 




Computer Command and Control Company 






PRESENTER: Gregory H. Chisholm 

SDIO Related Activities at Argonne 





a 

£h 

o 

<D bJO 

Vh 


<2 <d 

CD 

P4 ed 




Argonne National Laboratory May 13, 1991 



■a a -s 
1) 2 <L> 

S * 


W) 3 

m I 


§ g 

• rH j_, 

or) <S 


4) ,25 

S'!)® 

£ g Jj 

«S 2 3 

O c3 eS 
00 Oh PL, 


□ □ □ 


May 13, 1991 GHC 2 41st GRIM Mtg. 


Software Engineering Environment and Tools 

Goal: to assist SDI contractors in learning to use parallel 
processors and to provide them with access to parallel 


It 

s 

St 

so 

St 

2 


a 

o 

u 

<D 

Oh 

o 

tin 

Pi 

U 

< 


C/3 

V 

a 

• rH 

43 

o 


U 

§ 

2 § 

£ 

o 


C/3 

o 

o 

4-> 

W) 


T3 


bD 

O 

*h 


< 9 < 


I 

S 13 

* f-H f«-H 

C/3 03 

<D H jvt 
C/3 03 v/J 


•|»* i-H 

« Oh 
^ 2 

S) 

St 
* ^ 

-St 
SJ 

St 


C/3 

C/3 

<D 

o 

O 

< 



□ □ □ 


May 13, 1991 GHC 3 41st CRIM Mtg. 





s> 

s 


oi 

u 


§ 


S 


one-bit processors (8KB each). 



wo ~c 
H c 

2 5 

as w 

^ Vi 

£ ~5 
© ,© 
C/5 H 


S3 S' 
o G 
.£h ti 

ts 2 

§ 4D 

G 5 
o ^ 

U oo 

c/3 

<U ^ 

G 

3 2 

£ § 

oo 

<i <U 

g> o 

G Vh 

a ^ 

.5 .* 

rG ^ 
i 

H ID 

< § 


(N 


OX) 


CO 


<4H 


G 

o 

OX) 


’G 

x> 

G 


c3 

’S 

• 

pG 

C/3 

s 

pG 

C/3 

C/3 

C/3 

O 

• pH 

,£3 

C/3 

o 

^5 

i—H 

Vh 

o 

c/3 

C/3 

<D 

& 

OX) 

C/3 

C/3 

<D 

O 

O 

*£3 

• rH 
£ 

O 

O 

<D 

o 

Vh 

Ph 

<D 

pO 

Vh 

Ph 

*_, 

§ 

to 

^j- 

G 

2 

fH 

O 

4_J 

6 

£ 

<D 

Ph 

o 

<2 



<u 

> 

Vh 

0) 

£ 

pG 

^t 

pG 

Ph 

i 

pG 

OX) 

o 

o 

o 

<d 

T3 

O 

G 


"g 

G * 

I £ 

« a 

g ja 

<D G 

H <+h 

< o 

5=3 s 

< s 


h 

. o 

Gh S 

Th <d 

2 S 

h-> 

G 

PQ O 


So 

8S. o 

U G 

^0 u 

Oh V 


<D ^ 

4-» *H 

^ I 

< a 


e 

« qq 

c O 


H vi 

Ph 00 

£ _G 

o -S 

o -r; 

. > 

rv 

+-J 

c/3 <D 
<D X> 
M R 

<3 2 

o> 

^3 Ph 

G >> 
c3 pG 

“ -3 

oo 2 

13 c 
00 
C/3 <N 

T3 

o S 

> 00 


memory. Acquired jointly by CalTech, ANL and others. 



Software Engineering Environment and 
Tools (Cont’d) 

The ANL ACRF 


bD 

a 


</) 

C/5 





□ 




• • • 


May 13, 1991 GHC 6 41st CRIM Mtg. 



Software Engineering Environment and 
Tools (Cont’d) 

ACRF Activities 


C/3 

a 

o 

• rH 

C/3 

• rH 

> 

T3 

j—i 

<D 

43 


T3 

§ 

00 

U 


8 

3 

8 

43 

o 

c3 

*t 3 

<D 

i 

> 


O 

h-> 

a> 

■g 

> 

a3 

43 


<4H 

o 

C/3 

Vh 

<d 

a 

<D 

s 

>> 

o 

c3 

<u 

C/3 

<D 

^ <D 

<D g 

W) g 

2 o 

3 cd 
o < to 

Ch Oh 

w a o 


o 

,cd 

i 

<D 


^H 

<D 

C/3 

o 

I 


C/3 

<D 

o 

• f-H 

O 

c3 


£ 

<D 

o 

Vh 

£ 

<U 

X) 

H—» 
C/3 

<D 

a: 

<D 

+3 (D 

g T3 

Q, »H 

a S 

8 £ 


T3 

g 

<D 

c3 


<43 

O 

C/3 

43 

bS) 

O 

o 


43 

o 

c3 

<d 

C/3 

<D 

tn 

<D 

T3 

• 

C/3 

S3 

o 

£P<2 


<D 

C/3 

C 

o 

i 

o 


t3 ^ 

o g 

Oh O 
On\g 
c3 

°° a 

<d a> 
'O S3 


^ > 

Oh O 

Oh *h 


O 

o 

o 


ctf Ph 


T3 

g 


C/3 

c3 

a 


<u 

C/3 


C/3 

<D 

C/3 

C/3 

c3 

■a 

o 

o 

a 


C/3 

^H 

<D 

C/3 

S3 

13 

O 

<D 

H—* 

aa 

o 2 

08 'g 

a ^ 

•§ o 
W £ 


□ □ □ □ 


□ 


May 13, 1991 GHC 7 41st CRIM Mtg. 


Software Engineering Environment and 
Tools (Cont’d) 

Parallel Programming Classes 


• • 

C/3 

<D 

W) 

c3 




z 

< 

04 

< 2 u 


□ 


C/3 

<D 


C/3 



a 


<D 

• rH 



• 

r^ 

cd 


o 



g 


CJ 













i i 


13 

Vh 

£ 

<D 

X 

a 

g 

g- 



G 

H 

<D 


Oh 

zzj 

•rH 

C/3 

C/3 


+H 


Ch 

a 

o 

C/3 

Vh 

00 

s 

a 

O 

C/3 

i-i 

o 
• ^ 

c3 



C/3 


1 


<u 

rX 

• rH 

Pu 

g 

4-^ 

g 

<D 

• ^ 
Oh 

g 

Vh 

O 

£ 

O 

o 

gn 

<L> 

O 

o 

r g 

O 

o 

C 

3 


00 

W 

cj 

00 


"O 


< 

• 

• 

< 

• 

□ 



□ 



May 13, 1991 GHC 8 41st CRIM Mtg. 



Software Engineering Environment and 
Tools (Cont’d) 

Tools 


C/3 

3 

o 

bX) 

G 

e 

I 

?—i 

bD 

O 

Vh 

a. 

i 

3 

3 

c3 

Oh 

3 

cb 

ts 

o 

Oh 

<^-H 

o 

H-> 

G 

<D 


C/3 

3 

O 

h—> 

>> 

u 

O 

a 

D 

a 


G 73 
Oh (D 

o b 

i—H 03 

> C/3 

<D 

Q . 

□ 


eo 



C/3 

£ 


0 

• rH 

C/3 

o 

g 

3 

o 

• rH 

3 

i 

Vh 

bX) 

0 

<2 

bX) 

O 

• rH 

CO 

CO 

a 

CO 

§ 

Oh 

O 

Oh 

l 

JO 

• rH 

bX) 

<D 

G 

O 

bX) 

c3 

C/3 

CZ3 

Cj 

6b 

►J 

3 

CD 

o 

<r_. 


S 

rH 

Oh 

3 

?-H 



cb 

• 

• 

Oh 


T3 

s 

Vh 


bX) 

O 

3 

Vh 

Plh 

r-H 

3 

c3 

Oh 

ctf 

t-H 

O 

Vh 

G 


□ 


May 13, 1991 GHC 9 41st CRIM Mtg. 



Goal: to develop approaches for parallelizing SDI 
algorithms and to identify the optimal environment 


■S 

o 

■s 

£ 

a 


<D 


d O 
O 43 


<D 




□ □ 


4 > 

CL, 


a 

fJi 

• H 

C/3 

s 

a 

X) 

• 

4—> 

C/3 

CO 


o 

<D 


<D 

O 

•c 

o 

.3} 

43 

O 

< 

W) 

13 

1 

• 


May 13, 1991 GHC 10 41st CRIM Mtg. 




May 13, 1991 GHC 11 41st CRIM Mtg. 




Parallel Simulation (Cont’d) 


C/3 

4—> 

3 

C/5 

<D 

oi 

2 

• rH 

a 

<D 

4—> 

o 

□ 


r rt 

o s 

• fh jh 

a * t-H 

Sh 

O CT 1 

£ g 

cS O 
bX) x 
•13 X> 


C/3 

<U 

> 

.s ^ 

^ <D 


<L> 

Vh 

3 


2 

2 

o 

£ 


C/3 

<D 

t-H 

3 

.ti 

•fi 

i 

m 

3 

O . 

o 
£ 

0) 

bO - 

8 Q. 

(D H-1 

g Tb 

^ s 

q-H «* 

o bo 

>% .S 

r—H $—I 

r9 | 

C/5 4_> 

cd <D 

£ A ^ 75 

.S w o § s 

^ ^ §■ I S 

. § § 

^ 60 > 

• rH O o 

s is a 

S 3 & 

Tb 2 E 

HH pH • rH 


<D 

^H r—H 

<D x> 


2 

Vh 

o 


a W) 

ST s 

*H X 

t-t 

C/3 O 

W ~ & 

s « 

w 3 3 

O U 00 

g 3 H 
§20 
« a u 

CS rH 


_> 00 
2 CCS 
cS <£> 

Ph ^ 

4-> 

H O 

2 £ 

O S-H 

• pH pH 

tj s 

Oh S 
*H X 

°l 

C/5 

3 
o 

<D 

£ 

* n ^ 

W) 

cS O 

& 3 
g x > 


C/5 

a 

*5 

•c 

o 


o 

X> 


u 

O 


<4-| 


38 


T3 

<1> 

<D 

a 


J-H 

^ G 
<+h m 


o 

03 

g q=3 

p4 <3-> 

>> ± 

C/3 


C/5 

C3 


b0 

a 


r <D C/3 

PL, p 


£ 

0) 

Tb 


o 

o 

4—> 

o 

a 


<D 
O 

o 

Vh Vh 

P4 O 


May 13, 1991 GHC 12 41st CRIM Mtg. 



Fault-tolerant, Reliable, Portable 
Computing for the SDS 

Goal: to explore new technologies in support of the 


£ 


^ % 

« ,3 

O ^ 
S § 

« § 

O ^ 

-£3 

Cj 

Co o 

Q ^ 

00 

• 1-0 

•l*»a ^ 

•§ <£, 


£ 

<P Xi 


u >: 

(D ^ 

& ^ 
X ’Td 

W < 

5? T3 

.5 

c/3 N 
c/d irj 


O 

* g 

13 ^ 

^ oJ 

Lh r —h 

c3 X) 

Oh o3 

3 £ 

C o 
c3 Oh 

Vh 


£ C3 

l Vh 

+3 PS 
^ o 
5 ^ 
<1 

□ □ 


May 13, 1991 GHC 13 41st CRIM Mtg. 


WD 


S 

a 

S 

o 

U 

3 

ts 

o 

PU| 

•N 

QJ 

1 2 
"Z c 
© 

§ 55 
i2 cc 
3 a> 

ii £ 

I is 


s 

•l** 

v* 

S? 

bJ 

^o 

<>> 

>5 

s 

&H 

8 

si 

a 

I 

a 



□ □ 


□ 


May 13, 1991 GHC 14 41st CRIM Mtg. 


Fault-tolerant, Reliable, Portable Computing 
for the SDS (Cont’d) 

Fault-tolerant Parallel Processing Experiment (Cont’d) 


C/3 

P 

O 

• fH 
4-> 
C/3 

<D 

<D 

C/3 

£ 

<D 

bD 

• fH 

> 

C/3 

<D 

> 

a 


a 

<D 


4h 

o 


o 

a 

o 

V C/3 

I c 

<D 

a 6 

cS <D 

pH i-H 

Oh pq 

□ □ 



c 

4 ) 

S3 S 

& 'C 

X D 
& P< 
cj X 
c3 « 

<D 

fP 


s* < 

* pH 

^ C/3 

a, c 
S -2 

O d 
o W) 


r 1 r 

£ 2 

£ c3 


a 

o 

C/3 


55 « S 
S f3 S 



. ■ CD 

H O <+h 
<D cj 

W oo oo 


May 13, 1991 GHC 15 41st CRIM Mtg. 




WD 

c 

s 

a 

S 

Q 

QJ 

3 

as 

ts 

o 

#S 

S- 

a 2 
13 c 

cd ® 

3 , U 
* ^5 

s- Q 
.2 CO 

3 «, 

a 3 

« fe 


co 

•i«»a 

k. 

3 

« 

Q 


”§ 

N 

1 

"§ 

« 

a 

Co 

•s. 

a 

o 


a 

5 

a 

o 


o 

*/g 

a 


cd 

O 

•g 

<D 

a 

§ 

<D 

I 

n 

o 

cu 

<D 


§ 

T3 

c3 

'O 

s 

H—> 

C/D 

o 

a 

<D 

c3 


8 

£ .S 


G 

O 

$e 


1 

•a 

<D 

s 

§ 

t! 

o 

a 

&0 

a 


<u 

co 

§ 

O 

<D 

X) 


• r-i 

T3 « 

CO 


.. CO 

C C 
o o 

• rH *rt ^ 

-y +3 co 
cd cd <d 

^ 3 M 

5 & S 

co g 7 * 

-? s « 

O Oh 




I 


o 

co 


a 

<D 

a 

a 
o 

*> cd 

s 5 

CD < 

<D (D 

a > 

G *JH 

cd co 

■g | 

13 

O rd 

o S 


"G 

§ 

C/5 

<u 

& 

2 

a 

W) 

.2 cd 

C *g 

co ^-4 
<D <s 

4-> 

- a 

<D 

O co 
\£ <D 
O bD 
cd cd 
b ^ 

Qh o 

„ cd 
>, Oh 

1'S 

“ 1 
Oh G 

o a 

£> vh 

> o 

<D 4h 
^ CO 

I "2 

! cd 

a 

cd 

H—» 
CO 


§ 


□ 


□ □ 


May 13, 1991 GHC 16 41st CRIM Mtg. 



Fault-tolerant, Reliable, Portable Computing 
for the SDS (Cont’d) 


G “ 
G 73 


^ R 
g § 


3 *C 
a g 

£ « 


£? o 
3 Ph 
^g o 

G Vh 

O p, 


<D C/3 


i> (2 

2P c 

S o 

M rj 
u ^ 

G$ 


00 P 

G .fa 
O G 
\G CT 
o p 
G ^ 

cS 

ON S 
<N 3 

r-H C) 

13 ^ 

c/3 00 

O d> 

”2 ^ 

G oo 


<D 8 
2§ 3 

5 o 

Th C/3 

o ^ 

D, S 


G •! 


00 

in 

<D 

a 


G 

Oh 

G 

o 

u 

• rH 

£h 

O 

u 

> 

G 

a> 


May 13, 1991 GHC 17 41st CRIM Mtg. 



Fault-tolerant, Reliable, Portable Computing 
for the SDS (Cont’d) 


O 

.§ 

O 

1 

CZ> 


•s 




N 

•i«*a 

a 

CO 

•s 

IS 

a s 
Q ° u 




3 

a 


3 


S rg 


oa 

■3- 

C/3 

O 

o 

* 

+-> 

o 

a 

,o 

4—i 
X 

B 

o 

o 


C/3 

O 

o 

<2 


O 

o 


O 

<D 

e * 


C/3 

CD 

O 

.5 

-a 

o 

£ 


ccj <u 

' 4h 


U « fc 

□ □ 


C/3 


C/3 


C/3 

a 

.2 

c5 

4—> 

CD 


<D 


w *£ 

>* o _ 

S 73 S 


Jh 

o 

(D 


(D 


c3 

»1 


O 

-§ 

a 

o 


CD 

l—-H 

Oh 


03 

C/3 

bf) 

O 


5Q c3 


W) 

o 


> 

<D 


Oh 

<D 

tn 


3 U Q £ 


May 13, 1991 GHC 18 41st CRIM Mte. 



Fault-tolerant, Reliable, Portable Computing 
for the SDS (Cont’d) 

Fault-tolerant Parallel Processing Experiment—Summary 


73 

<D 

CO 

p 

13 

H—> 

S3 

I 

o 

o 


P 

o 


Vh 

o 

' .. 

fP CO 




<D 

•g 

o 

£ 

o 

& 


§ 

•N 

73 

<u 

<P 

O XJ 

g 8 

Ph cP 
co P 

o 


o 

Vh 

<D 
P 
bJ) 

• pH " 

s 

^ p 

<D ° 
rP 00 
+-» P 

* 43 

3? co 

*3 8 

3 3 

o “ 
< ^ 

□ 


C/3 

p 

bO 

.9 

P 

S3 

Ph 

O 

o 

« 


£ 

<D 

H—> 
C/3 

C/3 


<D 

Ph 

C/3 

C/3 

P 

bO 

P 

* 

P 

S3 

Ph 

o 


8 

<D 

H—» 
C/3 

C/3 


<D 

J3 2 
H -S 

t-h cN 


bO 

.a 

p 

b 

CO 

P 

O 


CD 

73 

P 

O 

H—» 

P 

<D 

73 

P 

D 

Ph 

<D 

73 


§ 


<D P 

b o 
^ cp 
. • 
> o 

^ CD 


P 

O 

*p 

p 

o 

tp 

• rH 

o 

<D 

Ph 

CO 

<D 

CO 

p 

73 

O 

O 

bO 


> 


Ph g 
CO P 

P b*'* 

2^3 

P3 O 
co .P 

O > co 

r~] ^ *i-h 

Q D co 
P o CO 
O c< (D 

fe § g 

OS* gn 3, 

5 * i « 

h; 8 5 


□ 


T 


May 13, 1991 GHC 19 41st CRIM Mtg. 


WD 

S 

• pN 

3 

a 

S 

o 
O 

3 
t2 

o 

&H 

•N 

QJ 

3 

"flS s 

cd ® 

3* u 

§ ^ 
fe O 

Ji co 
2 « 
2S 5 

II 


St 


k. 

Hr 

b3 

&o 

st 

oi 

tn 

g 

a, 

*> -v 

3 *^ 

s. <p 

Q « § 

O 

*«* 

st 

Q I 

4J ^ 

a s 


St 


CO 


'■o 

.§ 

co 

<u 

T3 

co 

<L> 

• rH 
4-> 

Vh 

<L> 

Oh 

O 

Vh 

Oh 

□ 


CO 

<D 

ts 

<D 

Oh 

o 

*H 

Oh 

13 

O 

0) 

co 

CO 

<D 

£ 

rr co 

^ o 

w O 

B w 

jl> 3 

& S 

^ a 

o 
o 


O 

.Si 

43 g 

o S 

a S 

p 4-> 

h <*> 

• S * 

-£3 “ a 

w -o S 

s s « 

O ctf co 

• 1—I >-x 

CO o 

2 & jg 

& <D O 

s ^ ^ 

O —( o 

O S 

3 8 * 

£ ; 


2 tS 
S ^ 


o 

• rH 
4-> 

o 


ctf 

H-J 

O 


O 

o 


CO CO 


<D fl: > 
cj bD 2 

fl T) 5 

p ‘P* 

C b co 

CO CO 

ti W CO 

o _ & 

S s & 

o *o S 

o *5 o 

o o 2 

D £ M 


May 13, 1991 * GHC 21 41st CRIM Mtg. 





bb 

I 


at 

u 


c* 

<s 

V 

ffi 

o 


»—* 
a 


& 


s 



WD 

S 

• pHp 

s 

a 

a 

© 

U 

eg 

ts 

o 

Ph 

•N 

a> 


ci ? 

• p* *7 

"S B 

fig{ o 

Q 

pS c a 

S V 


a 

ft o 
to « 


So* 

St 


?k 

5f 

fe3 

^0 

st 

•1^ 

^3 

S 

"■■OA 

<^> 


5 

a 


St 

o 


St 

Ho 


a 


o 

^ s 

2 ^ 

*3 .5 


C/3 

+-> 

a 

<D 

.fcJ 

£ 

a 

O 

o 

• 

H—> 

cj 

O 

<0 

o 

<D 

Oh 

C/3 


C/3 

<D 

•c 

o 

bD 

<D 

c3 

U 

□ 


C/3 

a 

<D 

o 

o 

Oh 

s 

o 

o 


o 

& 

o 

o 


C/3 
h—> 

O 

<D 

e 

o 

Vh 

• fH 

& 

2 a 

13 

o 

o 

• t-H 
+-> 

o 

O 

c2 


o 
o 

’O 
o 

d 

S 

■g | 

c3 «* 

£ 1 

s s> 

>* _rj 

03 '§) 
«4-H * Jj 

O 43 

•O § 
13 *h 
o 
o 


a> 


+-> x> 

O ctf 
O C/3 

^ a 

g 8 
cO 8 O 

<D (D <U 
'd T3 fl 


00 


r r 


03 

O 

<D 

£ 

<D 

Vh 

► fH 

§< 

<D 

Vh 


Oh 

§< 

J-H 

o 

o 

I 

o 

.a 

> 

o 

<D 

>% 

X) 

4> 

C/3 

o 

£ 

• fH 

00 


o3 

£3 

C/3 

o 

o 

o 

o> 

o 

cO 


J-H 

bJQ 

• ^H 

0 

T$ 

C/3 


<D 

t 

"d 

1 


O 

X) 


Oh 

O 


2 

<- 1—1 

-g 

c3 

<3 


43 

bX) 

o 

o 

£ 

o *2 

.O <U 
ti O 
c3 rj 

O 03 


T 


May 13, 1991 GHC 23 41st CRIM Mtg. 


OD 

a 

a 

a 

E 

o 

U 

QJ 

3 

a 

ts 

o 

cs 

If 

"33 c 

cd o 

lo 

§ <£ 

a3 Q 
Jj CZ3 

a « 

£} 5 

!l 




I 


KS 


G< 

r *HT 

fe3 

&0 

si 

•i** 

k. 

si 

Q 

s< 


s 


o 

o 

I 

<3 


M 

4h 


>% 

ts 

<D 

Oh 

o 

Vh 

Oh 

<D 

<G 


a 

<D 


<D 

Vs 

» 

a^ 

% 

13 

G 

O 


o 
G 

O G 

<D <+H 

G< 

00 • 

□ 


as 

13 

i 

o 

<D 

. . H 

Gh 


G 

g 


G 

<D 


2a 

► rH 

& 

0) 

Vh 

G 

bJ) 

* i-H 

C/3 


CZD 

15 

g 

o 

G 

a> 

<D 

& 

a$ 

x> 

0) 

o 

G 

<D 

TJ 

G 

<L> 

Gh 

<t> 

"O 

G 


T -8 T 


May 13, 1991 GHC 24 41st CRIM Mtg. 



F 




a 

| 


C/3 

a 

o 

•g 

cS 

• r-H 

O 

<D 

Oh 

C/3 

73 

o 

i 

Vi 

<D 


ctf 

4h 

O 

C/3 

<D 

Vh 

3 

cd 

r <D 

Ph 

□ 



§ 

+-> 

O 

cj 

£ 

£ £ 

£ — 

^ > 

s 2 

2 lib w 


1i 

I t> 


ctf 

4—* 

<D 

T5 


& a 
« 6 


May 13, 1991 GHC 25 41st CRIM Mtg. 



Fault-tolerant, Reliable, Portable Computing 
for the SDS (Cont’d) 


CZ) 


S £ 
o 3 

oS 

a Q* 

'E. o 
— e 

Ch CQ C2 

•2 « .2 

3 6 c3 
3 o 2 
Oh to ^ 

a js & 

o a 

^ £P 

£ -S S 
S S 

fc cs S 

o -S 2 

4-H 


tiik a> 

O rj 
O t 3 

Vh 

D. O 


< < .3 


o3 

XS * 

o .a 

a is 

g <u 

> Oh 

<£ o 
o ^ 

00 Oh 
<+h £ 

O g 

o 'S 
o 

00 

O 

g cj 

• rH 

+-> 'g 
C/3 C 

O O 

O °2 

U M 

<D 


described by the system specification. 


Fault-tolerant, Reliable, Portable Computing 
for the SDS (Cont’d) 

Fault-tolerant Parallel Processing Experiment 



representation 



May 13, 1991 GHC 28 41st CRIM Mtg. 




c 

© 

1/2 

j3 

C 

© 

U 


<D 

C2 


P 

• rH 


«h 


<D 

CO 

p< 

X 

Q 

rN 

<D 

CO 

<D 

Vh 

Vh 

o 

3 

<*H 

o 

bX) 

<D 

| ^ 

P 

• rH 

43 

\P 

P 

o 

Vh 

& 

03 

s 

C/3 

o 

P 

O 

u 

<D 

<D 

P 

<D 

2} 

bX) 

o3 

h—» 

O 

J-H 

Vh 

o 

a 

Ph 

<u 

#\ 

43 

<D 


-S 

Ch 

.2 & 
os 

v-H 4-> 

2 5 
S 2 
•a 

i> 

’Tj 

03 , T2 

5-h P 

o3 03 

Ph Ph 

□ □ 


p 

<D 


C/3 

<D 

•s 

£ 

hJ 


Oj 


u 2 

CD ^ 
& 1 ^ 
W < 

si? « 


C/3 

C/3 

<D 


N 

• rH 

■g 


V 8 

O 'O 


Vh 

Oh 


§ 


U CO 

rv 

oj o 
H 


03 ^ 

Oh P 

7! ts 

Ch o 

g PL| 

?—I 


<u 


<D 


£ c* 

I Vh 

*3 P 

^3 O 

PL, <| 


C/3 

e2 

§ 

g 


<4H 

o 

CO 

□ 


a 

i 

a 

o 

o 


Q 

CO 

u 

a 

C/3 

<L> 

• rH 

43 

8 

a 


bX) 

P 


c3 

^ Vh 

S bO 


c3 

Ph 


o 

i~t 

• rH 

> 

tS 

bO 

P 

• pH 

tH 

CD 
D 

l=i 

• rH 

bX) 

, 

HH i c/5 

g Ph g 

S3 P$ 


O 

?-l 

Ph 

s 

13 

8 S 

° Ph 


C/3 

C/3 

<D 


C 


C/3 

C/3 


U J2 
< U 


May 13, 1991 GHC 29 41st CRIM Mtg. 



