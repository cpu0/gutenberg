N89-19881 


A MULTI-SENSOR SYSTEM FOR ROBOTICS PROXIMITY OPERATIONS 


J.B. Cheatham, C.K. Wu, P.L. Weiland 
Rice University 

Mechanical Engineering & Materials Science 
Houston, Texas 77251-1892 

T.F. Cleghom 

NASA/Johnson Space Center - FM 7 
Houston, Texas 77058 


ABSTRACT 

Robots without sensors can perform only simple 
repetitive tasks and cannot cope with unplanned events. 
A multi-sensor system is needed for a robot to locate a 
target, move into its neighborhood and perform operations 
in contact with the object. This paper describes systems 
that can be used for such tasks. 


INTRODUCTION 

An integrated robotic system capable cf performing 
tasks in which the target is in an arbitrary location 
and orientation is described in this paper. This system con- 
sists of a PUMA 560 robot manipulator, a vision system 
with a camera mounted over the robotic work area, a 
force/moment sensor, a laser range finder, and an infrared 
proximity sensor. Two laboratory experiments were per- 
formed successfully using this system. One experiment 
simulates a simple industrial assembly task and the other 
permits battery replacement in a model of the Solaimax 
satellite. In the first experiment, the vision system is used 
to locate randomly placed parts. In the second, a laser 
range finder is combined with the infrared proximity 
sensor to locate the satellite model. These two ’visual’ sys- 
tems provide redundant information about an unk- 
nown environment. This redundancy can be used to check 
or confirm the visual information provided by either sys- 
tem. Fine motion control is achieved by employing the 
force/moment transducer and the infrared proximity sen- 
sor. 

Sensors are monitored and controlled by an expert 
system written in CLIPS (C Language Integrated Produc- 
tion System). CLIPS is a rule-based language written in C. 
All rules are sequenced by a priority rating. The priority 
rating is set by the rule ordering method [1] which 
arranges all rules in one long priority list. The rule with the 
highest priority, appearing earliest in the list, is triggered 
first. 


SYSTEM OVERVIEW 

The RTX robot, manufactured by UMI, is a six 
degree-of-ffeedom robot with five revolute axes and one 
translational axis. This manipulator is operated under pro- 
gram control using the C language. Belt drives are used 
for the shoulder, elbow and yaw joints and bevel gears 
for the roll and pitch axes. Accuracy of repetitive move- 
ments is within 0.5 mm. 

Figure 1 depicts the system configuration. The 
PUMA 560 is an industrial grade, six degree-of- freedom 
robot. This manipulator can be operated interactively or 
under program control using the VAL Et language. A 
NAMCO Lasemet scanner attached to the first joint of the 
PUMA provides a vertical search arc of 90 degrees to a dis- 
tance of twenty feet. Rotating this device about the verti- 
cal first joint of the PUMA effectively provides a complete 
search of the robot’s workspace. 



Figure 1. System configuration 

A specially designed tape that reflects incoming light 
back at the incident angle is attached to the target. The 
returning pulse is detected by the Lasemet, which signals 
that the target has been located. An infrared proximity 
sensor, built into the end effector, determines the target 
orientation by examining a known surface of the target. 

A JR3 force/moment transducer, mounted on the end 
effector of the PUMA, provides a means for controlling 
the motion of the robot while maneuvering the objects. The 
JR3, an industrial grade force/moment measuring device 
[2], consists of a six degree-of-freedom force sensor 
and an electronics enclosure that contains a 12-bit ana- 


467 





log to digital converter. Resolution of the force meas- 
urements is 1 part out of 4096 of the full scale, (+/- 25 lbs 

along the x and y axis, +/- 50 lbs along the z-axis, and +/- 
75 in- lbs for moments). The digital data output from the 
A/D board is transferred to the PUMA controller via the 
direct memory access (DMA) bus. A VAL II subroutine, 
provided by the JR3 company, can be called in a program 
for measuring the forces and moments exerted on the end 
effector. 

A 3M vision development language (VDL) system 
with a camera mounted over the robotic work area pro- 
vides a high-speed vision development workstation for an 
IBM XT computer. This system includes a 512 x 512 x 8 
digitizer with two 256 Kbyte frame buffers and a signal 
processing board for image acquisition and processing. A 
wide variety of vision algorithms can be implemented 
interactively using macros, programmed with a command 
interpreter called VDL-BASIC, or programmed using C 
language supported by a C command subroutine 
library. Work described in this paper use the C pro- 
gramming capabilities. 


INTELLIGENT VISION-ASSISTED 
ASSEMBLY TASK 

The first experiment simulates an assembly task 
involving two robots. A number of objects of various 
sizes and shapes are randomly placed in the work area 
(see Figure 2). A camera over the work area captures 
images of the objects and data relating to the images are 
passed from the vision system to the expert system for 
identification. One by one the objects are retrieved by the 
RTX robot and placed in a position for use by a PUMA 560 
robot in an assembly task. The objects are picked up by 
the Puma and inserted into holes of the proper sizes and 
shapes under force- moment control. 



Figure 2. Top view of the objects and receptacle 

An overhead camera connected to the 3M VDL 
vision system provides digitized images of the unknown 
targets. The vision program consists of a calibration 
subroutine, an image acquisition subroutine, a target 
extraction subroutine, and a properties measurement sub- 
routine. Video images of the objects are digitized and 
thresholded into binary images by the image acquisition 
routine. Targets are represented as white objects on a 
black background in the binary image. The target extrac- 
tion routine searches the binary image for a single white 


object and encloses the object within a window. The con- 
tour of the windowed object is then traced and the loca- 
tions of the comers are determined [3]. This procedure is 
repeated for all objects in the viewing plane. 

Processed vision data are transmitted from the vision 
system to the expert system on a Compaq 386 through an 
asynchronous communication line. The CLIPS program 
analyzes the vision data and determines the shapes, sizes 
and locations of the objects. This program consists of 
rules to identify the objects and to sort the targets by their 
geometric properties. It then transmits the position data to 
the RTX. 

The RTX is programmed in C to perform the disk 
retrieval task. Center of gravity and orientation data of an 
object are developed by the expert system program. These 
data provide the necessary information for the RTX to 
pick up the object and move it to a predefined location. 
Upon completion of this task the control program is 
notified of the task completion and the RTX returns to its 
home position. The PUMA 560, programmed in VAL n, is 
instructed by the expert system program to retrieve the 
disk and move it to the taught position for the assembly 
work station. Insertion of the object is accomplished 
using force/moment feedback provided by the JR3 
transducer. After successfully inserting an object a signal 
is transmitted from the PUMA to the expert system pro- 
gram. This process is repeated for the remaining objects. 
Upon completion of the insertion of the last object, both 
robots return to home positions. 

BATTERY REPLACEMENT IN A 
SATELLITE MODEL 

The goal of the second experiment is to develop a 
procedure for changing batteries in a satellite. A PUMA 
robot is programmed to perform this task autonomously. 
The robot carries a tray with tools and fresh batteries in a 
known position with respect to the robot. The location and 
orientation of the satellite containing the battery compart- 
ment is unknown. 

An essential part of this experiment is to locate 
the satellite model without using the vision system. A scan- 
ning laser range finder is used to locate a reflective material 
attached to the target and to determine the coordinates of 
the object. A proximity sensor, which is housed in the 
end effector (see Figure 3), is then used to determine the 
orientation of the target. This system provides an alterna- 
tive means of acquiring positioning information and is 
relatively inexpensive compared with a vision system [4]. 



Figure 3. Infrared sensor housed in an 
Interchangeable end effector 


468 




ORIGINAL PAGE IS 
OF POOR QUALITY 


The Lasemet senses only its reflected light. 
Commercially available reflective tape can reflect 
incident light back to the origin at angles up to 70 degree 
from the surface normal. However, variations in this angle 
are observed. The width of the target must be known for 
the Lasemet data to be used in determining distance. An 
object in a horizontal plane can readily be located by using 
a flat, circular reflective tape. As the object’s orienta- 
tion becomes more complex, so does the target selection. 
Ideally, a spherical target will provide the most uni- 
form projection at any orientation. In this experiment, a 
spherical reflective target was constructed out of the 
reflective tape adhered to a golf ball. 

During operation, the PUMA rotates the Lasemet 
about its first joint searching for the spherical target 
attached to the satellite. When the target is located the 
Lasemet sends a signal to the PUMA which records its first 
joint angle. The location of the spherical target is then 
computed from the range and angle data from the Laser- 
net and the PUMA’s first joint angle [4]. 

Once the spherical reflector has been located, the IR 
proximity sensor is used to conduct a search for a smaller 
circular reflector (see Figure 4). The proximity sensor 
receives a reflected "on" signal within 2 cm of the alumi- 
num plate on the satellite model and up to 50 cm from the 
reflective tape targets. The end effector is moved in a cir- 
cular search pattern of radius equal to the known dis- 
tance from the sphere. By locating the two reflective tar- 
gets a line is defined on which the satellite model is 
located. The proximity sensor is then moved midway 
between the reflectors and lowered to detect the object’s 
surface. A search for the edges of the aluminum plate 
defines it’s orientation. The sensor receives a positive 
impulse while over the surface, but loses the signal as it 
crosses over the edge. The orientation of the satellite 
model is found by tracing one of the edges. A false edge 
is used to determine which edge is found. A non-reflective 
strip is placed down the length of the object. When first 
seen by the sensor it appears as an edge. The sensor will 
continue onward after the 'edge' is found and regain con- 
tact with the surface. This on-off-on signal will signify 
the location of the false edge. Knowing these points the 
orientation of the object can be established. 



Figure 4. Object/Target Configuration 

Once the location of the satellite model is com- 
pletely determined, the battery replacement task may 
proceed. The PUMA engages an electric cover actuator 


to turn a power screw which opens and closes the door 
over the battery compartment. A J-Slot type assembly is 
used to pick up both the battery packs and the electric 
cover actuator [5]. Figure 5 depicts the electric actuator 
with the J-Slot. The retrieval and insertion of batteries 
into the battery compartment are performed under posi- 
tion and force control using the JR3 sensor. 



Figure 5. Electric actuator with J-Slot assembly 


CONCLUSIONS 

Two systems have been described that can be used for 
robot proximity operations. A vision system combined 
with a force sensor has been used to locate and identify 
objects then retrieve and insert them into receptacles of 
appropriate size and shape. An expert system written in 
CLIPS interprets the vision data and sends instructions to 
two robots used in the assembly task. 

The second system used a laser range finder, an 
infrared proximity sensor, a force/moment transducer and 
special reflective targets to locate a satellite model and 
replace a battery pack in the model. A combination of both 
system can provide redundancy that could permit improved 
reliability for space robotics activities. 


ACKNOWLEDGMENTS 

The authors appreciate support of this work by 
NASA/JSC under Grant NAG-208 and RICIS contract 
NCC9-16. 


References 

[1] Giarratano, J.C., CLIPS User’s Guide, A. I. Section, 
NASA/JSC, Houston, September, 1986. 

[2] JR3, Universal Force/Moment Sensor System, Opera- 
tional Manual, JR3, Inc., Woodland, CA, 1985. 

[3] Cheatham, J.B., Wu, C.K., Chen, Y.C. and 
Cleghom, T.F., ’’Interpretation of Robot Vision Images 
via An Expert System", ASME International Computers 
in Engineering Conference, San Francisco, CA, 1988. 


469 


i 



t 

r 


[4] Cheatham, J.B., Weiland, P.L. and Wu, C.K., M A Sen- 
sor System for Determining Position and Orientation of 
Robot Targets”, 3rd International Conference on 
CAD/CAM Robotics and Factories of the Future, 
Southfield, MICH, 1988. 

[5] Armstrong, A., Dunn, P., Hekma-Wierda, D., Hodge, 
A. and Howard, D., Design, Construction and Implementa- 
tion of a Robotic System for Replacing Batteries, Senior 
Design Project Report, MEMS Dept., Rice Univ., May, 
1987. 


470 


