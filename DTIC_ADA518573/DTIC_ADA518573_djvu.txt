A Timing Requirements-Aware Scratchpad Memory 
Allocation Scheme for a Precision Timed Architecture 



Hiren D. Patel 
Ben Lickly 
Bas Burgers 
Edward A. Lee 


Electrical Engineering and Computer Sciences 
University of California at Berkeley 


Technical Report No. UCB/EECS-2008-115 

http://www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-115.html 


September 12, 2008 









Report Documentation Page 

Form Approved 

OMB No. 0704-0188 

Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and 
maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, 
including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington 

VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it 
does not display a currently valid OMB control number. 

1. REPORT DATE 

12 SEP 2008 2. REPORT TYPE 

3. DATES COVERED 

00-00-2008 to 00-00-2008 

4. TITLE AND SUBTITLE 

A Timing Requirements-Aware Scratchpad Memory Allocation Scheme 
for a Precision Timed Architecture 

5a. CONTRACT NUMBER 

5b. GRANT NUMBER 

5c. PROGRAM ELEMENT NUMBER 

6. AUTHOR(S) 

5d. PROJECT NUMBER 

5e. TASK NUMBER 

5f. WORK UNIT NUMBER 

7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES) 

University of California at Berkeley,Electrical Engineering and 

Computer Sciences,Berkeley,CA,94720-1700 

8. PERFORMING ORGANIZATION 

REPORT NUMBER 

9. SPONSORING/MONITORING AGENCY NAME(S) AND ADDRESS(ES) 

10. SPONSOR/MONITOR'S ACRONYM(S) 

11. SPONSOR/MONITOR'S REPORT 
NUMBER(S) 

12. DISTRIBUTION/AVAILABILITY STATEMENT 

Approved for public release; distribution unlimited 

13. SUPPLEMENTARY NOTES 

14. ABSTRACT 

see report 

15. SUBJECT TERMS 

16. SECURITY CLASSIFICATION OF: 17. LIMITATION OF 

___ ABSTRACT 

18. NUMBER 19a. NAME OF 

OF PAGES RESPONSIBLE PERSON 

a. REPORT b. ABSTRACT c. THIS PAGE Same OS 

unclassified unclassified unclassified Report (SAR) 

17 


Standard Form 298 (Rev. 8-98) 

Prescribed by ANSI Std Z39-18 





Copyright 2008, by the author(s). 

All rights reserved. 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, to 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission. 


Acknowledgment 

This work was supported in part by the Center for Hybrid and Embedded Software 
Systems (CHESS) at UC Berkeley, which receives support from the National 
Science Foundation (NSF awards #0720882 (CSR-EHS:PRET) and #0720841 
(CSR-CPS)), the U. S. Army Research Office (ARO#W91 INF-07-2-0019), the 
U. S. Air Force Office of Scientific Research (MURI #FA9550-06-0312), the Air 
Force Research Lab (AFRL), the State of California Micro Program, and the 
following companies: Agilent, Bosch, HSBC, Lockheed-Martin, National 
Instruments, and Toyota. 



A Timing Requirements-Aware Scratchpad Memory Allocation 
Scheme for a Precision Timed Architecture 


Hiren D. Patel, Ben Lickly and Edward A. Lee, 
Department of Electrical Engineering and Computer Sciences, 
University of California, Berkeley, 

Berkeley, California 94720-1776 
Email : {hiren,blickly,eal}@eecs.berkeley.edu 

Bas Burgers 
University of Twente, 

Enschede, The Netherlands 
Email: b.j.burgers@student.utwente.nl 

September 12, 2008 


Abstract 

The precision timed architecture presents a real-time embedded processor with instruction-set extensions that 
provide precise timing control via timing instructions to the programmer. Programmers not only describe their func¬ 
tionality using C, but they can also prescribe timing requirements in the program. We target this architecture and 
present a static scratchpad memory allocation scheme that greedily attempts to meet these timing requirements. Our 
objective is to schedule minimum number of instructions and minimize data allocation to the scratchpads such that 
timing requirements in the program are met. Once the timing requirements are satisfied, the remainder of the scratch¬ 
pad memory can be used to optimize some other metric desired by the programmer. As an example, we minimize 
the frequency of main memory accesses in the program. This work presents the following: 1) high-level timing con¬ 
structs for C that synthesize to timing instructions and 2) a greedy iterative instruction and data scratchpad memory 
allocation scheme that attempts to first meet the specified timing requirements. 


1 Introduction 

In real-time embedded systems, scratchpad memories are often used as an alternative to on-chip cache memories. 
They offer a reduction in area, low power and low energy consumption solution. More importantly, they provide a 
high degree of timing predictability [2, 10, 18, 20], Scratchpads do not require hardware policies to determine the 
transfers on and off the scratchpads, thus reducing hardware logic, power, area and improving timing predictability. 
But, the cost of using scratchpads is that the programmer is responsible for scheduling these transfers. Consequently, 
automatic compiler-level memory allocation schemes are desirable. 

There are numerous scratchpad memory allocation schemes targeting non-real-time embedded systems [1, 12, 19] 
that focus on improving the average-case execution time (ACET) of a program. But simply improving the ACET 


1 



does not help in meeting the timing requirements of a real-time embedded application. Alternatively, worst-case 
execution time (WCET) based memory allocation schemes for real-time embedded systems are proposed in [17, 14, 
4], These approaches minimize a task’s worst-case execution path by scheduling instructions and data words to the 
scratchpads. A real-time operating system (RTOS) and its scheduling algorithms are used to specify the programmer’s 
timing requirements. They do not necessarily make an effort to meet the programmer’s intended timing requirements 
because that information is unavailable in the program. Unfortunately, it is difficult for a compiler to do this because 
today’s programming languages do not specify timing requirements and most embedded processors lack mechanisms 
to enforce them. 

An exception is the precision timed (PRET) architecture proposed by Lickly et al [9], Edwards and Lee [5] recently 
made the case to reintroduce time as a first-class property at all abstraction levels, and the PRET architecture [9] 
presents their initial efforts. This is a real-time embedded processor that provides timing as predictable as its logical 
function. It is a multithreaded processor based on the SPARC instruction-set architecture with scratchpad memories, 
extensions for precise timing control and no RTOS. It accepts C programs compiled with the GCC toolchain. 

While using the PRET architecture, we noticed two issues that make its practical use difficult. They are: 1) timing 
requirements are encoded via low-level assembly commands and 2) there is an assumption that instructions and data 
fit entirely on the scratchpads. In our experience, it is inconvenient to use assembly-level instructions intermingled 
with C and any realistic application is typically greater than the scratchpad size. Our efforts in this work are to address 
these two issues. 



Figure 1: Simple Example using Timing Constructs 

We address the first issue by introducing timing constructs in C with associated semantics. These synthesize to 
PRET’s timing instructions [7, 5, 9] and they are used as a means of specifying timing requirements in the program. 
The synthesis of timing instructions is a source-to-source transformation for which we use an open-source C front-end 


2 
















parser [3]. For the second issue, we propose a static memory allocation scheme that greedily assigns instructions and 
data words on the scratchpads to satisfy the timing requirements specified by the programmer. 

Typically, scratchpad memory allocation schemes focused on improving ACET use profile-based methods, and 
schemes concerned about real-time (WCET-based) use static program analysis. The latter is particularly important for 
safety-critical and hard real-time embedded systems. But, we are interested in soft real-time embedded applications 
where timing violations as a result of not exercising the absolute worst-case path are undesired, but not hazardous. 
Furthermore, constantly evolving architectures and traditional input-data dependent programming styles hinder the 
wide use of static WCET analysis [15]. Since Lickly et al [9] clearly indicate that the PRET architecture is still in its 
infancy and evolving, we opt for a profile-based method. In the future, we plan to generate probability distributions 
from the profiled data and then perform the allocation with knowledge of the expected cases as well. 

In this work, we leverage PRET’s timing instructions [5, 9, 8] and introduce timing constructs for C as a means 
of specifying timing requirements in the program. We then shift our focus to a static memory allocation scheme that 
greedily assigns instructions and data words to scratchpads to satisfy the timing requirements and then stops, leaving 
resources available for non-real-time functions. Our objective is to schedule the minimum number of instructions and 
minimize data allocation to the scratchpads such that the timing requirements specified in the program are met. This 
allows the remaining scratchpad space to be used as the programmer sees fit, such as to minimize power consumption 
or the execution time. We present one possible approach that uses the remaining scratchpad space to minimize the 
frequency of main memory accesses of the entire program. We select a static memory allocation scheme approach 
because dynamic schemes add transfer instructions in the original program; possibly resulting in changing execution 
times. To our knowledge, this is the first scratchpad memory allocation scheme that tries to meet timing requirements 
specified in programs. 

2 Related Work 

Some work on scratchpad memory allocation schemes for non-real-time embedded applications are presented in [1, 
12, 19, 13, 16, 21]. These approaches focus on improving ACET and investigate different criteria such as allocation 
schemes without fixed size memories, multiple heterogeneous memories in the memory hierarchy, an intersecting 
lifetime criterion for selecting arrays to allocate, reducing energy costs and so on. However, they are not concerned 
with real-time. 

There are several works that focus on real-time embedded systems. Suhendra et al. [17] present a static WCET- 
based data allocation scheme for scratchpads using static program analysis. The approach is to iteratively minimize 


3 



the worst-case execution path in a task. Puaut [14] presents a dynamic WCET-based scratchpad instruction allocation 
scheme and later. Deverge and Puaut [4] extend this allocation scheme to data as well. None of these memory alloca¬ 
tion schemes take into account timing requirements that may be specified by the user in the program and scheduling 
instructions and data to meet them. 


3 Timing Instructions & Constructs 


We begin by summarizing the timing instruction extensions for PRET called deadline instructions. Using these timing 
instructions, we define deadline blocks that are synthesized from our timing constructs used in C. 


Program P 


Phase 1 

Gen 

Subprc 

srate 

grams 



Pop, 

P P J 


Phase 2 

Synthesize 

Profiler 


- 

Timing 

"\ | i Commands 



Instructions 


1- 


1 tot,, 

t2.tl 

_y 



Phase 3 



Run on 
CSIM 



1 


Phase 4 


T 



Data sets (DST) 


Figure 2: Memory Allocation Usage Flow 


3.1 Deadline Instruction 

The deadline instruction introduced by Ip and Edwards [7] is defined as a pair (dr, , v ) where dr, is a deadline register 
and v is the value for the deadline (0 < * < 7). The semantics of the deadline instruction is summarized next. The 


4 










































deadline instruction sets the deadline value v to the register dr,; if dr, is zero and then decrements the value v every 
clock cycle. Note that the value in the deadline instruction represents a cycle timer. If however, dr, is non-zero, the 
deadline instruction blocks until dr, becomes zero, after which the new value v is loaded into the register. Additional 
implementation details of the PRET architecture are explained in [9]. 

3.2 Deadline Block 

We define a deadline block d as a pair of deadline instructions d = ((dr,,t>o), (dry, 0)). We call the first deadline 
instruction (dr,, t^o) the start and the second (dr,, 0) the end. Notice that the end deadline instruction has a deadline 
register value 0. The program code that can be executed between a start and end deadline instruction is said to be 
contained within the deadline block. The value Vq at the start of the deadline block specifies the timing requirement 
in cycles for the program code encapsulated. A timing violation of this requirement is when the code’s execution 
within the deadline block takes longer to execute than the specified value. Within a deadline block, we allow most 
constructs such as function calls, if-then, for and while. However, we disallow control jumps that jump outside 
the deadline block such as break, continue, return and goto. Note that a deadline block never finishes earlier 
than its specified value vq. This is a key part in achieving repeatable execution times from the architecture. The 
program’s execution stalls if the execution reaches the end of a deadline block before vq reaches 0 and it resumes once 
vq is 0. 


3.3 Timing Constructs 

We introduce two broad categories of timing constructs: sequential and loop. The sequential constructs (DEADSEQ ()) 
enforce a lower-bound on the execution time on a sequential segment of program code. On the other hand, the 
loop constructs (DEADFOR () , DEADWHILE () , . . .) enforce a periodic execution of the code within the scope. 

Figure 1 shows an example of the use of timing constructs and the result after the transformation. Note that the 
arguments after the deadline register values are unique labels used for profiling. Also note that we have specifically 
defined the synthesis of these instructions to match the appropriate semantics. Currently, we encode the deadline 
register in cycles as well, but at the same time we are investigating the specification of timing requirements in time 
and frequency units that are again automatically synthesized to cycles using a front-end parser. 


5 



4 Profiler and Memory Allocation Usage Flow 


Figure 2 presents a block-level diagram of the phases involved in profiling information about a program P. Program 
P is a C program with timing constructs. Note that these constructs are currently only supported by the PRET 
architecture. 

Phase 1 receives a program P as an input and produces multiple subprograms p t . The purpose of these subpro¬ 
grams is to exercise different paths in the program flow of the original program P. The subprograms can be either 
generated manually or via third-party test generation tools. Currently, we manually create the subprograms. 

Phase 2 takes these subprograms and synthesizes timing instructions from the timing constructs resulting in 
program tests /,. In addition, we automatically annotate the source with profiling labels. The profiler identifies the 
start and end of deadline blocks via these labels. A profiler command file c is generated during this phase. We 
use the open-source C/C++ front-end Clang [3] to implement the necessary rules to conduct the source-to-source 
transformations. We also automatically allocate deadline registers to the timing constructs. Figure 1 shows a simple 
example. By performing source-to-source transformations we do not have to change the implementation of the C 
standard implemented in GCC for these new timing constructs. 

The actual profiling is done in Phase 3. The program tests and the command file are input to the profiler CSIM. 
The output of the profiler are data sets DST for every test /,. The information these data sets contain are: 

• For every deadline block: Execution time, instructions encountered and their frequency, data words read and 
written to and their frequency. 

• Other blocks: instructions encountered and their frequency, data words read and written to and their frequency, 
total execution time of test. 

The final Phase 4 is where the memory allocation is done. The first part of the memory allocation is a feasibility 
check that ensures that a memory allocation that results in meeting the timing requirements does exist for some 
scratchpad size. If one does exist, then a greedy algorithm is used to allocate instructions and data words to the 
scratchpads. Note that there is a back arrow to Phase 3 because after an allocation is performed, the profiling is 
rerun. This process continues until Phase 4 yields an allocation that meets the specified timing requirements or fails 
due to scratchpad size. 


6 



5 Static Memory Allocation 


We pictorially represent our allocation algorithm in Figure 3 and describe the stages briefly. Note that Table 1 is a list 
of variables we employ throughout the remainder of the paper. 


T 

t G T 

INST(t) 

DATA(t) 

m 

d G D(t) 
X(d) 

X 

Y(d) 

Y 

SIZE{m ) 
A mm (d) 
N(x) 

N(x, d) 
Nr(y) 
N r (y, d) 
N w (y) 
N w (y, d) 

A 

-^mm 

ALLOC 
P REi nst 
PREdata 


set of tests, 
one test in T. 

set of instructions encountered in test t. 
set of data words written to/read from in test t. 
set of deadline blocks in test t. 
a deadline block. 

set of instructions executed in deadline block d. 

set of instructions executed in any deadline block. 

set of non-global data words read from or written to in deadline block d. 

set of data read or written in any deadline block. 

size of memory unit to. 

cycles missed for deadline block d when in main memory. 

frequency of instruction x encountered. 

frequency of instruction x encountered in deadline block d. 

frequency of reading data word y. 

frequency of reading data word y in deadline block d. 

frequency of writing data word y. 

frequency of writing data word y in deadline block d. 

accessing time to main memory via the memory wheel. 

number of words to allocate to scratchpad at each iteration. 

set of previously scheduled instructions. 

set of previously scheduled data words. 


Table 1: Variable Definitions 


5.1 Feasibility Check 

Before performing any allocation, we first perform a feasibility check. We say that a program P is infeasible if it 
cannot meet all timing requirements for every program test In order to check for infeasibility, we assume that all 
instructions and data are in a scratchpad, and run each of the tests. The feasibility check is successful if none of the 
timing requirements are violated. Otherwise, the program is infeasible and the user must alter the timing requirements 
in the original program. 

5.2 Memory Allocation 

The memory allocation in Phase 4 is refined in Figure 3. This step is reached only after the feasibility check passes. 
We begin by identifying deadline blocks that violate its timing requirements. If there are violations but the existing 


7 



A tl 

I +1 Z r , r 


to 

4 

exo 

To 

Wo 


d, 

ex, 

r, 

w, 


4 

exz 

n 

W; 


4 

ex, 

n 

w, 


n 


Timing Req. 
Violated? 


v: 


YES 



Allocation of code/data 

f NO * 

on other metric 


SPM Space 
Exhausted? 


^YES 

-If 

Done j 


SPM Space 

NO 

Static 


P.ispm 

Exhausted? 

— 


Allocation 


P.dspm 




/ 

\ YES 

Infeasible ' 

✓ 

✓ 

✓ 


for size 


Phase 3 


Figure 3: Iterative Static Allocation Flow 


allocation has occupied the maximum size of the scratchpad memories, then for the given scratchpad memory size it 
is not possible to meet the timing requirements. On the other hand, if there is space available on the scratchpads, we 
perform instruction and data allocation and merge the existing allocation with the generated allocation. Note that once 
an allocation is generated, those instructions and data words are not reconsidered for allocation during the following 
iterations. If there are no deadline blocks that missed their deadlines, then we have successfully found an allocation 
that meets all timing requirements. In this case, we can use any remaining space on the scratchpad as we see fit, 
including for scheduling instructions and data encountered outside of the deadline blocks. In our implementation, we 
allocate on the basis of the frequency of main memory accesses in the whole program, but any metric could be used. 


5.3 Previously Scheduled Instructions and Data 

The sets PREi nst and PRE data hold all the instructions and data allocated during the previous iterations. We allocate 
these instructions and data words at the beginning of each iteration and remove them from the set of instructions and 
data considered for memory allocation. 

5.4 Identify Violating Deadline Blocks 

We construct a set of deadline blocks V that violate their timing requirements. However, it is possible for a deadline 
block to violate its timing requirements in more than one test. It may also be the case that the program paths taken in 
the tests are different from each other; thus, different instructions and data words are encountered during profiling. As 
a result, we use a simple heuristic to collate the instructions, data words and execution times of these deadline blocks. 
For all tests where the deadline block missed its deadline, we select the maximum execution time of the deadline block 


8 



































and the union of both instructions and data. These updated deadline blocks are added in set V and their corresponding 
information is merged as described above. Therefore, any indexing based on deadline blocks in V yields the merged 
information. 


5.5 Exhausting Scratchpad Space 

At every iteration we check if the remaining space in the scratchpad allows for scheduling ALLOC addition words 
of instructions or data. In the event that the remaining space is less than ALLOC, we alter the variable size to that of 
the remaining space. If however, there is no space left at all on the scratchpad and timing requirements are violated, 
the allocation fails. Otherwise, if there are no timing violations, the allocation terminates successfully as shown in 
Figure 3. 


5.6 Instruction 

An access to the main memory goes through the memory wheel [9] that can take at worst A mm cycles to complete. 
For a deadline block d, the memory access cost for an instruction x £ X(d)\PRL lns t is no more than the frequency 
of that instruction N(x, d) multiplied by the main memory access time A mm . 

We formulate a binary integer linear programming problem for scheduling both instructions and data encountered 
in deadline blocks. We will start by defining the variables that correspond to instructions that could be added to the 
scratchpads. 

! 1 allocate instruction x to scratchpad. 

0 otherwise. 

The instruction savings IS(V ) is given below. Instructions that contribute most to the instruction access time from 
main memory are more heavily weighted than those that do not. 


is(v) = E E linst (x)N(x, d)A mm 

d&V xeX{d)\PRE inBt 


5.7 Data 


Once again, we are interested in scheduling data access that contribute most to the cost in memory access time. Due 
to the simplicity of the PRET architecture, accesses for data to main memory also go through the memory wheel with 
worst-case access time of A mm . For a deadline block d, a data word y £ Y(d)\PRE l i ata is written to N w (y,d) 
and read from N r (y, d) number of times. Therefore, the memory access cost for y is the memory access time A mm 


9 



multiplied by the sum of the number of times y was read from and written to in deadline block d. 


{ 1 allocate data word y to scratchpad 
0 otherwise. 

The savings from allocating data words DS(V) is highest when data words that contribute most to the main 
memory access cost are allocated. 

DS(V) = E E I data (y) A r nm j^-^r (y ■ 6?) 

deV v eY(d)\PRE data 

+ N w (y,d) 

5.8 Allocation 

Our binary integer linear program shown below maximizes the savings occupied by either instruction or data S(v). 
Thus, the objective function that we maximize is the sum of the savings from each of the instruction and data alloca¬ 
tions: 

Maximize: 

S(V) = IS(V) + DS(V) 

The constraint equations must make sure that we do not allocate more words than we are allowed in any given 
iteration. Remember that we keep history of the previously allocated instructions and merge the new ones with the 
previous ones. 

Subject to: 

Iinst{x) + Idat.a{y) $ ALLOC 

xeX\PRE inst yeY\PRE data 

Solving this binary integer linear program gives us the allocation of data and instructions to the scratchpads in a 
single iteration of our allocation scheme. These iterations continue as shown in Figure 3 modifying PREi nst and 
PREdata to reflect the contents added to the scratchpad in each iteration. 


10 



5.9 Minimize Frequency of Main Memory Accesses 

After all deadline blocks meet the timing requirements, one final iteration takes place to schedule the remaining 
scratchpad space. We decide to minimize the frequency of main memory accesses in the entire program, but other 
optimizations for low power and energy are also possible. We define INST and DATA as the remaining set of 
instructions and data words, respectively. 


INST = |J IN ST(t)\PRE inst 

teT 

DATA = |J DATA(t) \PRE data 

teT 


We redefine the binary variables for the instruction and data. 


t'instix) 


! 1 allocate instruction x to scratchpad in final phase. 
0 otherwise. 


{ 1 allocate data word y to scratchpad in final phase. 

0 otherwise. 

We select the instructions and data words that cost the most in terms of frequency of main memory accesses by 
solving another binary integer linear program. In this case, our objective function is S'(IN ST, DATA). 

Maximize: 


S' (INST, DATA) = E I inst( x ) N ( x )+ 

xGINST 


yGDATA 


fdataiv) N r (y) + N w (y) 


During this iteration, we want to allocate all of the remaining space on the scratchpad, which we describe in the 
constraint below. 


11 



Subject to: 


E 4rt(y) < SIZE(spm) - SIZE{PRE inst ) 

xelNST 

E ^ataiy) < SIZE(spm) - SIZE(PRE data ) 

y^DATA 


6 Experiments 

Our infrastructure uses Clang [3] to perform the source-to-source transformations from timing constructs in C to 
timing instructions compilable by SPARC’s GCC toolchain. We use Python for the profiling and integration with 
PRET’s cycle-accurate simulator and lp solve as the solver for selecting the instructions and data words that offer 
the most savings in terms of main memory access time. We select a few C benchmarks from the Malardalen [6] suite 
and augment them with timing requirements. 



Figure 4: Execution Times with Varying Scratchpad Size 


In Figure 4, we vary the scratchpad size (both instruction and data) as a percentage of the program instructions 
and data, and present the execution times of the benchmarks. At 0% only the main memory is used. We also denote 
whether allocation failed at each of the scratchpad sizes; a circle denotes failure and a triangle denotes success. As 
shown, the selected benchmarks eventually met their timing requirements. In addition to the benchmarks shown in 
Figure 4, we plan to port segments of real-time programs from PapaBench [11], 


12 











7 Conclusion 


The PRET architecture makes the execution time of programs repeatable through its timing instruction extensions. It 
also allows the programmers to specify their timing requirements within their programs, unlike traditional embedded 
programming languages like C. In this work, we leverage these timing instructions and provide high-level timing con¬ 
structs to specify timing requirements in the program. Afterward, we present a static scratchpad memory allocation 
scheme that focuses on meeting these timing requirements in the program. This effort removes the assumption made 
by Lickly et al. [9] of the program and data having to fit entirely on the scratchpads. In our approach, a programmer 
specifies these timing requirements in the program using timing constructs. We automatically synthesize timing in¬ 
structions from these timing constructs and additional profiling annotations. We iteratively schedule instructions and 
data words to the scratchpads for blocks that specify the programmer’s timing requirements such that the least amount 
of scratchpad space is used to meet the timing requirements. If and once they are satisfied, any remaining space can 
be used to optimize based on another metric, such as reducing power or energy consumption. 

References 

[1] Oren Avissar, Rajeev Barua, and Dave Stewart, An optimal memory allocation scheme for scratch-pad-based 
embedded systems, ACM Transactions on Embedded Computing Systems 1 (2002), no. 1, 6-26. 

[2] Rajeshwari Banakar, Stefan Steinke, Bo-Sik Lee, M. Balakrishnan, and Peter Marwedel, Scratchpad memory: 
design alternative for cache on-chip memory in embedded systems, CODES ’02: Proceedings of the tenth inter¬ 
national symposium on Hardware/software codesign (New York, NY, USA), ACM, 2002, pp. 73-78. 

[3] clang Community, clang: a C language family front-end for LLVM, Website: http://www.clang.llvm.org. 

[4] Jean-Francois Deverge and Isabelle Puaut, WCET-Directed Dynamic Scratchpad Memory Allocation of Data, 
Proceedings of the 19th Euromicro Conference on Real-Time Systems (2007), 179-190. 

[5] Stephen A. Edwards and Edward A. Lee, The case for the precision timed (PRET) machine, June 2007, pp. 264- 
265. 

[6] Jan Gustafsson, The worst-case execution time tool challenge 2006, Proc. 2 ndlnternational Symposium on 
Leveraging Applications of Formal Methods (06), November (2006). 


13 



[7] Nicholas Jun Hao Ip and Stephen A. Edwards, A processor extension for cycle-accurate real-time software , Pro¬ 
ceedings of the IFIP International Conference on Embedded and Ubiquitous Computing (EUC) (Seoul, Korea), 
vol. 4096, August 2006, pp. 449-458. 

[8] Insup Lee, Susan Davidson, and Victor Wolfe, Motivating time as a first class entity, Technical Report MS- 
CIS-87-54, Dept, of Comp, and Infor. Science, Univ. of Penn, Aug. (Revised Oct.) 1987, Taxonomy of timing 
properties that must be expressible. 

[9] Ben Lickly, Isaac Liu, Sungjun Kim, Hiren D. Patel, Stephen A. Edwards, and Edward A. Lee, Predictable pro¬ 
gramming on a precision timed architecture , Tech. Report UCB/EECS-2008-40, EECS Department, University 
of California, Berkeley, Apr 2008, This paper has been accepted for publication at CASES 2008. 

[10] Peter Marwedel, Lars Wehmeyer, Manish Verma, Stefan Steinke, and Urs Helmig, Fast, predictable and low 
energy memory references through architecture-aware compilation, ASP-DAC ’04: Proceedings of the 2004 
conference on Asia South Pacific design automation (Piscataway, NJ, USA), IEEE Press, 2004. 

[11] Fadia Nemer, Hugues Casse, Pascal Sainrat, Jean-Paul Bahsoun, and Marianne de Mischiel, Papabench: A Free 
Real-time Benchmark, 6th Inti. Workshop on Worst-Case Execution Time (WCET) Analysis, Dresden, Germany 
(2006). 

[12] Nghi Nguyen, Angel Dominguez, and Rajeev Barua, Memory allocation for embedded systems with a compile- 
time-unknown scratch-pad size. Proceedings of the 2005 international conference on Compilers, architectures 
and synthesis for embedded systems (2005), 115-125. 

[13] Preeti Ranjan Panda, Nikil D. Dutt, and Alexandra Nicolau, Efficient utilization of scratch-pad memory in em¬ 
bedded processor applications, European Design and Test Conference, 1997. ED&TC 97. Proceedings (17-20 
Mar 1997), 7-11. 

[14] Isabelle Puaut, WCET-centric software-controlled instruction caches for hard real-time systems. Proceedings of 
the 18th Euromicro Conference on Real-Time Systems (2006), 217-226. 

[15] Peter Puschner, Is worst-case execution-time analysis a non-problem? - towards new software and hardware 
architectures, Proc. 2nd Euromicro International Workshop on WCET Analysis (York YO10 5DD, United King¬ 
dom), Technical Report, Department of Computer Science, University of York, lun. 2002. 


14 



[16] S. Steinke, L. Wehmeyer, Bo-Sik Lee, and P. Marwedel, Assigning program and data objects to scratchpad for 
energy reduction. Design, Automation and Test in Europe Conference and Exhibition, 2002. Proceedings (2002), 
409^115. 

[17] Vivy Suhendra, Tulika Mitra, Abhik Roychoudhury, and Ting Chen, WCET centric data allocation to scratchpad 
memory , 05: Proceedings of the 26th IEEE International Real-Time Systems Symposium (2005). 

[18] Lothar Thiele and Reinhard Wilhelm, Design for timing predictability, Real-Time Syst. 28 (2004), no. 2-3, 157— 
177. 

[19] S. Udayakumaran and R. Barua, An integrated scratch-pad allocator for affine and non-affine code. Proceedings 
of the conference on Design, automation and test in Europe: Proceedings (2006), 925-930. 

[20] Lars Wehmeyer and Peter Marwedel, Influence ofonchip Scratchpad Memories on WCET Prediction, Proceed¬ 
ings of the 4th International Workshop on Worst-Case Execution Time (WCET) Analysis (2004). 

[21] Liping Xue, Mahmut Kandemir, Guangyu Chen, and Taylan Yemliha, Spm conscious loop scheduling for em¬ 
bedded chip multiprocessors, (2006), 391-400. 


15 



