AbAJors/s 

UNITED STATES AIR FORCE 
SUMMER RESEARCH PROGRAM - 1993 
SUMMER RESEARCH-PROGRAM FINAL REPORTS 

VOLUME 15 

WRIGHT LABORATORY 


RESEARCH & DEVELOPMENT LABORATORIES 
5800 Uplander Way 
Culver City, CA 90230-6608 


Program Director, RDL Program Manager, AFOSR 

Gary Moore Col. Hal Rhoades 


Program Manager, RDL Program Administrator, RDL 

Scott Licoscos Gwendolyn Smith 

Program Administrator, RDL 
Johnetta Thompson 


Submitted to: 


AIR FORCE OFFICE OF SCIENTIFIC RESEARCH 
Bolling Air Force Base 
Washington, D.C. 

December 1993 


94-12289 

I1IIHHI 


X.TIC C-u'AUTY «JG?ECTED 9 

94 4 21 


073 




Best 

Available 

Copy 







MEMORANDUM FOR DTIC (Acquisition) 

(Attn: Pat Mauby) 


SUBJECT: Distribution of USAF (AFOSR Summer Research Program (Air Force 
Laboratories) and Universal Energy Systems, Inc., and the Research Initiation Program 

FROM: AFOSR/XPT 
Joan M. Boggs 

110 Duncan Avenue, Suite BUS 
Bolling AFB DC 20332-0001 

1. All of the books forwarded to DTIC on the subjects above should be considered 
Approved for Public Release, distribution is unlimited (Distribution Statement A). 

2. Thank you for processing the attached information. 



Chief, Technical Information Division 



l/e/ 13, fy /&p C i/'/m 3, /y-mt 


&5£P Cl/ol JO t //>/<?<?,£ 
Hdfl/3('i/ 0 / 


/ P P/oo Pd 







Master Index For High School Apprentices 


Acktroann, Laura 
7801 Nilahire NX 
La Cueva High School 
Albuquerque, NM 87122-0000 

Alexanderson, Sarah 
7173 IM 1828 

laat Cantral High School 
San Antonio, TX 78263-0000 

Antonaon, Staphan 
800 Cypraaa St. 

Roma Catholic High School 
Roma, NT 13440-0000 

Arnold, Katharina 
1400 Jaekaon-Kallar 
Robart E. Laa High School 
San Antonio, TX 78213-0000 

Baita, Mark 
248 North Main Straat 
Cadarvilla High School 
Cadarvilla, OH 45314-0000 

Baker, Eugenia 
501 Moaely Dr. 

A. Crawford Mosley High School 
Lynn Haven, FL 32444-0000 

Bakert, Jonathan 
Oneida St. 

Sauquoit Valley Cantral High s 
Sauquoit, NT 13456-0000 

Banaasak, Brian 
9830 W. National Rd. 

Tecumaeh High School 

New Carliala, OB 45344-0000 

Barber, Jaaon 
1000 10th St. 

Floreaville High School 
Floreaville, TX 78114-0000 

Bautiata, Jennifer 


Laboratory: FL/LZ 
Vol-Faga No: 13- 5 


Laboratory: AL/HR 
Vol-Paga No: 12-25 


Laboratory: RL/XR 
Vol-Paga No: 14-12 


Laboratory: AL/OE 
Vol-Paga No: 12-30 


Laboratory: NL/FX 
Vol-Paga No: 15-11 


Laboratory: AL/EQ 
Vol-Paga No: 12-19 


Laboratory: RL/ER 
Vol-Paga No: 14- 7 


Laboratory: WL/PO 
Vol-Paga No: 15-44 


Laboratory: AL/CF 
Vol-Paga No: 12- 8 


Accesion for \ 

NTIS 

CRA&I 


OTIC 

TAB 

□ 

Unarm 

ounced 

□ 

j Justification 

.. ^ 

By_ 

mi 


Distribution / 

Availability Codes 

Dist 

Avail and/or 

Spe 

cial 

/H 




Laboratory: HL/MN 
Vol-Paga No: 15-26 




HSAP Participant Data 


Beha, Jeaaica 

3301 Shroyer Rd. 

Kettering Fairmont High School 
Kettering, 08 45429-0000 

Berty, Sara 
4524 Linden Ave. 

Carroll High School 
Dayton, 08 45432-0000 

Blanchard, William 


Laboratory: WL/ML 
Vol-Pago No: 15-21 


Laboratory: AL/OE 
Vol-Paga No: 12-31 


Laboratory: WL/MN 
Vol-Pago No: 15-27 


0 


Bond, Ryan 
North Jackeon St. 

Tullahoua High School 
Tullahoma, TN 37388-0000 

Bowlby, Andraa 
ttadgo Way 

Badford High School 
Bedford, MA 1730-0000 

Broeht, Juon 

5400 Chaaboraburg Road 

Wayne High Achool 

Bober Heighta, 08 45424-0000 

Brown, David 
12200 Lomaa Blvd. NB 
Manzano High School 
Albuquerque, MM 87112-0000 

Cabral, Aaron 
800 Odelia NB 
Albuquerque High School 
Albuquerque, NM 87102-0000 

Camero, Xlaa 
2515 Navajo St. 

South San Antonio High School 
San Antonio, TX 78224-0000 

Campanile, Nieholaa 
2880 Dayton-Xenia Rd. 
Beavercreek High School 
Beavercreek, OH 45434-0000 


Laboratory: AEDC/ 
Vol-Page No: 16- 1 


Laboratory: PL/OP 
Vol-Page No: 13- 1 


Laboratory: WL/FX 
Vol-Page No: 15-12 


Laboratory: PL/WS 
Vol-Page No: 13-19 


Laboratory: PL/SX 
Vol-Page No: 13-13 


Laboratory: AL/A0 
Vol-Page No: 12- 2 


Laboratory: WL/EL 
Vol-Page No: 15- 7 


n 




HSAP Participant Data 


Carranza, Jaaon 
SOS S. Ludlow St. 

Chaminade-Julienne High School 
Dayton, OH 45402-0000 

Carroll, Shawn 

1400 Jackson Kallar St. 

Robert K. 1st High School 
San Antonio, TX 78213-0000 

Casaros, Carman 

1215 H. St. Mary's 
Providence High School 
San Antonio, TX 78215-0000 

Cayton, Sabrina 
5005 Stahl Rd. 

Jamas Madison High School 
San Antonio, TX 78247-0000 

Chuang, Eloanoro 
2680 Dayton-Xenia Rd. 
Beavercreek High School 
Beavercreek, OH 45434-0000 

Ciooperlik, Kara 
7173 FM 1628 
East Central High School 
San Antonio, IX 78263-0000 

Cook, Theresa 


Laboratory: WL/AA 
Vol-Page Ho: 15- 1 


Laboratory: AL/C7 
Vol-Page Ho: 12- 9 


Laboratory: AL/AO 
Vol-Page Ho: 12- 3 


Laboratory: AL/AO 
Vol-Page Ho: 12- 4 


Laboratory: AL/CF 
Vol-Page Ho: 12-10 


Laboratory: AL/OE 
Vol-Page Ho: 12-32 


Laboratory: WL/MH 
Vol-Page Ho: 15-28 


Cosgrove, Kathlyn 
727 E. Hildebrand 
Incarnate Word High School 
San Antonio, XX 78284-0000 

Dailey, Kevin 
2660 Dayton-Xenia Rd. 
Beavercreek High School 
Beavercreek, OH 45434-0000 

Danelo, David 
25 Burwood St. 

San Antonio Christian School 
San Antonio, TX 78216-0000 


Laboratory: AL/CF 
Vol-Page Ho: 12- 5 


Laboratory: WL/AA 
Vol-Page Ho: 15- 2 


Laboratory: AL/HR 
Vol-Page Ho: 12-26 


m 





HSAP Participant DaU 


Davis, Jimi Laboratory: AL/1 Q 

1000 School Ava. Vol-Paga No: 12-20 

Bathartord High School 
Panama City, FL 32404-0000 

OoBrosso, Nick 
3301 Shroyer Rd. 

Kattaring Fairmont High School 
Kattaring, OH 45429-0000 

Dackar, Miehaal 
2601 Oneida St. 

Sauquoit Vallay Cantral School 
Sauquoit, NY 13456-0000 

Daiblar, Nancy Laboratory: WL/MN 

Vol-Paga No: 15-29 


Laboratory: WL/FO 
Vol-Paga No- 15-45 


Laboratory: RL/ZR 
Vol-Paga No: 14- 8 


Dodsworth, Christopher 
4916 National Rd. 

Northmont High School 
Clayton. OH 45315-0000 

Dominguax, Janetta 
114 E. Gerald Ava. 

Harlandale High School 
San Antonio, TX 78214-0000 

Kllana, Brandon 
711 Anita Or. 

Tahachapi High School 
Tehaehapi, CA 93561-0000 

Ethridge, Blake 
7801 Wilshire Blvd. 

La Cuava High School 
Albuquerque, NM 87122-0000 

Faldaraan, Janes 
N. Jackson St. 

Tullahoma High School 
Tullahoma, TN 37388-0000 

Feucht, Danny 
5833 Student St. 

West Carrollton High School 
Hast Carrollton, OH 45418-0000 


Laboratory: WL/ZL 
Vol-Paga No: 15- 8 


Laboratory: AL/HR 
Vol-Paga No: 12-27 


Laboratory: PL/RX 
Vol-Paga No: 13- 9 


Laboratory: PL/LX 
Vol-Paga No: 13- 6 


Laboratory: AEDC/ 
Vol-Paga No: 16- 2 


Laboratory: NL/FX 
Vol-Paga No: 15-13 


IV 



HSAP Participant Data 


Finoh, David 
SOI Niagara Avt. 

Colonel White High chool 
Dayton, 08 45405-0000 

Focht, Jeremy 
2640 Dayton-Xonia Rd. 
Beavercreek High School 
Boavororook, 08 45434-0000 

foloy, Jennifer 
2660 Dayton-Xonia Rd. 
Boavororook High School 
Beavercreek, 08 45434-0000 

foth, Angela 
501 Moaley Dr. 

A. Crawford Moaley High School 
Lynn Haven, ft 32444-0000 

Fowler, Brandon 
Chenango Ave. 

Clinton Senior High School 
Clinton, NT 13323-0000 

Oarcia, Stephanie 
650 Ingram 

Oliver Wendell Holmes 
San Antonio, TX 78238-0000 

Oarcia, Alejandro 
2515 Navajo St. 

South San Antonio High School 
San Antonio, TX 78224-0000 

Oarcia, Andrea 
6701 Fortuna Rd. NW 
West Mass High School 
Albuquerque, CM 87121-0000 

Oavornik, Jeffrey 
5110 Walzem Rd. 

Roosevelt High School 
San Antonio, TX 78239-0000 

Oilea, Mark 
1204 Harrison Ave. 

Bay High School 

Panama City, FL 32401-0000 


Laboratory: AL/OX 
Vol-Page No: 12-33 


Laboratory: WL/ML 
Vol-Page No: 15-22 


Laboratory: WL/XL 
Vol-Page No: 15- 9 


Laboratory: AL/XQ 
Vol-Page No: 12-21 


Laboratory: RL/C3 
Vol-Page No: 14- 2 


Laboratory: AL/AO 
Vol-Page No: 12- 6 


Laboratory: AL/CF 
Vol-Page No: 12-11 


Laboratory: PL/SX 
Vol-Page No: 13-14 


Laboratory: AL/CF 
Vol-Page No: 12-12 


Laboratory: AL/XQ 
Vol-Page No: 12-22 


V 




HSAP Participant Data 


Ginger, David 
500 E. Franklin SC. 
Centerville High School 
Centerville, OH 45459-0000 

Oonsales, Christophor 
1400 Jackaon-Keller 
Robert K. Lac High School 
San Antonio, IX 78234-0000 

Ooodan, Christie 


Laboratory: WL/ML 
Vol-Page Ho: 15-23 


Laboratory: AL/OK 
Vol-Page Ho: 12-34 


Laboratory: WL/MH 
Vol-Page Ho: 15-30 


Orabovski, Holly 
Shawsheen Rd. 

Andover High School 
Andover, MA 1810-0000 

Oorecki, David 
800 Cypress St. 

Rons Catholic High School 
Rom, HY 13440-0000 

Hanna, Melissa 

1312 Utica St. 

Criskany Central High School 
Oriskany, HY 13424-0000 

Harrison, Deanna 


Laboratory: RL/ER 
Vol-Page Ho: 14- 9 


Laboratory: RL/C3 
Vol-Page Ho: 14- 1 


Laboratory: RL/IR 
Vol-Page Ho: 14-13 


Laboratory: WL/MH 
Vol-Page Ho: 15-31 


Hartsock, David 
3491 Upper Bellbrook Rd. 
Bellbrook High School 
Bellbrook, OH 45305-0000 

Haydnk, Eric 
800 Cypress St. 

Rom Catholic High School 
Rom, HY 13440-0000 

Besewr, Laura 


Laboratory: WL/PO 
Vol-Page Ho: 15-46 


Laboratory: RL/OC 
Vol-Page Ho: 14-16 


Laboratory: WL/MH 
Vol-Page Ho: 15-32 


» 


0 


VI 





HSAP Participant Data 


■ill, Thuan 
North Jackson St. 

TullahoM High School 
Tullahoaa, TN 37388-0000 

Hodges, Malania 
5833 Studant St. 

Mast Carrollton High School 
Hast Carrollton, OB 45418-0000 

Jeffcoat, Mark 


I 


0 


Jost, Tiffany 
Lincoln Rd. 

Lincoln-Sudbury Regional High 
Sudbury, MR 1776-0000 

Kitty, Alexandra 

3900 W. Peterson 

Our Lady of Good Counsel High 

Chicago, XL 60659-3199 

Koslowaki, Peter 
500 K. Franklin St. 
Centerville High School 
Centerville, OH 45459-0000 

Kress, Barry 


Kulesa, Joel 
940 David Rd. 

Archbishop Alter High School 
Kettering, OH 45429-0000 

Lonaand, Bradley 
PO Drawer CC 
Rosamond High School 
Rosamond, CA 93560-0000 

Maloof, Adam 
251 Waltham St. 

Lexington High School 
Lexington, MA 2173-0000 


Laboratory: AEDC/ 
Vol-Page Mo: 16- 3 


Laboratory: WL/PO 
Vol-Page Mo: 15-47 


Laboratory: WL/MI 
Vol-Page Mo: 15-33 


Laboratory: PL/OP 
Vol-Page Mo: 13- 2 


Laboratory: PL/RX 
Vol-Page Ho: 13-10 


Laboratory: WL/ML 
Vol-Page Mo: 15-24 


Laboratory: WL/MM 
Vol-Page Mo: 15-34 


Laboratory: WL/KL 
Vol-Page Mo: 15-10 


Laboratory: PL/RX 
Vol-Page Mo: 13-11 


Laboratory: RL/ER 
Vol-Page No: 14-10 





HSAP Participant Data 


Maxlow, Chris 
925 Dinah Shorn Alia, 
franklin County nigh School 
Winchester, W 37398-0000 

Martin, Any 
3301 Shroyor Hd. 

Kottoring fiirunt High School 
Kattaring, OB 45429-0000 

Matthews, Susanna 
5323 Mont g oamry MB 
Dal Norte High School 
Albuquerque, *H 87109-0000 

MoBuan, Brie 
800 Odalia Rd. MB 
Albuquerque High School 
Albuquerque, W 87102-0000 

MeOovern, Scott 
3491 0)pper Bellbrook Rd. 
Ballbroek High School 
Bellbrook, OH 45305-0000 

McPherson, Sandra 
Je££erson 4 drove St. 

Bishop Broasart High School 
Alexandria, KX 41001-0000 

Manga, Sean 
Route 294 

Adirondack High School 
Boonnville, MX 13309-0000 

Merrill, Benjamin 
3491 Dppar Bellbrook Hd. 
Bellbrook High School 
Bellbrook, OH 45305-0000 

Middleton, Charles 
4524 linden Ave. 

Carroll High School 
Dayton, OH 45432-0000 

Mikaeh, Virginia 
727 B. Hildebrand 
Incarnate Word High School 
San Antonio, tX 78284-0000 


Laboratory: ABDC/ 
Vol-Page Mo: 18- 4 


Laboratory: WL/PX 
Vol-Page Mo: 15-15 


Laboratory: PL/SZ 
Vol-Page Mo: 13-15 


Laboratory: PL/VT 

Vol-Page Mo: 13-17 I 


Laboratory: WL/AA 
Vol-Page No: 15- 3 


Laboratory: WL/ML 
Vol-Page Mo: 15-25 


Laboratory: RL/C3 
Vol-Page Mo: 14- 3 


Laboratory: WL/FI 
Vol-Page Mo: 15-16 


Laboratory: WL/PX 
Vol-Page No: 15-17 


Laboratory: AL/CP 
Vol-Page Mo: 12-13 


vra 




HSAP Participant Data 


Moon II, Elliot 


0 


Mortis, Rebecca 
727 E. Hildebrand 
Incsrnsto Word High School 
Saa Antonio, HX 79284-0000 

Morton, Oilbort 
2001 McArthur Dr. 

Coffee County Control High Sch 
Manchester, HI 37355-0000 

Hoitsol, Laura 

H. St. Mary's 

Providonoo High School 

Saa Antonio, IX 78215-0000 

Nguyen, Quynhtrang 
5933 Student St. 

West Carrollton High School 
Mast Carrollton, OH 45418-0000 

Miolsea, Eric 
500 Turin Rd. 


Room, NT 13440-0000 

Northeutt, Chris 
925 Dinah Shore Blvd. 
Franklin County High School 
Winchester, TN 37398-0000 

Olson, Aeanda 
1000 School Ave. 

Rutherford High School 
Panama City, FL 32404-0000 

Ondrusek, Kimberly 
7173 IN 1829 

East Central High School 
Saa Antonio, TX 78263-0000 

Ortis, Benjamin 
8701 Fortune Rd. NW 
West Mesa High School 
Albuquerque, Mt 87105-0000 


laboratory: WL/MN 
Vol-Page No: 15-35 


laboratory: AL/HR 
Vol-Page No: 12-28 


Laboratory: AEDC/ 
Vol-Page No: 16- 5 


Laboratory: AL/OE 
Vol-Page No: 12-35 


Laboratory: AL/CF 
Vol-Page No: 12-14 


Laboratory: RL/C3 
Vol-Page No: 14- 4 


Laboratory: AEDC/ 
Vol-Page No: 16- 6 


Laboratory: AL/EQ 
Vol-Page No: 12-23 


Laboratory: AL/HR 
Vol-Page No: 12-29 


Laboratory: PL/LI 
Vol-Page No: 13- 7 


IX 




HSAP Participant Data 


Page, Meliaaa 
SOI Moalay Dr. 

A. Crawford Moalay 

Lynn Haven, FL 32444-5609 

Fanara, Michaal 
500 Turin St. 

Roma Fraa Academy 
Roma, NT 13440-0000 

Fann, Alexander 


Laboratory: WL/FI 
Vol-Paga No: 15-18 


Laboratory: RL/C3 
Vol-Paga No: 14- 5 


Laboratory: WL/MN 
Vol-Paga No: 15-36 


Parry, Kyle 
Craatviaw High School 

, - o 

Platchar, Mary 


Laboratory: WL/MN 
Vol-Paga No: 15-37 


Laboratory: WL/MN 
Vol-Paga No: 15-38 


Platl, Anna 
Burratona Rd. 

Notra Damn 

Utica, NT 13502-0000 

Pravoat, Danial 
3301 Shroyar Rd. 

Kottaring Fairmont High School 
Rattaring, OH 45429-0000 

Prica, Kriaty 
North Jackson St. 

Tullahoaa High School 
Tullahooa, TN 37388-0000 

Protx, chriatophar 
501 Moalay Dr. 

A. Crawford Moalay High School 
Lynn Havan, FL 32444-5609 

Radar, Thnaaa 
1505 Candelaria NW 
Vallay High School 
Albuquerque, MM 87107-0000 


laboratory: RL/C3 
Vol-Paga No: 14- 6 


Laboratory: WL/PO 
Vol-Paga No: 15-48 


Laboratory: AZDC/ 
Vol-Paga No: 16- 7 


Laboratory: AL/EQ 
Vol-Paga No: 12-24 


Laboratory: PL/WS 
Vol-Paga No: 13-20 


i 


X 




HSAP Participant Data 


Ray, Kxiatophar 
401 lagl* Blvd. 

Shelbyville Central High Schoo 
Shelbyville, TN 37160-0000 

H —d , Tracy 
711 Anita Or. 

Ta h ac h a p i High School 
Tahachapl, CA 93561-0000 

Riddle, Chary1 
Highway 55 

Moora County High School 
Lynchburg, TO 37352-0000 

Rodriguas, Luia 
5400 Chaabaraburg Rd. 

Wayna High School 

Bubar Haights, OH 45424-0000 

Roaanbaum, David 


, - 0 

Sali n as, Carol 
727 X. Hildabrand 
Incanata Word High School 
San Antonio, TO 78212-0000 

Schanding, Sarah 
7173 IM 1628 

Xast Central High School 
San Antonio, IX 78162-0000 

Schats, William 
500 Turin St. 

Rom Fraa Academy 
Room, NT 13440-0000 

Sc h i n dler, David 
Drawer 1300 
Los Lunas High School 
Los Lunas, NM 87031-0000 

Sanus, Joe 
500 Turin St. 

R om a Free Academy 
Rems, MX 13440-0000 


Laboratory: AEDC/ 
Vol-Paga No: 16- 8 


Laboratory: PL/RX 
Vol-Paga No: 13-12 


Laboratory: AEDC/ 
Vol-Paga No: 16- 9 


Laboratory: AL/CF 
Vol-Paga No: 12-1S 


Laboratory: WL/MN 
Vol-Paga No: 15-39 


Laboratory: AL/CF 
Vol-Paga No: 12-16 


Laboratory: AL/CF 
Vol-Paga No: 12-17 


Laboratory: RL/IR 
Vol-Paga No: 14-14 


Laboratory: PL/LI 
Vol-Paga No: 13- 8 


Laboratory: RL/XR 
Vol-Paga No: 14-15 


XI 






HSAP Participant Data 


Servaitea, Jonathan 
500 Z. Franklin St. 
Cantarvilla High School 
Centerville, OB 45459-0000 

Shao, Min 

•S9 Maaaaohuaatta Am. 
Arlington High School 
Arlington, MA 2174-0000 

Simon, Ryan 
701 E. Boa* Rd. 

Springfield North High School 
Springfield, OB 45503-0000 

Smith, Adam 

Phillips Academy 
Andover, MA 1810-0000 

Solseheid, Jill 

500 E. Franklin St. 

Centerville High School 
Centerville, OB 45459-0000 

Spry, David 
555 N. Byatt St 
Tippecanoe High School 
Tipp City, OB 45371-0000 

Starr, Jennifer 
221 E. Trotwood Blvd. 

Trotwood Madison Sr. High Soho 
Trotwood, OB 45426-0000 

Strickland, Jefferey 

501 Mosley Dr. 

A. Crawford Mosley High School 
Lynn Haven, FL 32444-0000 

Teeuaseh, Tony 
5323 Montgomery ME 
Del Norte High School 
Albuquerque, NM 87110-0000 

Terry, Nathan 
75 Chenango Ave. 

Clinton High School 
Clinton, NT 13323-0000 


Laboratory: HL/FO 
Vol-Page Mo: 15-49 


Laboratory: PL/GF 
Vol-Page No: 13- 3 


Laboratory: AL/OZ 
Vol-Page No: 12-36 


Laboratory: FL/GP 
Vol-Page No: 13- 4 


Laboratory: AL/OE 
Vol-Page No: 12-37 


Laboratory: NL/PO 
Vol-Page No: 15-50 


Laboratory: WL/AA 
Vol-Page No: 15- 4 


Laboratory: WL/FX 
Vol-Page No: 15-19 


Laboratory: FL/VT 
Vol-Page No: 13-18 


Laboratory: RL/ZR 
Vol-Page No: 14-11 


xn 







HSAP Participant Data 


Thomson, Randy 


Trlana, Zayda 
727 E. HildaBrand 
Incarnate Word Sigh School 
San Antonio, TX 78212-2598 

Trossbach, Christina 


0 


Tseng, Miranda 
3301 Shroyar Rd. 

Kettering Fairmont Sigh School 
Kettering, OH 45429-0000 

Tutin, Dareie 


Vaill, Christopher 
Route 31 

Vernon-Verona-Sherrill Central 
Verona, NX 13478-0000 

Ward, Jon 


t 


0 


Waterman, Sara 
North Jackson St. 

Tullahoma High School 
Tullahoma, TN 37388-0000 

Weidner, Susanne 
7173 1M 1628 
Hast Central High School 
San Antonio, TX 78263-0000 

West, Johnny 
2026 Stapleton Court 
Belmont High School 
Dayton, OH 4S404-0000 


Laboratory: WL/MN 
Vol-Page No: 15-40 


Laboratory: AL/AO 
Vol-Page No: 12- 7 


Laboratory: WL/MH 
Vol-Page No: 15-41 


Laboratory: ML/FX 
Vol-Page No: 15-20 


Laboratory: WL/MN 
Vol-Page No: 15-42 


Laboratory: RL/OC 
Vol-Page No: 14-17 


Laboratory: WL/MN 
Vol-Page No: 15-43 


Laboratory: AXDC/ 
Vol-Page No: 16-10 


Laboratory: AL/OE 
Vol-Page No: 12-38 


Laboratory: WL/AA 
Vol-Page No: 15- 5 


xm 


HSAP Participant Data 


Wick, Matthaw 
<400 Wyoming Blvd. 
Albuqaarqua Academy 
Albaquerqaa, MM 87109-0000 

Williams, Soott 
3511 Day ton -Xeni a Rd. 
Beavercreek Sigh School 
Beavercreek, OB 45434-0000 

Wright, Rady 
<701 Fortune Rd. MW 
that Mass Sigh Sohool 
Albuquerque, MM 87121-0000 

Young, Matthew 
5005 Stahl Rd. 

Jamas Madison High Sohool 
San Antonio, IX 78247-0000 

Siaamzman, Amy 
4524 Linden Ava. 

Carroll High School 
Dayton, OH 45432-0000 


Laboratory: PL/WS 
Vol-Page Mo: 13-21 


Laboratory: WL/AA 
Vol-Paga No: 15- < 


Laboratory: PL/SX 
Vol-Paga Ho: 13-1< 


Laboratory: AL/OE 
Vol-Paga Mo: 12-39 


Laboratory: AL/CF 
Vol-Paga Mo: 12-18 


XIV 




PROGRAMMING IN ADA 


Jason P. Carranza 
AAAF3 

Vright-Patterson Air Force Base 


Pinal Report fori 
APOSR Summer Research Program 
vrlght Laboratory 


Sponsored byt 

Air Force Office of Scientific Research 


August 1993 
1-1 





PROGRAMMING IN ADA 


Jason P. Carranza 
VL/AAAP3 

Vright-Patterson Air Force Base 

ABSTRACT 

During the ausuzer of 1993« I participated in the Air Force Office of 
Scientific Research (AFOSR) High School Apprenticeship Program as 
an apprentice of Charles B. Hicks at Vright-Patterson Air Force Base. During 
this time, my main goal vas to learn the computer language "Ada". The first 
half of my summer vas spent learning and reading about "Ada w . In order to 
improve my programming skills, I wrote numerous programs and sections of 
source code. By the second half of the summer I vas able to vrite full 
programs, packages, and associated teatdrivers vlth little outside help. It 
vas during this time that I finished my project vhlch dealt vith improving and 
completing a project initially started by a former AFOSR student. Using the 
many advantages of "Ada", I completed the project successfully vlthln the 
allotted time. 



1-2 





PROGRAMMING IN ADA 


Jason F. Carranza 
INTRODUCTION 

Vith a small amount of actual axparianca with computers, I vas at first 
hesitant when starting the High School Apprenticeship Program. I had only 
minor experience with computer games and programming and had never experienced 
the vide variety of computer systems and software development environments 
available at VL/AAAF3. During the program, I gained Invaluable knowledge vith 
a VAX-VMS computer system and a SUN workstation. Vith the use of these two 
computers I vas able to experience the advantages and disadvantages of 
different Ada compilers and development environments. The VI editor, which is 
available on most UNIX workstations, vas much more difficult to use due to the 
utilization of tvo modest the command mode, and the insert mode. Even though 
the Vi editor vas much harder to understand, once a person becomes familiar 
vith the VI environment, it is a quick, effective, and versatile tool. The 
Language Sensitive Editor (LSI) on the VAX-VMS vas much easier to use and 
understand because it did not have the confusing modes that the VI editor had. 
In addition, the LSB vas more user-friendly. However, the LSE vas not as 
versatile as the Vi editor and vas more limiting in what it could do. 

DISCUSSION 

Hy first fev weeks were spent learning hov to use the VAX-VMS system, 
then learn "Ada* from the Ada-Tutor on the VAX-VMS system. Before coming to 
this job, I already had one year of programming in Turbo Pascal from high 
school. Because of this important experience, I vas able to learn "Ada" 
quickly. After first relearning about enumerated types, arrays, records, 


1-3 





recursion, strings,...*ftc., I ltsrnsd about linked lists and packages. 

During this ralaarning stage, I realized hov similar "Ada" vas to Turbo 
Pascal. Many of the variable declarations as veil as the procedure 
declarations vere basically the sane, except for functions which were only 
slightly different. Recursion declarations vas one of the "Ada" structures 
that I had a great deal of problems with. In the Ada Programmer's Handbook by 
Dean V. Gonzalez, it defines recursion as "the use of a particular entity to 
define itself". To sum it up, recursion is vhen code calls itself. For 
example, a procedure might call itself vhen incorrect input is given as an 
error checking device. Instead of vriting a large number of lines over, the 
procedure calls Itself and starts over again. The only drawback with 
recursion is that it uses a great amount of memory. However, with the use of 
recursion, the source code of a procedure is shorter, but is generally harder 
to understand. 

Functions in Turbo Pascal are slightly different than functions in "Ada". 
In Turbo Pascal, input/output (I/O) statements such as, vriteln("Hello"), 
cannot be written in the function body. Also in Turbo Pascal, the function 
name is the variable that is being returned. Ada allovs I/O statements to be 
in the function body. In "Ada", the programmer has to define a separate 
variable to be returned, instead of using the function name as the variable. 

Linked lists are a very important part of "Ada". Instead of using 
arrays, which use a great amount of memory, linked lists are used. Vhen 
arrays are declared, a certain amount of memory is set aside for that one 
array. In a linked list, no memory is initially set aside. Vhen memory is 
needed, a linked list will use that one memory space. During the program, if 
the link is no longer needed, it can easily be given bsck to the computer for 

1-4 




other uses. In an array, the space set aside is not given back to the 
computer until the .rogram is terminated. 

There are tvo types of linked lists: single linked lists, and double 
linked lists. Pointers are the basis of both single linked lists and double 
linked lists. Pointers are defined as a variable that holds an address of 
another variable. Pointers organise the linked list into a chain of links by 
keeping track of vhere all the addresses of the information or storage/memory 
spaces are. Pointers are initially set to point to null or nothing. Vhen 
information nteds to be added, the pointer points to an address vhere 
information can be stored, vhen the storage space is no longer needed, it can 
be given back to the computer for other purposes. 


SINGLE LINKED LIST 


HEAD—> 


info l ) info | 

_| I_| <—TAIL 

Tn*xt~l—>1 .next I 


Figure 1 


In a single linked list, there are tvo pointers used: the head, and tail 
(refer to Figure 1). The head points to the first address while tail points 
to the last address in a list. Vhat points to the addresses in the middle of 
the linked list? The answer is simple. At each address is stored an internal 
pointer (.next). Vhen a new link is added, this internal pointer (.next) 
points toward the new address and continues to point to that address until 
told otherwise. The Internal pointer (.next) in the new address points toward 
the address that the old pointer used to point towards. During this whole 
time, the tail points tovard the last piece of information. If a new link is 


1-5 





added after the tail, then the tail is moved to point towards that new last 
piece of information. To sura everything up, everything between head and tail 
points in one direction and is linked like a chain with head at the beginning 
and tail at the end. If one link of a chain is added, the link before and 
after the new link has to be connected to the new link. 

DOUBLE LINKED LIST 

i info | | info | 

HEAD—> |_I j_j<—TAIL 

I .next I—>1 .next I 
l.xBiiail<— 1 iPriotl 

Plgure 2 

A double linked list is essentially the same as a single linked list 
except that there are two internal pointers (.next, .prior) In an address 
(refer to Figure 2). Instead of each link pointing towards the next link, one 
pointer points toward the link or address ahead of it (.next), and the other 
pointer points towards the link or address behind it (.prior). 

After exploring linked lists, I spent two weeks struggling to learn 
strings, mainly the disadvantages of strings in "Ada". Strings are very 
limiting in "Ada". Characters and Integers each have a package designed to be 
used specifically for characters and integers, while strings do not have such 
a package. I had to vrite a package designed to do exactly this called, 
STRFNS. This package allowed me to compare, search, translate, and assign 
strings to other strings. 

After my uphill battle with strings, I worked with a text package 
called, TEXT. Using variables defined as type text was much easier than using 

1-6 




many different verieblee for one string. The type text consisted of e record 
of e string end an integer. The string held the value of the input, while the 
integer would store the length of the string. Instead of using many 
different variables, 1 could use one variable that held all the information 1 
needed. I wrote a package for type text similar to that of strings, except 
that this package vent a little further. The TEXT package was programmed to 
compare variables of type text, translate strings into type text, get input of 
type text, and give output of type text. It took me two days to write the 
text package, compared to two weeks for the string package. 

Through my experience with the assorted packages I programmed, I had 
the chance to finally comprehend packages. Until this summer, 1 had only 
used packages, but had never written or really understood one. Peckages are a 
valuable advantage of "Ada". There are two parts to a package! the package 
specification, and the package body. The package specification contains the 
variable declarations for the package body and also contains all the 
function and procedure names the programmer may use. It does not, however, 
contain the source code of the functions and procedures, only the names and 
comments. The package body contains the functions and procedures along with 
the source code. When using a package, the user only sees the package 
specification, not the package body. Another advantage of packages is that a 
programmer can save a great deal of time because the programmer does not have 
to continue writing the same source code over for each program. He calls the 
package name in the program, and all the functions and procedures in that 
package specification is at the use of the programmer. 

After vorking with most of the different source code structures of "Ada", 


1-7 




I started ay final project. The point of this project v&s to prove hov far I 
had advanced in prograaaing vith "Ada”. I vas given the source code of a 
multi-package program vhich ran a menu for a VAX-VMS system. The code vas 
poorly written, and vasn't versatile enough because it could only run on one 
specific VAX-VMS system. Originally, the program vas about 1200 lines. After 
deleting a large amount of code, adding several procedures, and adding in 
error checking structures, the program vas 1000 lines. 

CONCLUSION 

When looking back over the whole summer, I know Z learned a great deal. 
During this summer, I realised that I vent to vork vith computers. The 
experience I gained vith computers and computer prograaaing in "Ada" will 
help me at any college that I attend and also will help me in future jobs 
dealing vith computers. 


1-8 





Computer Performance 
in Function 
Decomposition 

Kevin M. Dailey 


Final Report for 

High School Apprenticeship Program 
Air Force Office of Scientific Research 


System Concepts Group 
Avionics D i rect or ate 
Wright Laboratory 


August 16,1993 


2-1 




JL 


Computer Performance 
in Function 
Decomposition 

Kevin M Dailey 


Abstract 

The growth rate of run times of function decomposition with a tabular and non-tabular 
setting using a fixed number of cares was studied. To start with, six random recursive 
functions were chosen having an input range from 8 to 14 variables with the output of 
this function being a binary string. Portions of this binary string were then "masked" in 
two forms, one tabular and the other non-tabular. This masking is done by taking a fixed 
number of cares from the binary string and leaving die placement of the others in a don't 
care format where the polarity is not known. Then a computer progr am a ttempts to 
decompose the function into sub-functions and the run is recorded. After the results 
were gathered. The nan-tabular setting had better performance in low number of cares 
experiments, while the tabular stayed constant no matter what number of cares were 
used. 


2-2 





Computer Performance 
in Function 
Decomposition 

Kevin M. Dailey 




In recent computer testing, programmers and researchers are trying to develop a type of 
"pattern sensitive" program that can detect order, structure or regularity [1] in a natural 
environment. The problem is to make a program robust enough in order to detect all of 
the many patterns that appear. A way to show "pattem-neas” is through Decomposed 
Function Cardinality (DFC). The DFC can be found for any problem containing a sense 
of order or structure to it. The researchers of Pattern Theory at Wright Labs have 
developed FLASH (Function I ea rning and Synthesis Hotbed) along with a technical 
report [2], a program planned to be a robust pattern finding algorithm that will create a 
function, take the binary output from tint function and sample die dements (take a given 
portion of "cares" and leave the others "masked”), reconstruct die original function from 
the sampled data, and find the DFC of the function. 


This program was the key mol to die studies and experiments practiced here. This is a 
physical example of an undecomposed Benchmark Function (#19). (The Benchmark 
Functions are a standardized group of 30 different 8 variable functions used by AART 
for groups of partition and other decomposition plan testing.) 


2-3 













The primary objective of the program that my work evolved around was set to 
determine die "tabular" versus "non-tabular" growth rates for a fixed number of "cares" 
using randomly generated functions, then taking a sampling of those for decomposition 
and finally recording the time that it took to decompose the sampled binary string. This 
data gathered from the run time of these would be set into graphs and charts to see which 
version would have better performance. 


Methodology 

The two approaches, tabular and non-tabular, are two different ways of recording the 
binary string. The tabular approach is where the string listed in a long string of cares and 
don't cares. The cares are represented as l’s or 0's and the don't cares are represented as 
Xa. The number of variables directly determines the number of elements in the Unary 
string in an exponential form. Therefore, if using 8 variables, then there would be 256 
dements of that created binary string consequently, using 14 variables would create a 
binary string of 16384 dements in length. Depending on the size of the string, die 
number of sampled cares went from 64 cares to 100% cares. Therefore, the tests run on 
an 8 variable function would work with 64 cares, 128 cares, and 256 cares which is 
100% cares, seeing as there are 256 dements. 




Example of tabular 64 "cares" 8 variable random function. 


XXOX1XXXOXX1XXQXXLXXXIXOXXX1XXXXOXX1XXX1XXXX1XXXOXXX1XXXXXOX 

XXXDXX1XXXOXXXOXXX1XXXXOXXXOXXOXXXX1XXX1XXXXXX1XQXXX1XXXQXXX 

XXX1XXXXXXX1XXXX1XXXXXX1XX1XX1XXOXXOXXXX11XXXX1XXXX1XX1XXXXX 

XX1XXXX1XXXXOXXXXXIXXXXOXXXOXXXX1XXOXXXOX1XXXOXX1XXX1XXXOXXX 

XOXX1XXX1XXOXX1X 


The noa-tabular approach is different only by the way that it stores the binary string. 
The placement of each care is recorded along with the polarity of that care, and die don't 
cares are not recorded in die new file. The following example is a portion of an example 
of a non-tabular 64 care 8 variable random function. 


Example of non-tabular 64 "cares" 8 variable random function 

31 

40 

60 

111 

171 

230 

330 

431 

300 

360 

571 

580 

631 

700 

711 

770 

791 

831 

830 

900 

931 

960 


2-6 










If the previous example was not a "portion" the data set would continue on until there 
were 64 entries (cares). 

The reason that these two procedures were chosen is for the fact that when you have a 
14 variable function, die decomposition program will have to lode through 16384 
different cares and don't cares in a tabular approach regardless of the number of cares 
chosen, while die non-tabular approach only has to look through the number of cares 
given. For example, a 14 variable function is running on both tabular and non-tabular 
with 128 cares; the decomp pit (the pro-determined form that FLASH tries to find die 
DFC of a function) will have to scan through 16384 elements in die tabular before 
decomposition, whereas die non-tibular will only have 128 elements. Because of this, 
the hypothesis was that the non-tabular would p erform better than die tabular. With this 
in mind, all die test runs were executed from 8 to 14 variables and from 64 cares to all 
cares. This data is what is die bulk of my report. 

Being that a computer can not spontaneously generate a random number, a seed must be 
given to the compu t e r as a basis for the randomness. With this seed, die computer does 
many redundant math processes so as to make the final output a number highly unrelated 
to the original seed. When the co mpute r creates the random function, it automatically 
uses die seed of 0 in it’s co mp uta tions; therefore, if two functions were made die same 
way, then the output should be verbatim. When the random samples are taken from die 
random function's binary string, FLASH promp t s the user for a seed. For a basis of 
comparison, die seed was chosen to be l and stayed that way throughout all experiments. 
Likewise, if those two same functions were sampled die same way, die new binary 
strings would be verbatim. The actual values of 0 and 1 have no significance in 
themselves, but 


2-7 




just the fact diet they are constant throughout all experiments is enough to eliminate the 
varying degree in the e xp er iments. 

Some of die data gathered from the standard FLASH run times from decomposition had 
surprising results. Because of this, some excursions were chosen to see how these 
differences related to the standard testing. These excursions varied with select 
experiments from 12 to 14 variables and from 128 cares to 512 cares. The first excursion 
that was done was a constant 0. hi these tests, instead of having a random function create 
a random binary string, the binary string had a constant 0. From this, samples were taken 
and then decomposed. The second and third excursions were done using the same 
random binary strings, but this time, changing the decomp plan. The decomp plan is die 
way that FLASH sets up the partitions and the direction that it scans the binary string for 
die decomposition. The two excursions were Mosdy Row and Mosdy Column, hi 
Mostly Row, it scans mosdy the rows in die decomposition, and in Mosdy Column, it 
scans mosdy columns. 

Remits 

Tables 1 and 2 show the run time growth in tabular and non-tabular FLASH tuna using 
the standard procedures with random functions. The horizontal axis of the two charts 
r epre s e n t s the number of variable inputs. The vertical axis represents the number of 
cares. The blank portions in tables 1 and 2 are the impossible expe rim e nt s, where die 
number of cares exceeds die dements in the binary string. Tables 5 through 10 are 
charts of the excursions done. Tables 3 and 4 are portions of table 1 and 2 used for 
comparison with the excursion tables, likewise in these charts, the horizontal axis is 
number of variables, and die vertical bar is number of cares. 


2-8 





Run Time# for tabular and norvtobular FLASH run# 


# of variable Input# 

8 9 10 11 12 13 14 

0.129 0.312 0.59 1.383 2.566 6.164 11.749 

#of 512 0.32 0.59 2226 5.168 6.674 1455 

cam 256 0.132 0.367 0.687 2.066 3508 6.597 12.78 

128 0.14 0.558 1211 1.569 3289 6.116 12.577 
64 0293 0.367 0.774 1.476 3.015 5.859 12.703 

Tabto 1: Tabular approach 


# of variable Input# 



8 

9 

10 

11 

12 

13 

14 

A# care# 

023 

0.547 

1.109 

2469 

5.101 

10.992 

22589 

16384 







23516 

8192 






1059 

13.108 

4096 





5.168 

7484 

8474 

2046 




2488 

2.731 

7414 

12508 

1024 



1.148 

1527 

1545 

3598 

3.797 

512 


055 

0586 

1509 

2502 

2.133 

2.718 

256 

025 

0.332 

0532 

121 

0536 

0.997 

0.966 

128 

0.133 

0526 

0.613 

0542 

056 

0.715 

0.753 

64 

0246 

0.192 

0.183 

0583 

0214 

0441 

0536 


Tabid 2: Non-TabUar approach 
Run Tftnd ter tabular end nonHabutar AAM nm# 



2*9 



Run Tim# data for decomposition plan excursions 


Tabular 

# of variables 


12 

13 

14 

4 Of 512 

6474 


cares 256 3.508 

6.597 

12.78 

128 

6.116 


Table 3 Tabular Random 





#of variables 



12 13 14 

#of 

512 

5459 

cares 

256 

2.734 5.706 11.91 


128 

5.665 


Table 5:Tabuiar Constant 


Non-Tabular 

# of variables 
12 13 14 

#of 512 2.133 

cares 256 0.836 0.997 0.965 

128 0.715 

Table 4:Non-Tabular Random 


# of variables 
12 13 14 

#of 512 1.051 

cares 256 047 0.613 0.821 

128 0471 

Table 6:NoivTabutar Constant 


# of variables 
12 13 14 

*<* 512 5474 

cares 256 2527 5447 17448 

128 5467 
Table 7:Tabukjr Most Row 


#of variables 
12 13 14 

#of 512 0484 

cares 256 0425 0434 0471 

120 ai84 
Table 8:NorvTabuiar Most Row 


#of variables 
12 13 14 

*cf 512 7.973 

sores 256 2498 45406 17*247 

128 4<MW6 

Table 9:Tabuiar 


Most Column 


#of 


12 


13 


512 1482 

266 0436 0464 
128 0486 

Table 10 Non-Tabfuar 
Most Column 
to excursions 


14 


1.199 



2-10 










The results for the tabular run times in standard testing showed that the testing is not 
very sensitive to the given number of cares in each experiment, but there is an 
exponential growth in incrementation from variable to variable. The non-tabular 
standard test runs, however, show great sensitivity to die number of cares chosen and die 
number of variables used. Only on all cares does the growth appear to be exponential. 
With small amounts of cares, die growth is minimal, and with larger amounts of cares, 
the growth is increased. 

In die Constant excursions, die run time was better for non-tabular vs. tabular, and all 
the Constant excursions had faster run times that the standard random counter parts. The 
Most Row tabular excursion correlated well with the Constant tabular excursion, bat the 
Most Row non-tabular excursion ran faster dun the Constant non-tabular excursion. 
There were some definite differences with the Most Column tabular excursion, seeing as 
the 14 variable test was ten times slower dun die Most Row tabular excursion, and die 13 
variable 128 and 256 care experiments were about nine timet slower than the Most Row 
tabular excursion. However, the Most Column non-tabular excursion did not differ 
highly from die other excursions. In decomposition, if die function has a good DFC, 
then it will 

decompose. A problem encountered, was that with random functions having a good 
DFC will spend time in those sub-functions and decompose them. This adds run time to 
those functions with better DFC'i, but not to those of which with bad DFCs. 

^QOdBflflD 

The results of the standard experiments show that the performance of the non-tabular is 
greater than that of tabular in small care sampling, but gets worse dun that of tabnlar 
when the number of cares doses toward all cares. The run time for a non-tabular 
approach is not very sensitive to the type of decomposition or the type of binary string. 

2-11 





The tabular is not very sensitive to the type of binary string, but does not work well with 
a Mostly Column decomp plan. After working with FLASH, a bug was found in the 
program that caused the computer to spend tremendous amounts of time resolving a 
small bit of code in die program. 


2-12 






References 


[1] Dr. Timothy D. Ross, Michael J. Noviakey, Timothy N. Taylor, and David A. Gadd 
Pattern Theory: an Engineering Paradigm for Algorithm Design. Final Technical Report 
WL-TR-91-1069, Wright Laboratory, USAF, WL/AART, WPAFB, OH 45433-6543, 
August 1991 


[2] Dr. Timothy D. Ross. Function Decomposition Strategy for the Function Learning 
and Synthesis Hotbed. Technical Memorandum, Wright Laboratory, USAF, WL/AART, 
WPAFB, OH 45433-6543, August 1992. 


2-13 





Ap pendix A 

The decomposition plan used for standard FLASH runs is die following: 
Decomp Plan: 

Selection Plan: 

0 » use shared variables 

12 = method 
2 m find part type 
1 a stopping condition 
1 = stopping condition parameter 

Evaluation Plan: 

1 a no of partition tests 
1 a measure challenger by 
1 a measure ehamp by 
4 a threshold inn 
1 as cha>np_mnltjpl for 
1 = Random No generator seed (X)) 

1 a c^p_for_best_pait_childxen^is_8aine 


2-14 





Appendix B 

The decomposition plan used for Mostly Row FLASH runs is the following 
Decomp Plan: 

Selection Han: 

0 = use shared variables 

11 a method 
0 « first part type 
1 * stopping condition 

1 = stopping condition parameter 

Evaluation Plan: 

1 a no of partition tests 
1 * measure challenger by 

1 a measure champ by 
4 a threshold inn 
1 a champ_multiplier 
1 = Random No generator seed (>0) 

1 a dp_fpr best part chfldT wni a 


2-15 




Appendix C 

The decomposition plan used for Mostly Column FLASH runs is the following: 
Decomp Plan: 

Selection Plan: 

0 s use shared variables 
12 s method 
0 * first part type 

1 * stopping condition 

1 sb stopping condition parameter 

Evaluation Plan: 

1 » no of partition tests 

1 =s measure challenger by 
1 = measure champ by 

4 * threshold in n 

1 = rfiamp_iwn1fiplM»r 

1 a Random No generator seed (>0) 

1 * dp_for_best_part_children_ia_same 


2-16 







AppcatoD 


Actual partitions for each decorap plan used. 



Standard 

Mostly Column 

Mostly Row 

Number 8 

10001111 

10000000 

01111111 

of 9 

000001111 

100000000 

011111111 

Variables 10 

1000011111 

1000000000 

0111111111 

11 

00000011111 

10000000000 

01111111111 

12 

100000111111 

100000000000 

011111111111 

13 

0000000111111 

1000000000000 

0111111111111 

14 

10000001111111 

10000000000000 

01111111111111 


2-17 





FINAL REVIEW OF SOFTWARE PACKAGES USED IN VARIOUS 
COMMUNICATION APPLICATIONS 


Scott McGovern 
Junior 


Bellbrook High School 
3491 Upper Bellbrook Road 
Bellbrook, Ohio 45305 


Final Report for: 

AFOSR Summer Research Program, Wright Laboratory, 
Electronic Warfare Division 


Sponsored by: Air Force Office of Scientific Research 
Research & Development Labratories, Culver City, CA 

August 1993 


3-1 




FINAL REVIEW OF SOFTWARE PACKAGES USED IN VARIOUS 
COMMUNICATION APPLICATIONS 


Scott McGovern 
Junior 

Bellbrook High School 


Abstract 

Contained within this report are evaluations of software packages used in the day-to-day operation 
in electronic warfare and communications. Harvard Graphics, Mathcad, DADiSP, Graph Tool, Microsoft 
Visual Basic and Excel were evaluated. A brief explanation of each program will be followed by the 
application for which it was used. 


3-2 





FINAL REVIEW OF SOFTWARE PACKAGES USED IN VARIOUS 
COMMUNICATION APPLICATIONS 


Scott McGovern 

Introduction 

Today’s engineer is almost dependent on his computer. Most of the work he does in the day 
is done on his computer. He can create a waveform, plot it, and analyze it without getting up from 
his chair. This can be accredited to the growing power of the computer. Everyday advancements are 
being made, making life easier for computer users. To take advantage of these advancements people 
have to buy the software. Most of these advanced software packages are very expensive. For a new 
program, a person might be looking at a $25,000 price tag, or even more. Though this software isn’t 
for the everyday user but for business, the price is still high. Another setback with the growth of 
technology is the learning time. Many of these new programs are very complex. These software 
packages can sit on the shelf for months due to the lack of time to learn how to use the program. 
These problems are overshadowed by the amazement of the advances in the computer world. 
Hopefully as computers advance even further, these problems will be fixed. 

GraphTool 

GraphTool is designed to plot information on different types of graphs and charts. GraphTool 
has the ability to graph in two or three dimensions. It lets the user plot area charts, bar charts, xy 
plots, column charts, pie charts, scatter plots, and vector plots in two dimensions. It is able to plot 
carpet plots, earth plots, scatter plots, shadow-contour plots, surface plots, and vector plots in three 
dimensions. The program is easy to use and self-explanatory. The different menus guide the user 
through the program with little to no difficulty. With the exception of some three dimensional plots, 
GraphTool is quick to plot the information provided by the user and edits the plot as soon as the 
generating equation is edited. 

As a project on GraphTool, I used the Gauss Function 


f (xy) =e 


-n[ 


x-x, 


o,a 


+e 




to create three different graphs. The purpose of this application is to measure the intensity of light, for 
instance, when shone at a figure. At the tip of the figure the light is the most intense. As the light 
goes around the object it becomes weaker. Figures 1, 2, and 3 show this theory. Changing the 
variables in the equation changes the size and the shape of the figure as shown. 


3-3 





For the first graph I made x^= 0, y Q = 0, and b=l. This produced what is seen in Figure 1. For the 
second plot, Figured1 edited the equation, making x 0 =l, y 0 =l, and b=l. This centered the figure at 
1,1 instead of 0, as it was before. Finally, 1 edited the equation once more, this time making 0, 
y 0 = 0 and b=2. As seen in Figure3, the figure expanded, becoming wider, while staying centered at 0. 


M 

mi 

mm ii 




///mW 



My 
























Mathcad.v3.0 

Mathcad is a very useful program. It allows the user to work with numbers, formulas, text, 
and graphs. It too is very easy to use. To the left of the writing surface is a list of the different 
functions performed by Mathcad. The user only has to click the button for the function he or she 
wants and Mathcad will carry out that function. Everything on the screen appears as it would if 
written by hand. Text can be added as a reference without affecting the processing of the formulas. 
Mathcad allows the user to graph the results of an equation or series of equations. Mathcad 
automatically generates the scale that applies to the plot. 

Mathcad allows the user to import information from other sources. I imported information 
from a SunSparc program put on disk and converted to what Mathcad could read. I was doing this 
because the information was originally graphed on the Sun but when it was printed out, the Sun 
included the window containing the graph. To get rid of the window we tried importing the 
information to Mathcad and then graphing it. After getting the information plotted on a graph, we 
printed it, finding that die graph wasn’t big enough. It showed the information perfectly, but was too 
small in size for a presentation. Even after expanding the graph to the size of the screen it still wasn’t 
big enough. 

I also used Mathcad to write a program that generates shift register PN codes. By specifying 
the tap settings in the top row of matrix A in the program, maximum length codes are generated. 
These codes are the longest possible random codes that can be generated with those taps. Picking the 
tap settings that will give you the maximum length code takes a while because of the many 
possibilities. In the program I used I followed a list of polynomials. The program shown is from the 
polynomial, x 5 +x 2 +l, where the fifth and second blocks are tapped. Also in this program, I have 
graphed the result of the evaluation of the periodic auto corrolation as it would be seen in the 
frequency domain on a spectrum analyzer. I also used the polynomials, x 5 +x 4 +x 3 +x 2 +l and 
x 5 +x 4 +x 2 +x+l. The graphs for these were similar to the first polynomial, but the code they generated 
was different. 

Harvard Graphics.v3.0 

Harvard Graphics is a chart making program. It isn’t as powerful as GraphTool, but it is used 
more for graphs used in presentations or reports. Harvard Graphics lets the user choose to create pie 
charts, xy charts, and organizational charts. It took no time to learn how to use this program. 

Creating charts with this program took little time. Harvard Graphics lets the user change the 
information on a chart very easily, making it very user friendly. 


3-5 







This program generates shift register PN codes. Maximum size codes result from setting the correct 
taps in the first row of matrix A. 


Nr := 5 the numberof stages in the shift register 


N:= 2 


Nr 


v := 



initial starting conditioner the shift register 



output control vector for shift register 


A := 


0 1 0 
1 0 0 
0 1 0 
0 0 1 
0 0 0 


0 

0 

0 

0 

1 


1 

0 

0 

0 

0 


shift register matrix for the MLcode 
This is a (5,2) MLcode 


<o> 

Cq := v *m 


initial output of shift register 


j := 1..N-2 


i := O..Nr-l 


v i,j :=mod[[A«v <J " 1> ].,2] 

i 


<}> 

c, := v * »m 


3-6 





ompute the periodic auto -correlationfunction 


Mt col u mn to the right is the PN code generated by this program. The reason that the code only 
, n t»in < i*s and 0’s is because it is a digital code. Below is what the firs', ten numbers in the 
quence look like in the waveform. 



k := 0..N-2 


i 

j := 0..N-2 


cj := if£cj*0,-l,CjJ 



vc j,k := c mod[[j+lc].N-l] 


<k> 

rcor k := vc *c 


I := 0..N-2 


gcor, := rcor 


rnod [[ 1+ i]- N -‘] 



1 


3-7 















Harvard Graphics not only creates charts but also allows the user just to draw. I laid out the 
floor plan of a lab that my mentor will soon be moving into by just choosing "Draw" from the main 
menu. The grid on Harvard Graphics made it easy to scale the size of the room. Moving the figures 
which represented the different desks, lab tables, bookcases, etc., was tough with the grid and the 
"click to" on. The "click to" make the boxes go to the closest place on the grid. After turning the 
grid and "click to" off, it was easier to put the figures exactly were I wanted them instead of just 
having them close to that spot Harvard Graphics allowed me to label each piece with the text box. 
After writing the name of the piece of furniture, I placed the label on it and used the group function 
from the choice bar to the left of the screen. By doing this, 1 connected the name to the piece of 
furniture so that when I moved the furniture, the name would go with it. I also used Harvard Graphics 
to make a model of a communication system. As with the floorplan, I used the "Draw" mode. 
DADiSP.v2.01 

DADiSP is a program that generates waveforms. DADiSP stands for Data Analysis and 
Digital Signal Processing. It is set up like a spreadsheet, but instead of boxes, it allows the user to put 
information in windows. The user can refer to these windows when entering information just as in 
spreadsheets. Each window is assigned its own number. In referring to the windows the user can use 
"w2", this tells the program that you want the information in window 2 to be a factor in the window 
you are currently working in. 

I used DADiSP for an AM modulation and an FM modulation. I began the AM modulation 
by creating a signal by adding two sine waves together, shown in Wl. Alone, this signal could he 
disrupted by noise, other signals, etc. To make sure this doesn’t happen the signal is modulated or 
moved down the spectrum where disruption is less likely. To do this the original signal must be 
multiplied by a pure sine wave, called the carrier (W2). The result of this is seen in W3, as we would 
see it in the time domain. It can also be seen in W5, the spectrum of the new signal. In W4 there is 
only one band or spike because it is the spectrum of the pure sine wave. Now that the original signal 
has been added to the carrier the four other spikes appear, called sidebands. Notice that there are two 
on each side. This is because when the signal was modulated it was placed on both sides of the 
carrier. The reason there are two sidebands on each side is because the original signal was made up of 
two sine waves. If the e.rrier was to be stripped off, the result would be the spectrum of the original 
signal. 

In the FM modulation I started with a sine wave. Like AM modulation, you are trying to 
move this signal, but instead of changing the amplitude , you are changing the frequency. The sine 


3-8 



















































































































































































FM modulation 


n umunmu m m 


































































wave in W2 was used to change the frequency of another higher frequency or sinusoidal function to 
give what is seen in W3. It can be seen that the frequency has been changed greatly. In W3 it can 
also be seen that the signal fluctuates, going from a lower frequency to a high frequency. W4 showns 
how the spectrum of the new signal and the original sine wave are totally unrelated. 

Microsoft Visual Basic 

Visual Basic is designed to run in front of other programs and give them commands. It is 
based on writing code in the Basic language. This is one of those programs that had been waiting for 
someone to use so they set me on it I was to learn all I could about the program and tell everyone 
how the program worked. I slowly made progress by looking through the user’s guide, then the 
language guide, and finally the online help menus. I created some of the little projects from the user’s 
guide, such as a setup that during run time would reply "You clicked me,” in a text box after hitting 
the command button that was labeled "Click Me!”. As a way to get more familiar with the program, I 
made an investment calculator. I set up four text boxes for the initial amount, percent interest, 
duration in years, and total amount respectively. During run time, the user would enter the numbers of 
his choice, click the command button labeled "Calculate Total Interest," and the total would appear in 
the last box labeled "Total Interest." Another way to become familiar with the program was to create a 
blackjack game. I helped some, but my lack of knowledge of the Basic language made it hard for me 
to write some of the programs. After becoming familiar with the program I wrote a couple of little 
hints to make using the program easier for people just beginning to use the program. In the write-up 
it should be noted that I used the programmer’s guide for the definitions of the things I was not 
familiar working with. These are mostly the last couple of definitions under the "ToolBox" heading. 
Microsoft Excel.v3-0 

Microsoft Excel is a very useful spreadsheet. It is easy to use with extensive online help. The 
only difficulty I had with this program was picking the correct paste function for my calculations. I 
used Excel to set up a spreadsheet to solve for signal-to-noise ratio for a radar system given the range, 
power, antenna gain, radar cross section, temperature, frequency bandwidth, and noise 
factor. Using the following equation, 

Signal _ PCGAeo 
Noise (4tcJ? 2 ) KTjjBFn 

I substituted in those values mentioned above. Each of the variables in this equation is defined on the 
next page. The Antenna Aperature (Ae) was calculated within the program, as was the wavelength. 


3-11 





Visual Basic 


Saving and Recalling Projects 

When you start Visual Basic, it automatically opens a new project To open a 
previously saved project, the user must choose the "Open Project" command from the File 
menu Then the user must select the "Project Window" command from the Window 
menu. The user must then highlight the name of the project in that window and then click 
the "View Form" command button 

After finishing work on a project, the user must go to the File menu and select 
either thi "Save Project" or the "Save Project As. . ." command Both of these will save 
the project as shown, but the "Save Project As. " command enables the user to change the 
name of the project being saved. 

Visual Basic does not save individual files to be recalled These files must be 
saved as a project. The "Save File" command in the File menu only saves the current form 
in the project. This form cannot be called up by itself. It must be opened as part of a 
project. 

Tool Box 

The Tool Box provides fifteen applications to the user. The Tool Box allows the 
user to create the different applications on the form. Through the use of basic 
programming, any of these applications can interact with another. The following is a brief 
explanation of each of the applications: 

1. Picture Box - contains a graphical image (bitmap or icon) through the 

use of simple code. The code isn't directly placed in the box itself, 
usually contained in the command button. Text can also be printed 
out onto the picture box through the same method. 

2. Label - control box in which a caption is placed to show the purpose or 

procedure of another box Has no code written in it, but code 
refeato the name of the label to recognize the box it labels 

3. Text Box - enables user to write or print out text to a box. As with the 

picture box, code isn't written in but instead the box is refered to in 
the code of a command button. 

4 Frame - used to group option buttons or other controls. Carries out no 
special function. 

5. Command Button - control that carries out a command or action. In 
run mode this button is usually clicked to start procedure. Most of 
the written code in the program is written in this control box. 

6 Check Box - control used to represent an option(on/off, true/false) that 
the user can set or clear by clicking during the run mode. Ar X" 
appears when the box is selected. 

7. Option Button - Control similar to a Check Box, but it is used as part of 

a group. Only one button can be selected at a time during the run 
mode. 

8. List Box • provides a list of choices to the user during the run mode. 


3-12 







9 Combo Box - similar to the list box, but the user can enter or select 

items from the list provided during the run mode. 

10 Horizontal scroll bar - control that appears as a scroll along a horizontal axis 

It is not attached to a window, but can be used as an alternative way of 
getting input and displaying output during the run mode. 

11. Veritcal scroll bar - same characteristicsas the horizontal scroll bar but is 

instead on the vertical axis. 

12. Timer - control that responds to the passage of time: specifically, a period 

which the programer specifies. A form may have multiple timers, each set 
at a different interval. Only control not seen during the run mode. 

13. Drive list box - control that displays a list of all valid drives in the user's system 

A drive list box finds and switches between valid disk drives at run time 

14. Directory list box - control that displays a hierarchical list of dirctories. 

Enables the user to select directories and paths at run time. 

15. File list box • control that displays a list of filenames selected according to their 

tributes. Finds and specifies files to be opened, saved, or otherwise 
manipulated at run time. 

Division with Visual Basic 

Visual Basic uses two forms of division, real and integer. When writing code for 
these operations the user must know the difference between the two. The symbol for real 
division is /, whereas the symbol for integer division is V 

*Note - Program automatically converts values to real or integer, which ever the 
user chooses. 

Ex 

x is an integer x =3 

y is an integer y =11 11/3=4 

z is an integer z = y/x =11/3 but 

11\3 = 3 


The Immediate Window 

The Immediate Window appears in the break mode during run time When the 
break mode is selected from the Run menu the immediate window appears on the screen 
It is most useful when trying to debug your program Typing "Debug.Print [item in 
program] ; H will print out the error in the immediate window so you can fix it This 
process works best when your variable is known to change, like putting the statement in a 
loop that repeatedly alters the statement. 

Another way to debug the program is to tpye aTat the beginning of the line and 
then put in the variable. 

?x ? card(l) 

82 or as in the blackjack program 3 

?y ? suit(l) 

23 1 

If, for instance, you wanted to know what the value for the second card was, all you 
would have to do is scroll up the list and change the 1 to a 2, instead of retyping the line. 


3-13 





Properties 

Below the list of menus on the Visual Basic screen is the properties bar. The first box 
that appears on the bar contains a list of different properties that can be used . Clicking the 
down arrow button next to the box, allows the user to view the different choices for the 
application. It should be noted that not all of the applications have the same properties 
available to them. Listed below are brief explanations of some of the most commonly used 
properties: 

1. Caption - The caption property is used with forms, click boxes, command buttons, 

frames, labels, menus, and option buttons. Use this property to label a form or 
control descriptively. 

2. CtlName - This property is used with all applications. By default, the CtlName is 

automatically set. For instance, when you create a frame, the CtlName will be 
"Framel." The user must change this name into one that will describe the 
function of the application because this will be the name referee to in the 
written code. 

* 3. Alignment - Used with labels. Aligns text within the label. 

Left Justify = 0 (default) 

Right Justify = 1 
Center = 2 

* 4. Autosize - Used with labels and picture boxes. Automatically resizes control to 

fit it contents. 

True = 1 Automatically resizes 
False = 2 (default) Keeps size constant 

* 5. Autoredraw - Used with forms and picture boxes. 

True = (-1) Enables automatic repainting of a form on a picture box. 

. Graphics and print output are written to the screen and 

images are stored in the memory. 

False = ( 0 ) (default) Disables automatic repainting and writes only to 
the screen. 

6. Back Color / Fore Color - Used with forms, check boxes, combo boxes, 
command buttons(Back Color only), directory list boxes, drive list 
boxes, file list boxes, frames, labels, list boxes, option buttons, picture boxes, 
and text boxes.Back Color sets or returns the background color of an object. 
Fore Color sets or returns the foreground color used to display text and 
graphics in an object. 

There are two ways to get the color you want in the object you want it 
in. The first and easiest is to click the desired property and then just go down 
to the color pallet and click the color of your choice. The other way is to 
type the hexadecimal code for that color in the box to the right of the property 
box. 

*Note - For all of those marked above, there are subgroupings within each property. 

To be able to choose from those groupings you can go to the box to the right 
of the property box and click the down arrow key. This will show all of the 
choices for each property. 


3-14 




Problem: Determine the signal-to-noise ratio out of the IF filter for targets having cross sections of 
25m 2 and 50m 2 at ranges of 50, 100, and 200 km. Use single hit detection. 

Solutions 

Signol-to-Notse Computation 
Radar Cross Section 25 


Range (m) 

5.00E+04 

1.00E+05 

2.00E+05 

Range (m) 

5.00E+04 

1.00E+05 

2.00E+05 

Power (w) 

(dB) 

50000.00 

46.99 

50000.00 

46.99 

50000.00 

46.99 

Power (w) 

(dB) 

50000.00 

46.99 

50000.00 

46.99 

50000.00 

46.99 

Antenna Gain 

(dB) 

1000.00 

30.00 

1000.00 

30.00 

1000.00 

30.00 

Antenna Gain 

(dB) 

1000.00 

30.00 

1000.00 

30.00 

1000.00 

30.00 

Frequency (GHz) 
Wavelength (m) 

2000.00 

0.15 

2000.00 

0.15 

2000.00 

0.15 

Frequency (GHz) 
Wavelength (m) 

2000.00 

0.15 

2000.00 

0.15 

2000.00 

0.15 

ntenna Aperature (sqm) 

1.79 

1.79 

1.79 

Antenna Aperature (sqm) 

1.79 

1.79 

1.79 

adar Cross Section (sqm) 

25.00 

25.00 

25.00 

Radar Cross Section (sqm) 

50.00 

50.00 

50.00 

imperature (Centigrade) 
(Kelvin) 

17.00 

290.00 

17.00 

290.00 

17.00 

290.00 

Temperature (Centigrade) 
(Kelvin) 

17.00 

290.00 

17.00 

290.00 

17.00 

290.00 

iquency Bandwidth (MHz) 

2.00 

2.00 

2.00 

Frequency Bandwidth (MHz) 

2.00 

2.00 

2.00 

Noise Factor 

1.21 

1.21 

1.21 

Noise Factor 

1.21 

1.21 

1.21 

ve tor Slgnal-to-Nolse (dB) 

23.71 

11.66 

-0.38 

Solve for Slgnal-to-Nolse (dB) 

26.72 

14.67 

2.63 


Slgnal-to-Nolse Computation 
Radar Cross Section SO 


Variables Defined 
Pt = Power Transmitted 
G = Antenna Gain 
Ae = Antenna Aperature 
o = Radar Cross Section 
K = Botzman’s constant (1.38E-23) 
T, = Temperature (Kelvin) 

B = Frequency bandwidth 
Fn = Noise Factor 


3-15 





To convert the final answer into decibels (dB) I took the log of the result of the equation and then 
multiplied by 10. After getting the spreadsheet set up, I solved an actual problem. 

Conclusion 

Each of the programs that I evaluated can perform multiple functions, making them very 
useful in the office. Not all applications are as limited as the ones that I used. I worked with the 
basics mostly. I really didn’t get too in depth with any one program, though some projects were more 
in depth than others. Working with programs wasn’t all I did during my tour. Sure, most of the time 
I was on a computer, but there were times I wasn’t. During my tour, I also briefly used a digital 
spectrum analyzer and oscilloscope to observe different types of signals. I did little things like this 
throughout my tour, while doing most of my work on the computer. I would do little things to help 
out, such as install different programs. While here I installed the DOS6 upgrade and WordPerfect for 
Windows which most of this report is done on. This report goes over my work with the different 
programs, but there is no way to sum up all the things I’ve learned during my tour. I came into this 
program with the feeling that I wanted to be an engineer, and after experiencing it firsthand, I know I 
want to become an engineer. I’m not totally positive, but I think I will try and major in electrical 
engineering. As for the communications end. I’ll make that decision in the future after seeing more of 
my options. 

References 

Microsoft Visual Basic Programmer’s Guide, 1991, Microsoft Corporation 


3-16 







THE CHANCE OF A LIFETIME 


Jennifer A. Starr 
student 

Trotwood Madison Sr. High 


Final Report for: AFOSR Summer Research Program 


Wright Laboratory 


Sponsored by: Air Force Office of Scientific Research 


Boiling Air Force Base, Washington, D C. 


July 1993 


4-1 





CHANCE OF A LIFETIME 


Jennifer A Starr 
student 

Trotwood Madison Sr. High 


This report will explain the knowledge obtained, while working at Wright Patterson Air 
Force Base. It will cover computer operation, hypermedia ,and various computer applications In 
addition, this report will discuss my experience with Internet. Examples of the work done in the 
past eight weeks will be shown. 


4-2 






A CHANCE OF A LIFETIME 
Jennifer Starr 

INTRODUCTION 

"Opportunity is often difficult to recognize; we usually expect it to beckon us with beepers 
and billboards " This is a quote by William Arthur Ward which I can relate to I was given the 
opportunity to learn, in eight weeks, about computer engineering This was a great opportunity, 
because I was exposed to an area of interest to me. During the summer, I learned about 
computer hardware and software. In addition, I learned how to program in Ada Overall, this job 
enabled me to gain a better understanding of computers and programming. It gave me an chance 
to learn about computers though hands on experience. 

DISCUSSION 

Before coming to Wright Patterson Air Force Base, I knew nothing about programming and 
very little about computers. Over the course of time, I learned a great deal. 

Programming is defined as the logical decomposition of a problem into a set of procedures for 
execution on some computer structure in order to generate a solution to the initial problem. The 
computer is given logical instructions in the correct order using a programming language. It took 
me a while to understand and practice this. 

Ada is a high-level programming language. In other words, it is a notation that is intended to 
be natural and convenient for writing and reading computer programs. Ada is a blend of 
mathematical notation with English words and phrases that are used with precise meanings. In 


4-3 






this respect it is similar to other high-level languages such as Fortran and Pascal. However, it is 
different from low-level machine languages and assembly languages, which require programs to 
be written in terms of the instruction sets and registers. Ada has three units: packages, 
procedures, and tasks. An Ada package contains a specification and a body Generally, the 
specification identifies what procedures and functions are in the package. The body contains the 
actual code for the procedures and functions identified in the specification. My knowledge about 
programming has expanded more than ever It shocked and pleased me the first time a program 
was written by myself without the help of my mentor. It made me feel good to know that I was 
beginning to comprehend programming. 

Unlike programming, the computer was easier to understand. Several terms were discussed in 
the process of learning about the computer The difference between single and multi-user 
systems was learned. A single-user system, allows one user to access the computer at the same 
time. On the other hand, a multi-user system is such that multiple users can access the 
computer simultaneously. The operating system is another term ihat was learned. The operating 
system is a set of functions that translate user requests to operations that can be performed by the 
computer. For example, the PC operating system is DOS and the VAX operating system is 
VMS. The appendix following the report has some sample programs that I wrote after learning 
how to program. 

Another part of training was dedicated to learning to use word processing software. The 
word processor used was Ami Pro. In fact. Ami Pro was used to create this report Ami Pro has 
a spelling checker which allows the author to correct misspelled words. In addition, it also has a 







grammar checker which assists in identifying grammatical errors and supply suggestions on 
corrections It also has a diverse variety of styles in which the text can be written Ami Pro also 
has graphical capabilities which allows the user to create pie, line, and bar graphs Figures 2 & 3 
contain examples of pie and bar graphs 

During the summer, I was also exposed to a technology known as hypermedia. Hypermedia is 

r \ 


Reducin g the de ficit 




r 


\ 

a way of combining textual, audio, video, and graphical information for presentation. A popcorn 
popper manual was used as an example of hypermedia. 



4-5 










The creation of a hypermedia popcorn popper manual was done in several steps The first step 
was to enter the text and graphics into the computer, using a text editor and scanner The manual 
was then "marked up" according to a predefined mark up language For example, the manual was 
separated by the main headings and subheadings by using "cl." and " c2 " respectively 
Afterwards, a document description file which contains summary about the documents that make 
up the library, was created. It is mandatory that a document exists for each description If a 
document is removed the document description must also be removed. Next, the document 
loader was executed. The document loader is the application that loads documents into the 
"document project." The "Document Project" is a database that stores documents, figures, tables, 
movies, and audio. The next step was to import the document description and the document 
This enables them to be located anywhere on the hard disk. After that, the document project was 
put into the index. Lastly, the reference system was updated by adding the document to the 
library menu. 

Finally, this summer I learned about a computer network called Internet Internet is the world's 
largest computer network. On this network numerous computers can connect and communicate 
with each other. All the networks, using the IP protocol, cooperate to form a seamless network 
for their collective users, this includes federal, regional, campus, and some foreign networks. All 
these networks put together are only part of what makes up the Internet because now of *r 
non-IP networks are connected. There is no single authority figure for the Internet as a whole. 
Everyone pays for their part of the Internet. It was interesting how a message can be sent around 
the world on this network and then receive a reply from anyone that is connected to the network 


4-6 






Several years after the Internet was implemented, a tool called Archie was developed to help 
users search for information Archie is a system which allows searching of file indexes to locate 
publicly available information. The appendix contains an example of the use of Archie 
CONCLUSION 

A great deal of new knowledge was obtained while working at Wright Patterson. I still plan to 
learn how to program graphics in the future. I enjoyed spending my summer at Wright 
Laboratory It was an experience I'll never forget. The most important thing I learned while 
working here is that when opportunities like this come knocking at the door, let them in and 
appreciate them because if you don't they'll pass you by 


4-7 






Appendix 


JS>r average 

enter the number for a—> 30.0 
another number for a —>45.0 
another number for a —>48.0 
another number for a ->-999.0 
The average is --> 41.000 

JS>r square 
enter a_number --> 

34.0 

1156.000 

1.15600E+03 

JS>r square 
enter a_number --> 

6.0 

36.000 

3.60000E+01 

JS>r velocity 

enter a number for d-->200.0 
enter a number for t~>4.0 
velocity is —> 50.000 





> prog ada 


oat cac .Washington. adu (140.142.100.1) 
aat updated 06:57 6 Jul 1993 

Location: /local/emacs-18.57/liap 

FILE -rw-rw-r— 19512 bytes 00:00 9 Apr 1988 ada.si 


oat cac.washington.edu (140.142.100.1) 

sat updated 06:57 6 Jul 1993 

Location: /local/emacs-18.57/lisp 

FILE -rw-rw-r— 16382 bytes 01:00 9 Jan 1991 ada.elc 

•i* 

oat cac.washington.edu (140.142.100.1) 

tat updated 06:57 6 Jul 1993 

Location: /local/emacs.pmax4.0/lisp 

FILE -rw-r—r— 19512 bytes 00:00 9 Apr 1988 ada.el 


>at cac.washington.edu (140.142.100.1) 
ist updated 06:57 6 Jul 1993 

Location: /local/emacs.pmax4.0/lisp 

FILE -rw-r—r— 16382 bytes 00:00 11 Apr 1988 ada.elc 


»at cac.washington.edu (140.142.100.1) 
ist updated 06:57 6 Jul 1993 

Location: /local/emacs.sun386/lisp 

FILE -rw-rw-r— 19512 bytes 00:00 9 Apr 1988 ada.el 


•at cac.washington.edu (140.142.100.1) 

.st updated 06:57 6 Jul 1993 

Location: /local/epoch-next/lisp 

FILE -rw-r—r— 19512 bytes 00:00 16 Oct 1990 ada.el 


st cac.washington.edu (140.142.100.1) 
st updated 06:57 6 Jul 1993 

Location: /local/epoch-pmax/lisp 

FILE -rw-r—r— 19512 bytes 00:00 16 Oct 1990 ada.el 


4-9 


®JWT AVA ILABLE fD qqq DOES 


nnat mi? legible REpRODucrrair 







Comparison of the Rojos of Mean for t>« Baseuni Non-exhaustive Search Mithoos 

VERSUS THS Number of Partttons 

A Study in Pattern Theory 


Johnny R. West, Jr. 
Student 


Belmont High School 
2323 Mepleview Ave. 
Qeyton, Ohio 45420 


Rnel Report for. 

High School Apprenticeship Program 
Wright Laboratory / AART-2 


Sponaoredbyt 

Air Force Office of Scientific Research 
Wright-Patterson Air Force Base, Dayton, Ohio 


August 1993 


5-1 






COMPARSON OP TW FIGURES OP MEUrr FOR TW BASOJNE NON-KHAU9TWE SEARCH METHODS 

VImwtw Number op Partitions 
a Study In Pattern TWory 


Johnny R. Wot, Jr. 
Student 

Belmont High School 
Dayton, Oh. 


ABSTRACT 

The recent advancements In machine teaming theory, computational complexity, and 
logic minimization has lead to a new study in the world of computer sdence referred to as Pattern 
Theory. This is the study of how computers recognize patterns and, more Importantly, the study 
of "pattem-ness." Through yean of extensive research and experimentation, • robust Occam 
bated pattern recognition and learning has been developed. However, this program is not as fast 
as it needs to be, nor is it as convenient as it made to be for application to "real workf problems. 
Currently, these problems are being r ese arched . 


5-2 





PATTERN THEORY MEMO- Augurt 5,1993 
Johnny R. Wot, Jr. 

High School Apprenticeship Projpam of Research and Development Lab and the Air Force Office 
of Scientific Research for WL/AART-2 

Comfambon QFTBK PlQUKB OF hiEUI pomtbb Baseline 
Non-bhausttve Swch Methods VSR SOT 
THxNuMMtxarPjjamoNs 

LA BACKGROUND 

The Pattern Theory Paradigm is the result of a recent convergence of ideas from logic 
mlnhnirarion, compatadoosl complexity, sad machine learning theory V It is a promising, new approach 
to robust pattern finding. One of the keys to this robostness is the ability to decompose, or break op, a 
fanction into smaller parts for use. The basic test for doing this is c*n*A Asbeobarst-Cortis 
Decomposition. In the late 1930‘s, R- L. A«henhnm pnhHiheri tho heric test ft" HulMiwtning ^tithfr ^ 
not a binary function decompose*. In particular, the test tells os whether or not P and O exists so that a 
binary fimctUm/can be represented by a composition of F and In other words, is there a truth to the 
representation: 

TMa teat became the bade for considerable new work in logic mirdmiaatton. We refer to the partition of 
f% inputs (or vadahlec) Into irgrats for • and inputs foe F at rimpiy a "partition" or a "variable partition." 

The result of doing a partition test on any one partition never changes. However, the question of 
how to arrive at "good" partitions, or a partition search strategy, is not as easy to find. By "good" we 
mean partitions that decompose to optimal The partition space, or simply the number of paxtidoM, is 
ecpiivaler* to x> partitions, where x is the number of groups in which each iiqxit may be placed, and y is 
foe number of variables, or isgrata. The variable x is dependent on whether or not "ahared" variables are 
being need. The set of groups {O, F) is called a ’disjoint” set beesase the input most be in either group 
• or hi groop F. This gives x a value of 2. However, the set of groups (4b, F, 4> and F) repre se nts 
"riMtetf'variables, since any input can be in both 9 and in group F. Thii wooklglvexa value of 3. So, 
for a ftmetion with eight inputs the partition ipace is 2®, 256, or there are 256 ffigfdnt partitions to search 
through. The m et h ods used to find a good partition were exhaustive, meaning they performed foe 
decomposition test on every partition throughout foe hierarchy. This test is impractical of foe 

large amounts of time sad memory needed to nm the test thoroughly. 

In order to compensate for foeee problems, new search strategies have been devised. Theee 
strategies, or decompos i t i o n plane, selectively perform decomposition testa. They are performed to find 

1 Timothy D. Boas, Michael J. Novixkey, Timothy N. Taylor, and David A. Gadd. Pattern Theory: An 
Engineering Paradigm for Algorithm Design. Final Technical Report WL-TR-91-1060, Wright 
Laboratory, USAF, WL/AART, WPAFB, OH 45433-6543, Aagest 1991. 

5-3 




the optimal parddona, dM parddona that give tha beat decompoaidon, with eaae. TUi aavaa tfaaa and 
memory. 

Id order to oaa any atrategy, a measurement of the "weOnaea" of a partition moat be evaluated. 
Theta axe many waya of arriving at tfaeaa figure# of merit, bat which way ia the beat? Which way win be 
the futeat and the moat accurate? How much a p ee d ia given up for a more accurate meaaure? What 
truly ia the moat accurate measure? 

Thepurpoaea mwty m? (1) aupptyinfbnnadou tn nun nf 

child cerrHnaUdea (SOCQ or the am of grandchild canbnaHtiea (SOOCQ ia a better figure of merit 
(POM) for aelecting partition# during the decompoaition proceea and; (2) for each POM, determine the 
beat atrategy for getting die partitioa to be evaluated. TMa will help to determine the futeat and moat 
effi ci ent way to find partition# with the ndrdmuoi decompoped flmction cardinality (DFC). 


5*4 







10 DEFINITIONS 


The Aljoritbma: 


Tbe Inaearing Row to Cohnnn Ratio Algorithm meant that the partitions wen choeen for 
evaluation in aaqnential order, muting whit one raw x aeveu colnmna and ending with oeven rowa x one 
column and chooaing all pertitkxu with i tow variables before chooatng any paititiona with M tow 
vtrithloa 


The Decreaaing Row to Coinmn Ratio Algorithm meana that the pazdtiana were choeen for 
evaluation in sequential order oppoaite IRC, itaiting with teven rowa x one column and ending with one 
row and aeven columns. 


The Random Algorithm meana that the paititiona were not aelected sequentially. They wen 
aelected randomly with replacement. With zeplacement meana that after a paithion wu aelected, it coold 
be aelected again, hi example, if yon have three red balla and two bine balk in a beg and randomly take 
one out, than yon toot whatever ball yon have aaide and draw another, that would be random without 
replacement. Aa yon kept drawing, yoa would eventnelly get all the baDa oat of the beg. However, in 
random with replacement, yon woold have replaced the bell and, yoo conld after five tries, w i thdraw the 
•ante ball five times. 

Cardinality: 

Ca rdinal i ty ia the number of elementi in a set Far fanctiona (• eat of ordered pahs) it la the 
possible vetoes raised to tbe power of the nomber of input*. For example, for the fonedoa pictnmd on 
tbe next page: 


5-5 





*1 



— 

1 

t 9 . _1 

N 

<1 

oooa 




„$(1 < x2 # ...jcn) 


tfaegob-&acdoa^h«f «cud^ttlity<tf2 3 , or 8, wfaOflfcb-faoctkiiFku ji cardinality of 2^, or 16, if nd 
ooiy if tfae iopoti to the fooctioa ere til disjoint and tro ill represented in the drawing (Le. *»r»3). 

Tfae Sam of Child CtnttntKty of § ftmedoo is tfae aam of the ctnfantHries of the wb- ft metioos 
of tfttnedon after fin^le*Bldecoaq>ocition. In (be above fnnedoo, the ram would be 24. 

The Son of Grandchild Ctnflnality of t ftmedon is tho mm of (be citBnaliti— of tfae nib* 
nf fnhJnnrtkvtm rtf » ftmrrinfi fmenttUtwl dOCOmpOSidOQ ). TfaU is OOt pictured in ( fa * SfaOVO 

fcneriwi 


A Hgure of Merit is a wqjr of determining the vilne of a partition, e.g. SOCC tnd SOGCC 

Tw«iifM «a Rmction Cardinality is tfae fignre of merit topored when t ftnctioa is 
decomposed. 


5-6 







AshenhantrCords Decomposition la a way of breaking down a ftmctioo into gntllcr para. The 
taction pictined above can be written as f(yl, y2, ye, *1, z2, ir). This eqoetioo ia too latfe in the sense 
of digital cricnit design. It has a cardinality of 2* or 64. In other woods, it takes 64 bits of memory to 
atom this taction. If it is decomposed, it becomes f(F(®(yl, y2, ys), *1, z2 or)). While it is mom 
dfUcy l t faf iv imam mi»i to comprehend, it hu a mall cardinality of 2^ + 2* - 8 + 16 ■ 24. By 
decomposing the taction, 40 bits of memoiy has been saved. This is just one example of wfaat taction 
decomposition is capable of Them are many other applications. 





Hat the partition aslecdon algorithms ww a r a min e d 


XLL Ordar of Partition Selection 

In tfaaaa axpadmaats, we wen concerned with tbay order in which foe paztitiaQa wen evaluated. 
Specifically, wawm concerned with the leaaha of examtnlDf the partitions in a particolar order baaed on 
tfat numbers of row and colamn variahlaa in a partition. 

We used FLASH, tfae /’'unction Learning And Synthesis Hofatd (a program developed under the 
Wright Lab Ptfttm Theory Project) to gaoaniD tfae data using the Increasing Row to Colamn Ratio (IRC) 
algattesa for the SOCC and SOGCC for tba aet of 30 functions referred to as tbe Benchmark Set of 
Rmctione. However, we oaad only 27 out of the fbit 30 Benchmark Rmcdons In this atndy. Tbe three 
exdoded ftmetiona did not decompose aod ther efo re would anpply no i n fo rmation to foe study. Thaae 
ftmctions were binary with eight inputs, ao foe omnber of partitions that we evaluated 256 partitions far 
eachfonction. 

We aajaired tba beat FOM for SOOC and SOQCC for each partition by naing foe data aopplied 
by FLASH. The POM for partition n ia foe FOM for foe «+i** paztllioo looked at For example, tfae 
FOM for partition five simply meant foal at foe forth partition looked at by the dacompoaitlon plan, foe 
FOM waaX,hasping in mind that partition one actoaOy oeems before any partition axe looked at The 
beat FOM ia acquired fay looking t* the Agates of merit in order and dn t niminin g if it ia lower than foe 
previooe beat If it is better * "old beat" it becomes foe "new beat” H, however, it ia not better 
than tfae old beat, foe IRC data foe foe 5^ partition looked at become foe value of the old beat hi 
pseudocode: 
begin 
x-0 
beat-999 
repeat 
x-x+1; 

gtt basic data for partition x> 
ifbtric_data <best 


best.*-basicjdata; 

IRC_dataJbrjs^jtartitionJookedjd ?■ beat; 
until x - nmaberjtiLpoeaiblejvaloeaiJbrjrecfaJnpat aaaber_o<Jnpttti 

and. 


5*8 






For example, if fee original, or baaic, POM dim far the flat five partitions uk 


PARTITION NUMBER 


POM 


1 256 

2 a 

3 124 

4 28 

3 236 

the IRC date, or "bon FOM ao fta" data, for each partition would be: 

PARTITION NUMBER FOM 


1 

2 

3 

4 
3 


236 

48 

48 

28 

28 


Since this proem is tedious and slow (considering 236 partittoos on 27 fimedons and two flfinea of 
merit), we developed a simple yet efficient Microsoft QBASIC program to do this p roe m . We then 
graphed this data using Microsoft Excel and saved tfae dusts for flam e use. 

We then creeled tfae Decreasing Row to Cohnan Rsdo (DRC) Algorithm date. To do this, we 
reversed the original FOM date and evaluated it similaiiy to the IRC data. Using the original FOM data: 


PARTITION NUMBER 


FOM 


1 

2 

3 

4 
3 


236 

48 

124 

28 

236 


3-9 





PARTITION NUMBER 


1 

2 

3 

4 
3 


236 

28 

124 

48 

236 


HMD entailed the date as seen If tbs partitions *&• looked at in teveae Older as the DRC data: 


PARTITION NUMBER POM 


1 236 

2 28 

3 28 

4 28 

3 28 


ftving in the necessary DRC data. We again dev el oped a tot Mtomoft QBA5IC progma to do tbs 
necessary date manipulation. We ti*B plotted thii data with Microsoft Excel and stored this for Artoi s 
use. Keep in mind that >U the above dau is partly fictitious ml is ben oaly in the hopes tbit it will blip 
the reader on de m a n d the proced ni ea involved 


3-10 




3 ,L2 Imadeni Selection 


The data for tha Random (RAND) Algorithm was not u eery to generate. The objective hem 
vre* to aynthaaim what would happen if we went through the partitions in a purely random order, as 
oppoeed to IRC and DRC We ooold have done many random aampUnga and averaged it However, we 
wanted a more direct and accurate approach. Thia approach called tor acme experimental bade, aa noted 
in the procedure that follows. Hat, we generated experimental data in the form of hfetograma with 
FLASH. A hiatogram might have looked similar to: 

POM Number of Oocarrencea 


16 6 

8 1 

4 1 

2 1 

1 3 


We generated histogram inch aa theaa tor each of tha 27 Benchmark Hmcdona being evaluated in both 
tha SOCC and SOOGC formate. Again, the above hiatogram ia parely fictitioua and ii only here for the 
benefit of die leader: To thia point, the data worked with waa purely experimental, meaning we 
generated it oaing FLASH. The data plotted, however, is theo r etical data, baaed on expe rime nta lly 
generate histograms, aa obtained tiring the procedure that follows. 


James F. Ftenxel, a professor in the Department erf Electrical Engineering from the Uofreraity of 
Idaho, supplied the following theory to obtain (be data needed: 

Awning n partitions am befog looked at, the probability that a certain figure of merit ia tha beat Agaze 
of merit is to tbs p ow e r of the figure of merit's occ ur r en c e s plus the number of occurrences of 
better figures of merit divided by the total number of partitions minus the probability that any better 
figure of merit is the beat figuzeof merit The expected best (theoretical) figure of merit would than be 
the sum of each probability multiplied by its respective figure of merit 


3-11 





We derived lb* folkroing formulas from this theory tod tbe table (keep in mind that the variable n 
|^| m an frtf o f** 1 * pn titicB looked it in ill chm): 


POM Number of Occurrences 


16 6 

8 1 

4 1 

2 S 

1 3 


probability 16 is beet POM ) - ( 6 /16)° - 0 
probrtility(8i*be«POM)-(7/16) 0 -(6/16) n 
probability 4 le beat POM ) - ( 8 /16 P - (7 /16)« 
proMbOlcyC 2 ia beet POM ) - (13 /16 )P - ( 8 /16 V 
probebOity( lie best POM) *(16/16)°-(13/16 y 1 


expected beet( partition n )■ probability 16 ia beat POM )* 16 + probability 8 ia beet POM )• 8 ♦ 

probability 4 ie beat POM ) • 4+probability 2 is bast FOM ) • 2+ 
probability lie beat POM ) * 1 


In tbs 3 s * equation, probability Is tbe fancdon, 4 is tbe figure of merit teed from tbe chart, 8 ii the sum of 
the oocomncea to that point ( 6 ♦1 +1), 16 is dm total number of oocmnoces ( 6 +1 ♦ 1 + 5 + 3), n is 
the number of part i tio n! being looked at Qn the ease of this study 1. .256) and, 7 is the number of 
occanencea of figures of merit better than tbe currant flguse of merit (6+1). 

So, tbe theoretical figure of merit of tbe fifth partition ( n ■ 3 ) of tide fimcdon, assuming tide fonction is 
represented comedy by die abort Mstogm, would be: 

probability 16 is beat FCM)* .0074137715-0 *.0074157713 
probability 8 is beat FOM) - .0160284042 • .0074157715 * .0086126292 
probability 4 is bast POM) - .03125 • .0160284042 «.0152215958 
probability 2 is bast POM ) - .354092398 • .03125 - .322842598 
probability l is bast FOM) -1 • .354092598 * .645907402 
expected best( partition 5 )-1540032359 


5-12 






By WTditeg this theory into a Microsoft QBASIC program and supplying it with the proper hutegrimi, we 
were able to generate all the RAND data points. Than we graphed the data with Microsoft fixed «nd 
stored for ftkue use. FImm note tte these RAND datapointa an atziedy theoretical. We obtained none 
of tfaa RAND data poiata by udng FLASH. 

To make tfaa tnoda in tba data mon compatible between fimetioos of different DFCs, we 
normalised all the dent by naiag AxtdTa normalization twchntqoa. which la tba following: 

Por aach partition in each Auction: 

noraiaUaad(POM)*((2S6-POM)/(236«LoiraatPOMofltortk»)) 

TUa brings eve ry t hin g into tfaa aama acale. Now all tfaa numbers become a decimal representation of leaa 
than or equal to one whose negative la indicative of increase on tfaa cawBnality of a decompodtioo, one ia 
100% dscompodtion. aero la 0% decompodtioo. The lowoat POM of tfaa Auction ia the lowest known 
POM (or minhnnm DPC) over tfaa o utli e Amotion. TUa normalization technique allows os to make 
dedaioos baaed on tfaa graph of tfaa data. Aa a rule, by tfaa time we reach the last partitioo, we reach the 
lowest POM. We label thaee oew values aa percentage of optimal, optimal being the bast POM that this 
particular ftmedon can have. 

U Figure ef Marfa Kxperimsota 

The next logical step was to Mod? the figures of mark themselves. In order to dete rmin e which 
partition! ware the best, a good POM had to be established. To compare them, several testa were ran. 

&&1 Percent of Optimal 

In this mrpurittuait, for a trb fared"" we compared the POM values of each partition with what 
we knew to be as "true*, or best known optimal. TUa gave os a good idea of wfaich partitions were the 
best to look at 

In order for tUa experiment to wotic, a set of optimal! had to be established. We compiled a Mat 

of true optimal! from all the decongtodtion we had including, but not limited to: Child 

3-13 






Cardinality, Grandchild Cardinality, AJPD. (Ada fraction Decomposer), and manually. Than, we 
mart the petition data fron PLASH*! out put files. Using Axtell'a doth aHration tedinkps. we 
acalsd the data and plottad it with Microsoft Bzoal for comparison. TMa waa dona for the lint 27 
fowcriftqf in the *-“*"»•* Sat Tha data poitaad to certain petitions being optimal mat* fteqaeady 
chan others. In orde to better itody this phenomenon, another test waa devised. 


3.2J Optimality Counts 


The bast way to aea which p a rtiti ons occoned mo* ft cqncndy waa to actnally count tfaa nombar 
at times that each partition was optimal in all Auctions. This way, any trends coold be noted and stnded 
Author. To axttapoiata tfaa optimal partitions from each tonedon, a Microsoft QBASIC program waa 
developed. Tfais program fare ns a listing of exactly wfaiefa partitions were optimal in each faoedon. To 
coimt die number of times they o cc urr e d throughout til the tooedone, another Microsoft QBASIC 
program wu developed Wbat we fotmd ia that while certain blocka of paxtitioos axe never optimal over 
the 27 ftmertona, certain partitions that occur near (within three partitions of) or, in some cases, directly 
after a major event far the way a pettitioa is evaluated, tend to have a very high fteqoeocy of optimality. 
TVs means that certain events may ense, or at least atoirxflcsdve of optimal parddone. This wouldgive 
os a way to "predict" the optimality of a partition, or at least know where a good starting point tor our 
aearch is. However, this is only theory and we have no evidence to bade it 19 at tfais time. A outfor 
cause of thla phenomenon may be our choice of tonedona tor the Benchmark Sat. These tacts wen" ran 
tor data geaetatad fay both the SOCC and tha SOQCC algorithms. To better visaaHx* the results, the data 
was plottad using Microsoft fixed. 


The neat map waa to determine to correlation of the optimal parddons from child canfinality and 
the optimal partitions from grandchi ld cardinality. Using the data files of optimal paidtions from the last 
experiment, a Microsoft QBASIC prognm wu developed to determine the condition between these two 
•eta. Specifically, it toond all of die matches to die child cardinality optimal sat in tha grandchild 


S-14 




caidtaality optimal aet, and floaod ill of the matches to the grandchild cardinality optimal aet in tbs child 
optimal set An wpimtioo of lb* printout: 


3A4Lwtfiib«fOpteali 


ta an attempt to father atody tba optimal partition aata of a faction, several attampM Man 
made to datannina tfaa laaat aat« or naaDaat aet. of optimal paititiona that woold repmeaot every ftmctian. 
Non Potynomial, or NP fancdona am claaaHlort aa factions wham tba tima it talma to find a rotation 
increase* faster tban polynomial aa ftmctjonpunber of input variables). TUa problem, however, is 
chsaHlwd aa "NP hard”, -hicb means that a fiut aotadon to tfaia problem is probably impossible. The 
problem of aet coverage ia daaaiflad u NP bard. After many failed attampM to jet a perfect sat of 
optimal partitions, a "bitty perfect", or "near perfect”, aat wu compiled. 

The ita attempt at (tfc problem was a basic "search sod deatroy” algorithm. It s ear chad through 
an array of all the optimal pertitiona for the 27 factions, and triad to acconot for the ftuicfiotia with die 
partition that occanod moat ftcqueody by marrhing from the first fraction to the last fared on. skipping 
over any already accounted for Auction. It heaps processing Iflm that nodi all of die fbncdona hare bean 
accounted for. Unfortunately, certain degree of randomness occora in this algorithm. Enough so, if ihe 
program ia redesigned to start at the last faction and and with the first faction, the an swer set is 
dfflbront This was not sodroly unacceptable, but (fid not provide the sbsohits best set. either. 

While examining tbs data, we rfiscovered that the optimal partitions of e certain factions were 
w w ot ox oujer «d cnona. idno wmw wot w omu xor tat dwi b^wiimol a pfoooo-vooc 
lepreaentatian of the program is on tba next page. 


5-15 







program baQttMnpt; 

begin 

read in FNX Partition array , 

doos-frlse 
while not dons do 
begin 

find largest valid function 1 , 

nput 

search other valid functions for subeetr, 
if rob—tjfbond ■ troe then eliminate subset 
else f 0(0 next function 
nodi oot_o<Lftmctiooo ■ true; 
invalidate largest valid function; 
mark it as a base function; 
if nojraII4JbactUniJeft ■ tnw din 


•o* 

determine monber of occurrences of partitions in base functions 
select optimal set; 
write optimal set; 

and. 


As the niiif of the program might <««««■*«, the experiment wts onmccessfbL The program 
gsve as e set of fbar optimal partitions. This, howev er, wss not the answer we wrre trying to teach. The 
program selected the best set from the s obeeta. Whetess all tbs subsets men represented, not all of the 
Auctions ware rep r esen t ed. Keep in mind that (2,3 } is a subset of the major sat { 2,3,3,8 }. bat if 5 
ocean the most in the nugor sets, tbs subset, in this case, will not be represented 

The idee then came to try to "cross-reference" the original data and tbs new data. The theory 
was that by first eliminating what it ccnld by subset, than determining the best sat from the r e m ai n in g 
unrepresented sobsets, a better optimal sot could be compiled. However, the m eri tin g sat was no batter 
than the original. The problem is to be studied Anther later. 


5-16 





4J CONCLUSIONS 


The initial results of this study indicaw that the IRC algorithm ii deflniwly the baft of the time 
baaaline Don-exhaustive aeaxcb methods. However, it is not dear which of tfae figures of merit (SOCC 
and SOOCC) should be need to determine the fitness of a partition. At flat (lance, it aeema that 
grandchild cardinality is a safe assumption as tfae beat However, 70.6% of tbe time the partitions that are 
optimal cfaUd cardinality are optimal for cardinality. This vjold prove to make cfaQd 

cardinality dominant considering that grandchild cardinality takes a much longer time to process than it 
takes to use chQd cardinality. lUs is only speculation and at this time them is no evidence to back it op. 

Theae recalls may be anecdotal -biased by the figures of merit, or fonedoos in the Benchmark 
Set Additional experimental and theoretical work is necessary before the results presented here can be 
coocideted conclusive. 


5-17 






6-1 


/ 





Interpolation in Load-Pull Measurement 
System's Power Device Characterization 


Nicholas T. Campanile 
Beavercreek High School 
University of Cincinnati 
Cincinnati, OH 

Lois Kehias 
Wright Laboratory 
Wright Patterson AFB, OH 

Final Report for: 

Summer Research Program 
Wright Laboratory 

Sponsored by: 

Air Force Office of Summer Research 
Wright Patterson Air Force Base, Dayton, OH 

August 1993 


7-1 



INTERPOLATION IN LOAD-PULL MEASUREMENT SYSTEM'S 
POWER DEVICE CHARACTERIZATION 

Nicholas T. Campanile 
Beavercreek High School 
WL/ELM 

Abstract 

The primary summer focus was on the analysis of a load amplifier and the 
measurement of impedance's at various tuner states. Taking actual 
measurements is a time consuming and inefficient process for the professional 
world. Therefore, it is necessary to consider that an amount of values is 
accurate without actually measuring them but by effectively gathering data by 
interpolation. EEsof's commercial package Anacat allows for such a program to 
be written while storing the newly created data in recallable files. 


7-2 





INTERPOLATION IN LOAD-PULL MEASUREMENT SYSTEM’S 


POWER DEVICE CHARACTERIZATION 


Nicholas T. Campanile 


INTRODUCTION: 

Accurate power device characterization is critical in achieving high 
yields and design success with few design/fabrication iterations. However, in 
the early history of microwave transistor power amplifiers, the amplifier had 
to be disassembled so that the experimentally determined source and load 
reflection coefficients could be measured. Load pull measurements alleviated 
this problem by allowing for impedance measurements which did not require 
amplifier disassembly. Below is a load pull system setup: 



BLOCK DIAGRAM OF SCALAR POWER SET-UP FOR LOAD-PULL MEASUREMENTS 
(FOR POWER DEVICE LARGE-SIGNAL CHARACTERIZATION) 


7-3 





















Large signal device modeling and load/source pull measurements are two 


approaches presently being used to meet power amplifier performance goals. 
This task focused on software and system development of a passive source/load 
pull measurement system which is used to perform microwave scalar power 
measurements on in-house (Solid State Electronics Directorate, Wright 
Laboratory) developed novel, high-power density, heterojunction bipolar 
transistors (HBT's). 

Discussion of Problem; 

Load pull measurements provide power transistor input and output 
impedance match information under large signal conditions, output power, 
gain, and power-added-efficiency contours can be mapped under various 
impedance match conditions. These contours are then used in designing input 
and output matching circuits for power amplifiers. Measurements of the S- 
parameters were taken for approximately 720 manually set impedance states. 
This accomplishment still left thousands of states uncalculated. To save 
time, the implementation of an interpolation program became a necessity. The 
tuner to be interpolated (below) has a slider, a low frequency (left) probe, 

input 











and a high frequency (right) probe. Since measurements were to be taken 
mainly at 10 gigahertz (GHz), the ideal interpolation program must interpolate 
for slider and right probe positions. EEsofs Anacat, which uses a scripting 
language similar to BASIC, was used to develop the programs necessary for 
interpolation. Below is a chart of how Anacat, the network analyzer, and the 
load pull measurement system interact to eventually give a contour plot 


(right). 



POWER DEVICE CHARACTERIZATION FLOW CHART 



EXAMPLE OF POWER CONTOUR PLOT 
(3-D itpnmn oi owpat po«r # *mou <xnp« impedance much coodu 

Methodology^. 

Upon my arrival at Wright Laboratory, the idea of a user defined 
interpolation program had not only been discussed but also started. Dr. James 
Hwang, a consultant from Lehigh University had already written two user 
script programs on Anacat to interact with stored impedance states. The first 
program was a single frequency extract program. Anacat stores data of one 
tuner setting (i.e. slider position, left probe position, right probe 
position) for a range of frequencies (usually we measured between two and 
eighteen gigahertz); however, the extract program recalls the measured data of 
one frequency through different tuner positions. Due to Anacat’s 


7-5 











setup, though, when data is displayed the tuner range is displayed under the 
frequency heading. Please note this in plotted charts. This first program is 
the basis for all other Anacat programs written by both Dr. Hwang and me; 
furthermore, this first program was a good reference for me to look at while 
trying to understand the MLF (language) file system -- I could ‘look and 
learn" as opposed to having to read the manual. Dr. Hwang's second program 
actually did interpolations for unmeasured slider positions, thus setting the 
stage for numerous other interpolation possibilities. 

Creating left or right probe interpolator programs was not too difficult 
because many of the same principles used in the slider interpolation program 
could be implemented. The recalled files and interpolation functions had to 
be revised while new angle debugging situations had to be accounted for,- but 
for the most part, no new code needed to be added. 

Quite the reverse was true while I dealt with my final program. The 
final program was to be a user defined interpolation program allowing for the 
simultaneous interpolation between right probe and slider positions to support 
measurements between tuner positions 503309 and 993333 (sli lfp rtp). Many 
questions arose in designing this program. What extra variables need to be 
added? What limitations will the program have? What is the most efficient 
way to set up the program? In order to cut down on processing time, I wrote a 
program that would call two pre- ously stored measurements and interpolate the 
positions between these measurements (i.e. 503309 & 603312 would be recalled 
to interpolate 523311). The problem with this program principle was that there 
was nothing in the interpolation subroutine to decipher diffe.ances between 
change in slider position and change in right probe position. The end result 


7-6 






was a smooch curve going from position one co position two as opposed to the 
jagged curve I was expecting. In order for the program to work correctly, 
four positions had to be considered (i.e. 503309, 503312, 603309, 603312). In 
this setup, three interpolations are done -- two are done to interpolate the 
probe position at its respective slider position and the final interpolation 
combines the two slider positions. Had the previous idea worked, it still 
would have been limited in that it couldn't jump between different tested 
values (the slider must stay in range of ten multiples and the probe must stay 
in range of three multiples). The final version had no measurement 
restrictions; however, processing time practically quadrupled due to the added 
code. See appendix I for a flowchart of program setup. 

Results; 

The final output of interpolated data from 723310 to 723311 appears like 

this: ZO “ 50. 000000 



Freq(GHz): 723310. to 783311. 

Note the straight lines are just connectors between the jagged interpolated 


7-7 






measurements. Had my original double interpolation routine been used instead. 


there would have been a smooth curve going from one box to the other-- not 
representing the actual impedances. 

After finishing a program it is important to verify that it works. I 
verified the single interpolation program by graphing the gathered magnitude 
and angle calculations for interpolated and measured states. The interpolated 
data appears at one third intervals of the measured data as expected. 

Magnitude vs. Angle for 700909 to 703309 
At frequency 3 GHz measuring S11 
Mag with interpolated values 

GR10.S2P 



1 70 1 80 1 90 200 21 0 220 230 240 

Ang 


7-8 
























































Conclusions; 


By using Anacat and the scripting program I wrote, one can quickly and 
accurately illustrate path and dir< zion of impedance states as the slider and 
right probe change positions. 

To illustrate how vital a method it is to interpolate impedances rather 
then measuring all of them, it is necessary to consider what had happened 
before I worked with the Anacat program. For use as foundation measurements, 
my mentor along with Dr. Hwang measured data from the network analyzer at 720 
different positions. This task took three straight ten hour days. After ray 
program was finished, I was able to define it to calculate about 3000 
positions in one sitting. This took an estimated four to six hours, for which 
I never once had to be at the computer! 


7-10 





Rggerences; 

Michael G. Adlerstein and Mark P. Zaitlin; 'Power Contours For Microwave 
HBTs*; Ratheon Research Division; Lexington, MA; copyright 1993. 

Ken Kotzebu; "Load Pull Methods and Applications'; Hewlett - Packard Co.; 
Santa Rosa, CA. 

M.P. Mack et. al.; 'Microwave Operation of High Power InGaP/GaAs 

Heterojunction Bipolar Transistors'; Electronics Letters; copyright 
1993; vol. 29, No. 12. 


7-11 





initialize vara. 


program procedure 


input Iraq, 
fila.atart, 
stop .step 




open output 
file & execute 
program 



ssb is greatest 
multiple of 10 

<«ss 


sea * sab + 10 
- 

convert to 2 
digit strings 



















































Appendix 2T,; 


!T0 EXTRACT SINGLE FREQUENCY DATA FROM S-PARAMETER FILES AND 
!MERGE THE SINGLE FREQUENCY DATA INTO AN MDF FILE ?.S2P 
! FREQUENCY DATA IN THE FILE WILL BE REPLACED BY TUNER POSITION SSLLRR 
• LEFT TUNER SLUG POSITIONS MUST HAVE BEEN MEASURED 

!rt pribe & SLIDER POSITIONS THAT HAVE NOT BEEN MEASURED WILL BE INTERPOLATED 
!HOWEVER, FINAL SLIDER POSITION MUST BE LESS THAN 100 

! WHEN PROMPTED YOU MAY GIVE THE SAME INITIAL AND FINAL POSITIONS TO EXTRACT 
!SINGLE POSITION DATA. IN THIS CASE GIVE ANY INCREMENTAL VALUE EXCEPT ZERO 
!MARCH 5, 1993, JAMES C.M. HWANG, LEHIGH UNIVERSITY, (215) 758-5104 
(July 1993, revised by Nick Campanile to DO THE DOUBLE INTERPOLATION 


! DATA SPECEFICATIONS AND INTIALIZE VARIABLES 


INTEGER SSA, SSB !nearest integers above or below SS 

integer RRA, RRB (nearest integers above or below RR 

REAL DELTA (difference between SS and SSB 

REAL FF (frequency in GHz 

REAL SS (current tuner slider position 

REAL LL (current left slug position 

REAL RR (current right slug position 

REAL RE11, IM11, MAG11, ANG11 (real & imaginary value,magnitude,angle for Sll 
REAL RE12, IM12, MAG12, ANG12 
REAL RE21, IM21, MAG21, ANG21 
REAL RE22, IM22. MAG22, ANG22 

REAL RE1IA, RE11B, IM11A, IM11B !s-parameters of nearest slider positions 

REAL RE12A, RE12B, IM12A, IM12B (above and below for interpolation purpose 

REAL RE21A, RE21B, IM21A, IM21B 

REAL RE22A, RE22B, IM22A, IM22B 

REAL MAG11A, MAG11B, ANG11A, ANG11B 

REAL MAG12A, MAG12B, ANG12A, ANG12B 

REAL MAG21A, MAG21B, ANG21A. ANG21B 

REAL MAG22A, MAG22B, ANG22A. ANG22B 

REAL MAG11R1,MAG11R2,ANG11R1,ANG11R2 (intermediate interpolation values 


DIM D$ (data base file name without SS.DAT 

DIM F$ (frequency in GHz 

DIM S$. SSTARTS, SSTOPS, SSTEPS 

(current, initial, final and incremental slider positions 
DIM SA$, SBS (string representations of SSA and SSB 
DIM L$, LSTARTS, LSTOPS, LSTEPS 

(current, in.-ial. final and incremental left slug positions 
DIM R$, RSTARTS, RSTOPS, RSTEPS 

(current, initial, final and incremental right slug positions 
DIM RA$, RB$ (string representations of RRA and RRB 
DIM FIS (MDF file name to save the extracted data 


USERIN 

USERIN 

USERIN 

USERIN 

USERIN 

USERIN 

USERIN 

USERIN 

USERIN 

USERIN 

USERIN 


•enter frequency _GHz", F$ 

•enter data file name _SS.DAT', D$ 

•enter initial slider position", SSTARTS 
•enter final slider positio n*, SSTOPS 
•enter slider increment*, SSTEPS 
•enter initial left slug position*, LSTARTS 
■enter final left slug position*, LSTOPS 
■enter left slug increment*, LSTEPS 
•enter initial right slug position*, RSTARTS 
•enter final right slug position*, RSTOPS 
•enter right slug increment*, RSTEPS 


(input data values 

! •50*<=SSTART$<=“100* 

(must be greater then 0 
! *09*<=LSTART$<=*33* 

(must be greater then 0 
! *09*<=RSTART$<=*33* 

(must be greater then 0 


USERIN 'save under file name _.S2P*, FIS (save file 

FIS * FIS & *.S2P" 

IF EXIST(FIS) THEN 

IF USERYN{•overwrite existing file? *) THEN 
OPEN 1, •OUTPUT*, FIS (store in exterior file 

CLOSE 1 


7-14 







A:\INTERDBL.MLF 


END IF 
END IF 

IF NOT USERYN(*0.K. to execute? *) THEN GOTO DONE (execute program 

FOR SS » VAL(SSTARTS) TO VAL(SSTOPS) STEP VAL(SSTEPS) '.open slider loop 
S$ = LJUSTS(STR$(SS) , 2) (store SS as a string 

SS 8 = 10*INT(SS/10) (initialize SSB as the greatest multiple of 10 <= SS 
IF SSB < SO THEN SSB = 50 

SBS = LJUSTS(STRS(SSB), 2) (store SSB as string 

SSA = SSB 10 (initialize SSA as the smallest multiple of 10 > SS 

IF SSA > 99 THEN SSA * 10 

SA$ = LJUSTS (STRS (SSA) , 2) (use only the first 2 digits of position 

FOR LL = VAL(LSTARTS) TO VAL(LSTOPS) STEP VAL(LSTEPS) (open left probe loop 
L$ = STRS(INT(LL) ) (store LL as a string 

IF LEN(LS) < 2 THEN L$ * *0* & L$ (concatinate *0* before a single digit # 

FOR RR = VAL(RSTARTS) TO VAL(RSTOPS) STEP VAL(RSTEPS) (open right probe loop 

R$ * STRS(INT(RR)) (store RR as a string 

IF LEN(R$) < 2 THEN R$ * *0* & R$ (concatinate “0* before a single digit # 
RRB=3*(INT(RR/3)) (initialize RRB as the greatest multiple of 3 <= RR 

IF RRB<9 THEN RRB=9 

RB$=STR$(RRB) (store RRB as a string 

IF LEN(RBS) < 2 THEN RBS* ‘0* & RBS (concatinate *0* before a single digit # 

RRA = RRB + 3 (initialize RRA as the smallest multiple of 3 > RR 

RA$ a STRS(RRA) (store RRA as a string 

IF LEN(RAS) < 2 THEN RAS= *0* & RAS (concatinate *0* before a single digit # 
IF RAS = *36* THEN RAS = *33* (keep largest number within range limit 

FF * VAL(FS) * le9 (convert gigahertz to hertz 

(DISP DS & SBS, LS, RBS 

(BREAK 

MSEL DS & SBS, LS, RBS (magnitude/angle procedure for pt BB 

FGETS11 FF, RE11B, IM11B (recall stored measurement values 

FGETS12 FF, RE12B, IM12B 

FGETS21 FF, RE21B, IM21B 

FGETS22 FF, RE22B, IM22B 

MAG11B a SQR(RE11B*RE11B+IM11B*IM11B) (distance formula to find magnitude 
ANG11B = 57.29577951*ATN(IM11B/RE11B) (arctangent to find angle 
IF RE11B < 0 THEN (quadrant selector routine 

IF IM11B < 0 THEN 

ANG11B = ANG11B - 1B0 
ELSE 

ANG11B 3 ANGUB 180 
END IF 
END IF 

MAG12B 3 SQR(RE12B*RE12B+IM12B*IM12B) 

ANG12B 3 57.29577951*ATN(IM12B/RE12B) 

MAG21B 3 SQR(RE21B*RE21B+IM21B*IM21B) 

ANG21B » 57.29577951*ATN(IM21B/RE21B) 

MAG22B 3 SQR(RE22B*RE22B+IM22B*IM22B) 

ANG22B * 57.29577951*ATN(IM22B/RE22B) 

! DISP DS 6 SBS, LS, RA$ 

I BREAK 

MSEL DS i SBS, LS, RAS (magnitude/angle procedure for pt. BA 

FGETS11 FF, RE11A, IM11A (recall stored measurement values 

FGETS12 FF, RE12A, IM12A 

FGETS21 FF, RE21A, IM21A 

FGETS22 FF, RE22A, IM22A 

MAGI 1 A s SQR(RE11A*RE11A+IM11A*IM11A) (distance formula to find magnitude 
ANG11A 3 57.29577951*ATN(IM11A/RE11A) (arctangent to find angle 
IF RE11A < 0 THEN (quadrant selector routine 

IF IM11A < 0 THEN 
ANG11A = ANG11A - 180 
ELSE 


7-15 


A:\INTERDBL.MLF 


ANG11A = ANG11A + 180 
END IF 
END IF 

MAG12A = SQR{RE12A*RE12A+IM12A*IM12A) 

ANG12A a 57.2957795l*ATN(IM12A/RE12A) 

MAG21A * SQR(RE21A * RE21A+IM21A *IM21A) 

ANG21A a 57.29577951*ATN(IM21A/RE21A) 

MAG22A = SQR(RE22A*RE22A+IM22A*IM22A) 

ANG22A = 57.29577951*ATN(IM22A/RE22A) 

DELTA * (RR-RRB)/3 !first probe interpolation 

MAG11R1 = MAG11B+DELTA*(MAG11A-MAG11B) (magnitude interpolation 

IF ANG11A < ANG11B THEN 

IF ANG11A <0 AND ABS (ANG11A-ANG11B) >180 THEN ANG11A*ANG11A+360 

END IF 

ANG11R1 = ANG11B+DELTA* (ANG11A-ANG11B) '.angle interpolation 

IF ANG11R1 > 180 THEN ANG11R1 * ANG11R1 - 360 !keep angle positive 

MAGI2 a MAG12B+DELTA*(MAG12A-MAG12B) 

ANG12 a ANG12B+DELTA*(ANG12A-ANG12B) 

MAG21 a MAG21B+DELTA*(MAG21A-MAG21B) 

ANG21 a ANG21B+DELTA*(ANG21A-ANG21B) 

MAG22 a MAG22B+DELTA*(MAG22A-MAG22B) 

ANG22 a ANG22B+DELTA*(ANG22A-ANG22B) 

!DISP D$ & SAS. L$, RBS 
!BREAK 

MSEL DS & SAS, L$, RBS !magnitude/angle procedure for pt AB 

FGETS11 FF, RE11B, IM11B !recall stored measurement values 

FGETS12 FF. RE12B, IM12B 

FGETS21 FF, RE21B, IM21B 

FGETS22 FF, RE22B, IM22B 

MAGIIE a SQR(RE11B*RE11B+IM11B*IM11B) 

ANG11B « 57.29577951*ATN(IM11B/RE11B) 

IF RE11B < 0 THEN 
IF IM11B < 0 THEN 

ANG11B a ANG11B - 180 
ELSE 

ANG11B a ANG11B + 180 
END IF 
END IF 

MAG12B a SQR(RE12B*RE12B+IM12B*IM12B) 

ANG12B a 57.29577951*ATN(IM12B/RE12B) 

MAG21B a SQR(RE21B*RE21B+IM21B*IM21B) 

ANG21B = 57.29577951*ATN(IM21B/RE21B) 

MAG22B a SQR(RE22B*RE22B+IM22B*IM22B) 

ANG22B = 57.29577951*ATN(IM22B/RE22B) 

! DISP DS & SAS, L$, RAS 
! BREAK 

M SEL DS & SAS, L$, RAS !magnitude/angle procedure for pt. AA 

FGETS11 FF, RE11A, IM11A !recall stored measurement values 

FGETS12 FF. RE12A, IM12A 

FGETS21 FF. RE21A, IM21A 

FGETS22 FF. RE22A, IM22A 

MAG11A a SQR(RE11 A*RE11A+IM11 A*IM11A) '.distance formula to find magnitude 

ANG11A « 57.29577951*ATN{IM11A/RE11A) (arctangent to find angle 

IF RE11A < 0 THEN (quadrant selector routine 

IF IM11A < 0 THEN 
ANG11A a ANG11A - 180 
ELSE 

ANG11A a ANG11A +180 
END IF 
END IF 

MAG12A * SQR(RE12A* RE12A+IM12A*IM12A) 

ANG12A a 57.29577951*ATN(IM12A/RE12A) 

7-16 


(distance formula to find magnitude 
(arctangent to find angle 
(quadrant selector routine 






A:\INTERDBL.MLF 


MAG21A = SQR(RE21A* RE21A+IM21 A* I M21A) 
ANG21A a 57.2957795l*ATN(IM21A/RE21A) 
MAG22A = SQR(RE22A*RE22A+IM22A*IM22A) 
ANG22A * 57.29577951*ATN(IM22A/RE22A) 


DELTA = (RR-RRB)/3 'second probe interpolation 

MAG11R2 = MAG11B+DELTA*(MAG11A-MAG11B) iraagnitude interpolation 

IF ANG11A < ANG11B THEN 

IF ANG11A <0 AND ABS(ANG11A-ANG11B)>180 THEN ANG11A=ANG11A*360 


ANG11R2 = AMG11B+DELTA*(ANG11A-ANG11B) !angle interpolation 

IF ANG11R2 > 0 THEN ANG11R2 = ANG11R2 - 360 ikeep angles negative 
MAGI2 * MAG12B+DELTA*(MAG12A-MAG12B) 

ANG12 a ANG12B+DELTA*(ANG12A-ANG12B) 

MAG21 = MAG21B+DELTA*(MAG21A-MAG21B) 

ANG21 = ANG21B+DELTA*(ANG21A-ANG21B) 

MAG22 = MAG22B+DELTA*(MAG22A-MAG22B) 

ANG22 a ANG22B+DELTA* (ANG22A-ANG22B) 


DELTA =1-(SS-SSB)/10 'combine interpolation 

MAG11 = MAG11R2+DELTA*(MAG11R1-MAG11R2) 'interpolate magnitude 

!IF ANG11R2 >* ANG11R1 THEN 

ANG11 = ANG11R2+DELTAMANG11R1-ANG11R2) !interpolate angles 
! ELSE 

!ANG11 = ANG11R2+DELTA*(ANG11R1+360-ANG11R2) 

j j^jj ^ Jp 

IF ANG11 > 180 THEN ANG11 » ANG11 - 360 ikeep angle range bet. -180 & 180 
MAGI2 = MAG12B+DELTA*(MAG12A-MAG12B) 

ANG12 = ANG12B+DELTA*(ANG12A-ANG12B) 

MAG21 = MAG21B+DELTA*(MAG21A-MAG21B) 

ANG21 = ANG21B+DELTA*(ANG21A-ANG21B) 

MAG22 = MAG22B-fDELTA* (MAG22A-MAG22B) 

ANG22 a ANG22B+DELTA*(ANG22A-ANG22B) 


OPEN 1 'APPEND", FIS ire-open file in append mode 

WRITE 1, S$ & L$ t R$\ !store data 

4 • • 4 STRS(MAGll) t ' ' & STR$(ANG11)\ 

& ' * & STR$(MAG12) 4 ' * t STR$(ANG12)\ 

4 ' * 4 STRS(MAG21) 4 * ' & STR${ANG21)\ 

4 ' * 4 STRS(MAG22) & * * & STR$(ANG22) 

CLOSE 1 


NEXT RR 
NEXT LL 
NEXT SS 
DONE: 
END 


! close right probe loop 
!close left probe loop 
!close slider loop 
! end program 


7-17 



CALIBRATION OF A REACTIVE ION ETCHING SYSTEM 


Chris Dodsworth 
WL/ELR 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratories 


Sponsored by: 

Air Force Office of Scientific Research 
Wright-Patterson Air Force Base 
Dayton, Ohio 


August 1993 


8-1 




Calibration of a Reactive Ion Etching System 


Chris Dodsworth 
WL/ELR 

Wright-Patterson AFB 


Abstract 

A calibration of the mass flow controllers in our Reactive Ion Etching system in 
the cieanroom was performed. The correct flow rates for five different gases at 
multiple settings was determined and graphed. In addition, the maximum flow rates 
for the gases was also determined and compared to the previous values. This new data 
will be helpful to researchers because it will enable them to perform reproducible 
etches. 

I studied circuit design for the remainder of the time. I created and attempted to 
build a traffic light controller using combinatorial logic. Although the model I built 

did not work, an analysis of the circuit design showed that the design was correct. I 
came to the conclusion that some of the circuit components used were bad. 


8-2 





CALIBRATION OF A REACTIVE ION ETCHING SYSTEM 


Chris Dodsworth 

I. Introduction 

During the term of my 1993 summer research program, I focused my efforts on 
two major projects. The first of these was the calibration of the mass flow controllers 
in our Wafr Batch Series 70 Dual Chamber Reactive Ion Etching (RIE) machine. The 
mass flow controllers regulate the flow of gas into the chamber. This is a critical 
measurement, as a small variation in gas concentration or pressure could lead to a 
non-reproducible etch. To insure that the RIE users would obtain the correct amount 
of gas flow, a series of graphs were created which correlate the percentage setting 
on the machine to the actual flow rate in standard cubic centimeters per minute 
(seem). The second major focus of my summer research term was understanding 
basic electronic devices, such as capacitors and transistors, and then utilizing that 
knowledge to build some basic circuits. This project was not as task-specific as the 
other; rather, it was designed to provide me with a basic understanding of electronics 
in preparation for college. 

II. RIE Calibration 

The Wafr Batch Series Dual Chamber 70 Reactive Ion Etching machine is used to 
etch through various layers on a wafer and is a key tool in the fabrication of 
heterojunction bipolar transistors (HBTs). The wafer is placed in an aluminum 
chamber which is evacuated to .001 torr. Gases then flow into the chamber through 
miniature holes evenly distributed at the top of the chamber. An RF generator 
creates a plasma in the chamber by breaking up the gas molecules and exciting the 
electrons in the resulting ions. These ions are pulled down toward the wafer by a DC 
bias created by the potential of the plasma relative to the bottom electrode. They can 
perform either a physical or a chemical etch, or some combination of the two. A 
physical etch refers to the process of removing layers on the wafer by physically 
bombarding them with ions; in a chemical etch, the ions react with the layer they 
are etching to produce a new volatile etch product which is drawn out of the 


8-3 





chamber by the vacuum pump. While a chemical etch can be performed in either 
the RIE or a simple dish Tilled with the gas's corresponding acid, a physical etch can 
only be accomplished inside the RIE. The advantage to a physical etch is shown in 
figure 1. While a chemical etch is usually isotropic, etching in all directions, a 
physical etch will provide a very anisotropic etch, creating very straight structures 
with vertical sidewalls and little undercut. The amount of anisotropic etching one 
gets from a plasma is dependent on several factors, such as the type of gas used, the 
DC bias, and the gas pressure. The higher the DC bias, the more anisotropic the etch 
will be. However, increasing the DC bias too much results in damage to the wafer 
caused by the impact of the high energy ions. The gas pressure also contributes to 
the nature of the etch. The higher the pressure, the more isotropic the etch will be. 
This is due to the reduction in the mean free path of the ions that results from the 
greater number of gas molecules in the chamber. The ions fail to achieve a 
significant downward acceleration because of the increase in the number of 
collisions per unit time. Because of this, they are more readily propelled along other 
planes, thus etching the structure from all directions. It now becomes apparent why 
the calibration of the mass flow controllers is so critical; an error in gas flow can 
result in an altogether different etch. A mass flow controller (figure 2) works by 
sensing a temperature change caused by the flow of gas. A power supply provides a 
constant power input to the heater, located at the midpoint of the sensor tube. When 
there is no gas flow, the heat reaching each temperature sensor is equal. When gas 
flows through the tube, the upstream sensor is cooled while the downstream sensor is 
heated, producing a temperature difference. This difference is directly proportional 
to the gas mass flow according to the equation 

A i = A * P * Cp * m (1) 

AT = change in temper ture (°JO 

Cp = specific heat of gas at cor t pressure (kJ / kg*°K) 

P = heater power (kJ/sec) 
m = mass flow (kg/sec) 

A = constant of proportionality (S^*k2/kj2) 


8—4 




Isot rop ic 
Etch 


An iso tro pic 
Etch 


figure 


1 . 


Examples of different types of etches. A physical etch produces a very 
anisotropic structure, while a chemical etch will produce a more isotropic 



figure 2. 


Schematic of a mass flow controller. 

8-5 












III. Testing Methodology 


The RIE allows the user to key in a percentage of the maximum flow rate for each 
gas. The mass flow controller then uses a flow restictor to allow that percentage of 
gas to flow. In order to ascertain if the mass flow controllers were indeed working 
correctly, the flow rate was determined mathematically and then compared to what 
the value should be. The RIE chamber was first pumped down to .002 lorr. One gas 
was turned on at a certain flow percentage, and the resulting pressure was noted. 

The vaccuum was then turned off, and the gas was allowed to flow for a set amount of 
time. The resulting change in pressure was recorded. Since the volume and the 
temperature of the chamber is known, it is possible to calculate the gas flow rate 
according to the equation 


*t= AP*T*V*78.9 (2) 

Q = flow rate of gas (seem) 
t = time (seconds) 

AP = change in pressure (torr) 

T = temperature (°K) 

V = volume (Litres) 

78.9 = constant converting torr*L/sec into SCCM 

The flow percentages used were 10%, 20%, 50%, 70%, 90%, and 100%. The correct flow 
for each of these settings was calculated by taking that percentage of the maximum 
flow rate achieved at 100%. 

IV. Results and Discussion 

The data obtained through this testing process was placed in a spreadsheet and 
then graphed. In general, it was found that the heavier gases strayed more from the 
ideal than the lighter ones (figure 3). This was anticipated, since the equation 
applies to ideal gases, and intermolecular forces of attraction are greater among the 
heavier molecules. In addition, it was found that the maximum flow rates for the 
gases were different than what they were believed to be (figure 4). This was very 
true in the cases of Helium and Oxygen. The graphs will compensate for the non- 


8-6 







figure 3. Graphs of Helium gas and Freon gas flow. Helium, the more ideal gas, strays less from its 
expected flow than does Freon. g _7 























































Chamber 1 


figure 


figure 


fias 

Qld-Mi 

ix Flow Rate 


New Max Flow Rate 

Oxygen 

100 

SCCM 


87.8 SCCM 

Helium 

290.8 

SCCM 


234.6 SCCM 

Freon-14 

84 

SCCM 


80.2 SCCM 



Chamber 

2 



Old Max Flow Rate 


New Max Flow Rate 

Freon-12 

35 

SCCM 


36.1 SCCM 

Oxygen 

100 

SCCM 


131 SCCM 

Helium 

290.8 

SCCM 


246.2 SCCM 

Argon 

145 

SCCM 


99.3 SCCM 

Old and new 

maximum 

flow rates for 

RIE 

gases in both chambers. 



Q 


Q 


5. An S/R (set/reset) flip-flop with inputs S and R, and outputs Q and not(Q). 
The flip-flop will change states only when there is a clock pulse, and will 
hold that state indefinitely. 




linearity of the mass flow controllers by providing the RIE user with an accurate 
knowledge of the flow rate at whatever flow percentage is desired. 


V. Electrical Circuit Design 

After completing the calibration of the RIE, I concentrated on electrical circuit 
design and analysis. I spent an initial week studying basic circuit components, such 
as resistors, capacitors, and transistors. I then breadboarded an S/R flip-flop, shown 
in figure 5. This served as an introduction to the study of combinatorial logic, which 
combines both sequential logic, such as binary adders, and state machines, which use 
flip flops. Once I had a good understanding of combinatorial logic, I put it into 
practice with the design, of a traffic light controller. The controller is designed to 
drive 7 lights: 3 on a side street, 3 on a main street, and a turn signal from the main 
street to the side street. There are two inputs: a reset switch, which sets the main 
street light to green and the side street to red, and a car sensor switch which tells the 
controller when there is a car on the side street. The state diagram (figure 6) is 
designed so that the main street light will always be green until there is a car on the 
side street. When a car does come, the controller will then automatically cycle 
through all the lights. After designing the state diagram, I created the Karnaugh 
maps for the input to each of the flip flops. A Karnaugh map (figure 7) is a tool used 
to simplify the logic. A 1 is placed wherever the input will be true; a 0 is placed 
where it will be false. Any grouping of Is indicate a redundancy. Thus, it is possible 
to reduce large boolean equations to a simpler form without going through the 
gyrations of boolean alegbra. After I derived all the equations for the flip flops and 
the output lights, I drew the circuit schematic. To ascertain the circuit's 
functionality, I wrote a pascal program that would build a truth table for a circuit, 
given the circuit equations. I also breadboarded the circuit. Unfortunately, the 
breadboarded circuit did not work. However, when we manually analyzed the circuit, 
it did work. Therefore, we believe that faulty components were the cause of the 
circuit's failure. 


8-9 






00 / 01 / 1 1 


11/01 


00 / 01 / 10/11 



figure 6. State diagram of traffic light controller. The numbers beside the arrows show 
when the controller changes to a different state. The first number represents 
the car sensor; the second number, the reset switch. 



figure 7. A Karnaugh map for D 3 , the D-flip-flop that puts out yj. With the help of the 
map, the equation reduces to y 3 = not(R) AND not (yj). 


8-10 





VI. Conclusion 


This summer, I performed a calibration of the Mass Flow Controllers in the RIE 
system and I designed and tested a combinatorial logic circuit. The new data obtained 
from the MFC calibration should be of help to researchers. The tests revealed that the 
MFCs were not providing the proper gas flow. However, the graphs and data collected 
allow researchers to compensate for that by correctly correlating the gas flow 
the percentage setting of gas flow on the machine. This provides an extremely 
effective way to insure reproducible etches. I learned how to design and create b ic 
circuits capable of performing various tasks. I then utilized this knowlege to design 
a traffic light controller. While 1 was unable to build a working model, a hand check 
of the circuit diagram showed that it did work. Moreover, I gained a better 
understanding of the circuit design process. 


8 - 1-1 







STUDYING THE TRANSMISSION CURVE OF KTP 


Jennifer A. Foley 
High School Apprentice 
Electro-Optics Sources Branch 
Wright Laboratory, Wright Patterson Air Force Base 


Beavercreek High School 
2940 Dayton-Xenia Rd. 
Beavercreek, Ohio 45385 


Final Report for: 

AFOSR High School Apprenticeship Program 
Wright Laboratory 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 


August 1993 


9-1 





STUDYING THE TRANSMISSION CURVE OF KTP 

Jennifer A. Foley 
High School Apprentice 
Electro-Optics Sources Branch 
Wright Laboratory, Wright Patterson Air Force Base 

Abstract 

The characteristics of the nonlinear crystal KTP were studied primarily 
using the Nicolet FT-IR. The Perkin Elmer Lambda 9 was also used, though, to 
measure the transmission of KTP. These instruments measure the transmission 
of specific wavelengths through a substance. This study lead to the testing 
of different techniques for using the Nicolet and the Perkin Elmer. During an 
experiment, certain aspects, such as the amount of time the compartment had 
been purged, were altered to see how the measurement ras affected. By 
attempting different methods, it is believed that the best possible results 
were found for studying the crystals. Once this was done, the true transmis¬ 
sion of a 10mm long crystal was determined. This was accomplished by subtrac¬ 
ting out losses in transmission due to Fresnel reflection and absorption of 
water vapor and carbon dioxide. An equation was also developed to predict the 
true transmission of a KTP crystal despite its length. Data from the trans¬ 
mission of two other crystals, 7mm and 13mm in length, were used for predic¬ 
ting the transmission of a 10mm long crystal. The predicted and actual 
transmission curves show excellent agreement. 


9-2 




STUDYING THE TRANSMISSION CURVE OF KTP 


Jennifer A. Foley 


Introduction 

The first task undertaken was to find the true transmission curve of 
Potassium Titanyl Phosphate, KTiOP0 4 (KTP). This was to be done by subtrac¬ 
ting any absorption due to water vapor or carbon dioxide and any Fresnel 
reflection. While attempting this, it was observed that changes occurred in 
the transmission depending on the existing conditions in the spectrometer 
conqpartment. For example, the longer the compartment was purged, the fewer 
absorption peaks due to water vapor and carbon dioxide occurred. Also, the 
amount of transmission was highly dependent on how the crystal was located in 
relation to the beam. Therefore, in order to obtain the best possible 
measurement, setting the instruments correctly was first studied. It was 
noted while studying transmissions of different crystals that the amount of 
transmittance was dependent upon the path length of the crystal. Using a 10mm 
crystal as the standard, a 7mm crystal acted as a basis to predict the 
transmission of a 10mm crystal. The same was also done using a 13mm crystal. 

Apparatus 

The Nicolet Fourier Transform Infrared (FT-IR) spectrometer was used 
primarily for the study. Before a measurement can be taken, the beam splitter 
must first be aligned to maximize throughput. Then, any apparatus needed to 
take the measurement must be placed in the compartment. After this, a 
nitrogen purge is turned on to push out any elements in the atmosphere that 
may affect the measurement, such as carbon dioxide and water vapor. Testing 
was first done to find the correct amount of time needed for purging. This 
was accomplished by taking transmission scans of an eiqpty compartment after it 
had been purged for different amounts of time. It was discovered by measuring 
the depth of the spike at 4.3 microns that it was reduced the most after 
twenty or twenty-five minutes of purging (Figure A). 

Once the compartment has been purged for approximately twenty minutes a 
background scan is min. This accounts for any substances in the compartment. 
It then subtracts these from the measurement so that only the sample being 
tested is part of the resulting curve. (The purge may be left on even during 
a measurement.) After the background, the sample to be measured should be 
placed normal to the beam. Because the compartment has been opened to place 
the crystal inside, it is best to let the system purge for about ten minutes 
before taking the measurement. Numerous measurements, or scans, may be taken 
until the majority of water vapor and carbon dioxide peaks are gone. This is 
just an indication of how well the compartment has been purged and has no 


9-3 











bearing on the signature curve of the material. 

The Perkin Elmer Lambda 9, another spectrometer that was used for 
measuring the transmission of KTP, must also be purged before a measurement is 
taken, but only for about five minutes. To ensure that the amount of purging 
is correct, a scan of the empty compartment might be made. Once everything is 
ready, the sample should be placed in the compartment and a measurement should 
be taken. 

The Nicolet measures the wavelength region between 1.35 and 28.57 
microns. Scans taken at the extreme ends of this region may not be accurate, 
though, because of the stress on the detectors. The Perkin Elmer measures a 
range of 0.185 to 3.2 microns. 

Methodology 

The transmission of a KTP crystal measuring 10x10x10mm 3 was first 
measured in the Nicolet over a range of 1.35 to 28.57 microns. A measurement 
was then made using the Perkin Elmer in the 0.185 to 3.2 micron region. 
Transferring this data to SigmaPlot, a software package used for graphing 
data, the corresponding regions were able to be graphed to ensure the accuracy 
of the measurements. This graph is displayed in Figure B. 

Once this had been established, two other KTP crystals of varied lengths 
were obtained so that they might also be tested (Figure C). Data points from 
these were loaded into SigmaPlot where a math transform was performed. Using 
the equation R-[ (n^-n^) / (n^+n^) ] z , the amount of Fresnel reflection (R) was 
calculated. The index of refraction for air was used as n in , but because KTP 
is a nonlinear crystal, the index of refraction is different for each axis and 
for each wavelength. Values for 1.4 and 4.0 microns for the same axis were 
placed into the equation for n out . 

The values calculated for R were placed in the equation T-(l-R), where T 
represents the transmission of the crystal. The values obtained for T were 
then squared to account for the two surfaces of the crystal through which the 
light must pass. Transmission at 1.4 microns was calculated to be eighty- 
eight percent, while transmission at 4.0 microns was calculated to be eighty- 
six percent. Therefore, the average value of both transmissions was about 
eighty-seven percent. Looking at the graph that was plotted of the original 
transmissions and assuming that the region between 2.0 and 2.75 microns 
contains no absorption, it is observed that the transmission with the Fresnel 
losses is about eighty-three percent (Figure C). Essentially this would mean 
that the transmission in this wavelength region is one hundred percent when 
correction has been made for Fresnel losses. The assumption that there is 
only reflection and no absorption in this region is based on the graph that 
plots alpha, the absorption coefficient, versus wavelength (Figure D). The 
graph shows that there is very little absorption occurring between 2.0 and 


9-4 





2.75 microns. Therefore, any loss of transmission is due to reflection and 
scattering. It was decided best to use eighty-three percent because it was an 
amount that was actually measured, not just calculated, and because it takes 
into account some loss of transmission due to scattering. 

The value T was then placed into the equation T n >Te' ain in which the 
absorption coefficient, a, was solved for using a known transmission, T n , and 
length of the crystal, l n . The absorption coefficient was then placed into 
the equation Tp-Te"* 1 * to find the predicted transmission of a crystal with a 
specific path. This predicted value corrects for any Fresnel losses. For a 
predicted transmission which does not correct for Fresnel losses, the same 
equation may be used but the value T is not placed into the equation. It must 
be noted that the absorption coefficient which is used for the prediction is 
only a constant at the wavelength at which it was calculated, but it may be 
used with a crystal of any length path. In this experiment, two different 
crystals, one 7mm long and the other 13mm long, were used as bases for 
predictions. The predicted transmissions were for a crystal with a 10mm path. 
The actual transmission of a KTP crystal with a 10mm path was then compared 
with the predictions (Figure E). 

To find the transmission curve for a KTP crystal of 10mm in length with 
correction for Fresnel losses and absorption of carbon dioxide and water 
vapor, the compartment of the Nicolet FT-IR was first purged with nitrogen to 
clear the atmosphere. Once a background was taken, any absorption by carbon 
dioxide and water vapor were subtracted from the measurement by the computer. 
Assuming that no absorption occurred between 2.0 and 2.75 microns, the amount 
of transmission should be one hundred percent because any loss is due to 
Fresnel reflection. At this point, the amount of transmission should be 
divided by eighty-three percent to obtain a true transmission curve for the 
given crystal (Figure F). 

Rqpuii;? 

Measuring transmission without absorption due to carbon dioxide and 
water vapor was very successful. Transmission curves were made of these 
substances (Figures G and H) and the signatures of neither were found in the 
KTP curves. The approximate amount of time necessary for purging the compart¬ 
ment was also found (Figure A). The corresponding absorption coefficients for 
any given wavelength were very similar between the 7mm, the 10mm, and the 13mm 
crystal (Figure D). This verifies that alpha is a constant for each wave¬ 
length and is not dependent on the length of the crystal. The value T, used 
to correct for Fresnel reflection, was also accurate due to its similarity to 
the calculated value. The predictions for a 10mm crystal based on the 7mm and 
the 13mm crystal were highly accurate (Figure E). 


9-5 


Concluaion 

The results of this experiment are important because the equations used, 
primarily Tp-Te'* 1 * and R“ [ (n ln -n out ) / (n ln +n oot ) ] *, can be used to calculate the ab- 
sorption coefficient for a specific wavelength, to calculate the amount of 
Fresnel reflection, or to predict the transmission of a crystal with a 
different length than the one that was measured. These equations can be 
applied to any material and to any wavelength. Attention was footed on KTP 
and the 2.0 to 5.0 micron region because of their importance for midinfrared 
lasers, one of the primary research interests of the Air Force. Knowing the 
true transmission will aid in discovering if other elements are affecting the 
transmission of a substance. By being able to predict transmission, it will 
be easier to decide what size crystal of the material is needed to achieve the 
desired transmission without actually having to measure many different 
crystals. 


9-6 






Purging of the Atmosphere 
Spike at Approximately 4.3 Microns 



Amount of Purging (min.) 
(Figure A) 





Comparison of KTP 

(using the Perkin Elmer Lambda 0 and the Nicolet FT-1R) 



Wavelength in Microns 
(Figure B) 









Transmission of KTP Crystals 



Wavelength in Microns 
(Figure C) 














KTP Crystal 

Absorption Coefficient (Alpha) vs. Wavelength 



in 


fO 


CM 


(j_ mo ) 


^naioijjaoo uoi}djosqv 


9-10 


Wavelength in Microns 
(Figure D) 






Predictions vs. an Actual Transmission of a 10mm Long KTP Crystal 



UOISSIUISUCJJ, % 


9-11 


Wavelength in Microns 
(Figure E) 






Transmission of a 10mm KTP Crystal 
With and Without Fresnel Reflection 



o o o o o o 

O 00 ID ^ CN 


uoissiuisunjj, % 


9-12 


Wavelength in Microns 
(Figure F) 








aoiix?'VV? UISU10 * I, k 


100 



o o o o o 

00 CO ^ CN 


UOISSIUISUT9JX % 


9-14 










Ion Implantation Simulation 
in Three Dimensions 

Joel Kulesa 

Archbishop Alter High School 
Bradley University 
Peoria, III 

Dr. Charles Cemy 
Wright Laboratory 
Wright Patterson AFB, OH 

Final Report for: 

Summer Research Program 
Wright Laboratory 

Sponsored by: 

Air Force Office of Summer Research 
Wright Patterson Air Force Base, Dayton, OH 


August, 1993 






Ion Implantation Simulation 
in Three Dimensions 


Joel Kulesa 

Archbishop Alter High School 
Bradley University 
Peoria, III 

Dr. Charles Cemy 
Wright Laboratory 
Wright Patterson AFB, OH 

Abstract 

A program that simulates Ion Implantation was developed 
so that an aging BASIC program could be replaced. This 
program would not only duplicate the former program but 

would improve upon the existing program through better 

simulation and improved graphics plotting routines. The 
result was a package written in portable C on the Sun 

workstation that could be easily expanded in the future. 

In-house simulation routines such as these prove how 
computer simulations make electronic designs much cheaper 
and less labor intensive. 


10-2 




t'hfoesbttle/h- 

In 1945 the first totally electronic computer was born. Called 
the ENIAC, it represented the latest technology at the time. This 
particular system filled an entire large room and required countless 
technicians to keep it running. Not too long after the ENIAC was built, 
a new technology was pioneered that would change the face of computers 
and electronics forever. That revolution known as the integrated 
Circuit made electronic devices more efficient, compact, and reliable. 
Bulky electron tubes would then be replaced with small black IC packages 
that could perform the job of several thousand tubes. 

A similar technological revolution is happening now, only this 
particular revolution replaces IC laden circuit boards with Xery large 
Scale Integration chips. VLSI chips replace complex multi-chip circuit 
boards and assemblies with one highly compact electronic module. This 
module has the full capabilities of the original circuit plus additional 
speed, increased reliability, and substantial heat reductions. 

VLSI technology works on the simple premise that when you pack 
circuitry closer together, electrons have less distance to travel, which 
means that they can reach their destination faster. Faster electron 
interactions give the overall circuit a speed boost. However, speed is 
not the only benefit to implementing VLSI circuits. Any time electrons 
move through a conductor heat is generated. Since VLSI packs the 
circuitry closer together and the electrons have to pass through less 
wiring, thus less heat is generated. This can be even taken one step 
further. Since heat is essentially "wasted* energy, the less heat that 
is produced by a circuit, the less energy that is needed to power that 


10-3 






circuit -- and less power means lower operating costs and smaller power 
supplies. VLSI has the ability to vastly improve upon almost every 
aspect of electronic circuit design. However, this new technology has 
not been fully developed therefore there are several difficulties to 
overcome. 

P\e&lt*h4 

A major concern among VLSI designers is just how far can this 
circuit down sizing can go. When designers attempt to push the size 
envelope a bit more, sometimes the result is not so advanced. Chips 
would either overheat, act unreliably, or just self-destruct. To better 
understand why this is happening, a basic understanding of how VLSI 
circuits are built is necessary. 

VLSI circuits are built up from a one to three inch round disc 
known as a wafer. The wafer can be compared to the foundation of a 
building with the only difference being that millions of "buildings" 
will fit on any particular wafer and that they will rarely be taller 
than 10 microns. Different layers of semiconducting materials are laid 
down on the wafer in sequence. These layers will eventually be 
patterned into electronic circuits but before that can happen, the 
electrical properties of the materials must be modified. A technique 
known as ion implantation is one common way to modify the conductive 
properties of these substances. Ion implantation involves the 
acceleration of charged particles known as ions onto selected areas of 
the wafer. The trick is to make sure only the right parts get hit by 
the ion beam. This selective application of ions is achieved through 
the use of a mask. A mask is a layer of an inert substance that blocks 




specific portions of a wafer from a selected process -- in this case the 
mask blocks the ion flow. The problem is that the mask doesn't 
guarantee that ions only reach regions directly beneath their edges. 
Why is that? When the ions pass the edges of the mask on their route 
through the semiconductor materials, they scatter laterally a certain 
amount. This scattering limits the density of the device since the 
closer one pack circuits onto a chip the closet the diffusion regions 

will get. If they 
get too close, 
short circuits 

result and the 
device becomes 

worthless. 

To prevent 
this from 

happening there 
are two methods to 
determine just how dense circuits can get. One is trial and error; the 
other is computer simulation. The trial and error method is nice 
because it produces physically measurable results which work (or do not) 
in real life. Unfortunately this method is both time consuming and 
expensive. The material cost alone can run $10,000 per test using the 
trial and error method. On the other hand, the computer simulation 
method can produce similar results but it does so on a much quicker time 
scale. Furthermore the cost of simulation is much lower since there is 
a one time charge for the software (either purchased or developed in- 
house) which can be used countless tiroes. It doesn't take a genius to 


Ion Beam 





10-5 








figure out that the computer simulation models offer significant 


advantages over the experimentation method. 


MtX&eAeLc’Cy 


Unfortunately at this moment there is very little simulation 
software on the market for this technology. Previously, a package from 
Implant Sciences Inc (IS), was used but it was not able to plot multiple 
device layers at one time, plus it couldn't calculate a three 
dimensional model. It was decided to write a custom package that would 
extend the abilities of the IS program. 

The completion of the software package was divided into several 
simpler steps to guarantee accuracy and to make programming easier. The 
first step was to take parameters that were calculated by the IS program 
and use them to re-simulate the single-layer, single-dimension plot that 
the IS program could already do. Once the simulations matched, the next 
step was to let the program simultaneously calculate multiple 
substances. Again these results were verified to be accurate. The final 
step was to use an error function to show the ion distribution in the 
lateral or y-direction. 

One important goal of this project was to eliminate the in-house 
duplication of commercially available software. This goal was met 
through our use of the IS software parameters and through the use of a 

powerful workstation based 
scientific graphing 

package called PV-Wave. 
Instead of writing custom 
X-Windows graphics routines, PV-Wave took the output from the simulation 



10-6 















Simulation routines such as this one, shows how powerful (and cost 


effective) modern computers can be when used to verify a design or 
theory before actually implementation. Several of the graphs in this 
document are actual output plots of the ion implantation simulator which 
were used to evaluate designs. Due to the brief nature of the Summer 
Internship, complete results and conclusions are not possible but one 
thing was quite apparent: ion implantation simulation is the one of the 
quickest ways to verify the validity of new designs that are so critical 
in the continuing nature of today's electronic revolution. 





COMPUTER AIDED DESIGN 
OF ACES-II FLIGHT ESCAPE MODULE 


Mark D. Baits 
High School Apprentice 


Cedarville High School 
248 North Main Street 
Cedarville, Ohio 45314 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratory 


Sponsored by: 

Air Force Office of Scientific Research 
Wright-Patterson Air Force Base, Dayton, OH 


August 1993 


11-1 





COMPUTER AIDED DESIGN 
OF ACES-II FLIGHT ESCAPE MODULE 

Mark D. Baits 
High School Apprentice 
Cedarville High School 


Abstract 


The computer aided design (CAD) of the Advanced Dynamic 
Anthropomorphic Manikin (ADAM) test dummy was generated for use in 
Computational Fluid Dynamics (CFD) testing. The CAD drawing of 
the ACES-II ejection seat was provided by the United States Navy. 
The ACES-II ejection seat is the current model incorporated in the 
F-15 and F-16 cockpits. A CAD drawing of the ADAM was needed to 
perform Computational Fluid Dynamics (CFD) of both the seat and a 
test dummy during the crew escape sequences. The CAD drawing of 
the ADAM was made to fulfill the first requirement of the CFD 
generation. 


11-2 






COMPUTER AIDED DESIGN 
OF ACES-II FLIGHT ESCAPE MODULE 


Mark D. Baits 


Table of Figures 


Computer Aided Design of Advanced Dynamic Anthropomorphic Manikin 
(ADAM).Page 11-4 


11-3 









Figure 1 - CAD Drawing of ADAM 








COMPUTER AIDED DESIGN 
OF ACES-II FLIGHT ESCAPE MODULE 

Mark D. Baits 

Introduction 

The purpose of this project was to create a Computer Aided 
Drawing (CAD) of the ACES-II flight ejection seat with a pilot. 
This drawing was needed to perform Computational Fluid Dynamic 
(CFD) analysis of this seat during an ejection sequence. The CFD 
data gathered would later be used to study many different 
ejections. An analysis of the different ejections would be used 
to modify the seat and extend the flight envelope in which a pilot 
may safely eject. The ACES-II flight ejection seat was chosen for 
study because it is the seat currently incorporated in the F-15 
and F-16 fighter jets. The current envelope for safe ejection in 
the ACES-II seat extends to speeds slightly faster than Mach 1. 

At speeds greater than this, the pilot's head and helmet act as a 
wing and produce lift. Current state of the art technology uses 
CFD testing to produce the data necessary to evaluate the flight 
characteristics the pilot and seat combination may encounter 
during an ejection sequence. CFD testing was to be used as 
opposed to wind-tunnel testing due to the greater speed at which 
the data could be produced. 

Methodology 

The ACES-II seat was chosen for CAD modeling due to its current 
use in the F-15 and F-16 fighter jets. Therefore, there is a 
large compilation of accident data available for use concerning 
this seat. The seat also provides a suitable test base for which 
to incorporate advanced technology using CFD analysis. The CAD 
data for the seat was provided by the Naval Air Warfare Center, 
Aircraft Division, Warminster, Pennsylvania. The CAD data was in 
Initial Graphics Exchange Specification (IGES) format. A 
translator was used to convert the IGES code into Cadkey code for 
our computer systems. A suitable CAD drawing of a pilot was 
needed to incorporate with the ACES-II seat. After discussion 
with systems engineers, it was decided that a versatile test dummy 
was needed so that the CAD drawing could be incorporated with 
other ejection seats and used to produce additional data. The CAD 
drawing of the manikin and the other seats would later be united 
to run Three-Dimensional Computational Fluid Dynamics at various 


11-5 



angles of attack. Then the test data would be used to experiment 
with different seat designs and extend the flight ejection 
envelope. 

Data was found for a test dummy at Armstrong Laboratories that 
was in a format not recognizable by our computer systems. Another 
test dummy, the Advanced Dynamic Anthropomorphic Manikin (ADAM), 
also provided by Armstrong Laboratories, was chosen to utilize 
with the ACES-II seat. It was chosen because of the multi-purpose 
utilities that could be employed with the CAD drawing of this 
dummy. Some of these include: (1) CFD analysis with the ACES-II 
seat, (2) extensive aerodynamic study and animation, (3) 
simulation of different flight gear, helmets and oxygen masks, and 
(4) combination with a number of other seats to produce data. 
Finally, the ADAM was chosen due to its size. The dummy's size 
represents the size of ninety-five percent of the pilots in the 
United States Air Force. Thus it was of a scale feasible for CFD 
data that represents the actual flight ejection sequence as 
accurately as possible. 

The existing CAD data of the ADAM also was not available in a 
format recognizable by our computer systems. No translator was 
available; therefore, the manikin had to be measured using vernier 
calipers and the data collected was used to put together a CAD 
drawing. The original drawing had to be discarded because of its 
block construction. The connected cylinders and blocks that 
comprised the model did not accurately represent the individual 
limbs of a human. Such a construction would produce questionable 
aerodynamic data, because a block acts differently than a smooth 
surfaced torso or head. Conversation with section engineers 
revealed that a model closer to real life was needed. Af*-er 
brainstorming, it was decided that the second drawing should be 
made of polygon meshes with an enlarged head representing the 
helmet. Further refinements included smoothing the elbow and knee 
joints to represent an actual human body as closely as possible, 
thus providing accurate aerodynamic data during CFD generation. 

The hands were made of spheres, because a closed fist resembles a 
sphere more so than a block. The model was rendered as a polygon 
mesh, representing the skin surface of the pilot, as opposed to a 
block figure composed of boxes and cylinders. These polygons 
allow for later renderings of the manikin as a solid. A solid 
rendering of the ADAM would provide additional data that would 
include mass and inertial properties. 


11-6 





At first, Computer Aided Design of the manikin was performed on 
an IBM compatible personal computer using Cadkey version 3.53. 
Cadkey was chosen due to its availability and its capability of 
rendering solids out of wire-frame models. Due to the size of the 
part file, the drawing could not be finished on the personal 
computer system. The ADAM had to be completed on a UNIX-based 
Silicon Graphics Indigo2 (SGI) Extreme. Therefore, knowledge 
about operating the UNIX system had to be acquired before any 
further modeling could continue. This SGI computer system allowed 
for the size of the file and also produced renderings at a much 
greater speed. Cadkey version 5.04 on the SGI also includes 
Stereo Lithography Applications. Such applications allow for 
quick manufacturing of the part composed of plastic resins for 
further analysis. The CAD data is sent to a stereo lithograph 
machine that generates a plastic part of the CAD drawing. 

Data from different ejection seat technologies can later be 
acquired and used to modify the current seat. Current Russian 
escape seat (K36 series) technology incorporates a flow diverter 
directly in front of the pilot to alleviate increased air pressure 
and lift on the head. A ventilated helmet is also used by the 
Russians to minimize pressure in the event of air pushing up on 
the helmet. Existing technology and further refinements can be 
compiled to develop a new seat for the F-15 and F-16. 

Results 

The CAD drawing of the test dummy was correctly drawn and 
scaled to fit the ACES-II ejection seat for CFD testing (Figure 
1). Figure 1 is a polygon rendering of the ADAM as it appeared on 
the personal computer. The other leg and arm are not shown due to 
the limits placed on the personal computer system to store and 
manipulate that additional amount of data. The finished rendering 
completed on the SGI could not be printed because of the lack of 
file transportability. The SGI file differs in that the dummy is 
fully drawn, seated at a thirty degree angle of declination and 
has spheres for hands, as opposed to blocks. Additionally, the 
helmet is shaded as an ellipsoid, as opposed to the stacked block 
construction that is shown in Figure 1. 






HIGH SCHOOL APPRENTICESHIP 

PROGRAM 
FINAL REPORT 

1993 WL/FIVEC EXPERIENCES 
REVIEWED 


BY: JASON BRECHT 
AUGUST 6, 1993 

DISCRETE BASE PAGE NUMBER 12 




Many tin>«a in my Ufa Z have been offered opportunities which enrich 
both my intelligence and my problem solving skills. This summer, another one 
of these opportunities presented itself. Z applied for and was accepted into 
the HSAP (High sehool Apprenticeship Program) at Wright Patterson Air Force 
Base. Although I did not realize it at the time, my stay on the military base 
would not only enhance my intelligence, which is one concrete aspect of my 
knowledge, but would also provide me with a much more practical and well- 
rounded education. The knowledge which I will take with me from this job will 
make me a better person, not only in the immediate future, but for the rest of 
my life. 

My official job title was "Data Collection Analyst." I feel it was 
fortunate that the job description did not directly apply. My first day on 
the joh T met George Kurylowich, my supervisor, and Dave Brown, a man with 
whom I would be working with very closely for the next two months. Dave 
introduced me around the office, and almost immediately I felt comfortable. 

The other office employees all welcomed me and treated me ae if I had been 
there for years. I was now an official member of the Environmental Control 
Branch of the Flight Dynamics Directorate (FXVZC). I was then taken to see 
the teat site. 

At the test site 1 learned of microencapsulated phase change material 
(micro PCM) and how it was supposed to revolutionize the future of avionics. 
The micro pcm capsules tested were approximately 10 to 30 micrometers in 
diameter. The theory behind micro PCM technology is that when the PCMe are 
encapsulated and suspended in a carrier fluid, they increased the heat 
capacity of that fluid by up to 400%. The combination of the PCMs and the 
carrier fluid is then classified aa a slurry. In a future where more complex 
avionics are born from more advanced technology, a coolant fluid would be 
needed in order to cool increased avionics heat loads which result from 
increased energy densities on the boards. As heat is applied to the micro 
PCMs, the phase change material melts and absorbs heat. The latent heat of 
melting increases the effective heat capacity of the fluid near the melting 
temperature of the phase change material. PCMs seemed like the ideal solution 
to high energy avionics cooling. The PCM slurry was tested in a loop which 
consisted of two main components, a water coolant loop and an avionics coolant 
loop. The avionics coolant loop was to flow through a simulated avionics 
board, a board similar to those planned in the next generation of fighter 
aircraft, in order to apply heat to the slurry. The slurry then would flow 
through the hot side of a heat exchanger. The chilled water loop would flow 
from a water chiller, through the cool side of the heat exchanger (in order to 
transfer heat from the fluid), and back into the chiller. The result was a 
very compact yet effective test loop which provided much information about the 
advantages and disadvantages of the micro PCM slurry. Through temperature, 
pressure, and flow measurements that were taken from different locations in 
the test loop, conclusions could be drawn about the feasibility and 
practicality of the slurry. My job was to write a computer program which 
would arrange all of the information compiled from the computer test rig data 
acquisition system into a well-organized and comprehensive format. The task 
took several days. I also wrote a program which would pull information from 
the data spreadsheets and place it into a table to be used for graphing the 
information. 

Chet Brewster, the head administrator of the TAVLAB (Thermal and 
Vibrations Lab), the area where the test loop was located, tasked me with a 
project after my computer programs were complete. I was to enter all of the 
ozone depleting chemicals from the lab's MSDSs (Material Safety Data Sheets) 
into a database so that any file could be called up by its chemical name, 
trade name, or file number. There were over 100 MSDS forms which consisted of 
approximately 300 chemicals. It was my first attempt at working with a 
database software program, and this project took me quite a bit longer than my 
first two tasks. The manual labor of copying all of the chemicals to notebook 


12-1 






paper took me two days, tod the database itself took me one week to complete. 
The database gave me an opportunity and a purpose to work with computers in a 
new advanced way which I never had before. But working with a database was 
only one of the many things Z learned to do on a computer. 

Z learned how to manipulate files and information to make them more user 
friendly. Before my experience on base, z had no idea that a computer had 
more than one drive to select from. I worked with several different computer 
soft-ware packages, including Microsoft Exeel, Microsoft Word, Harvard 
Graphics, and PC Shell, all interfaced through MS DOS windows, At one point, 

I used a HP Scanner to scan fifth generation copies into Paintbrush, a drawing 
program, ao that X could enhance them and make them appear like their 
originals. Z now feel comfortable on e computer where as before I felt just 
plain loot. True, I was spoiled by working on a 486, but the knowledge I will 
carry with me from this job will be practical for many years to come. 

Perhaps the most informativs time I spent in the lab was acquired when 
Dave left Mike Browning, a technician, and I to run the test loop for a week 
while he wes out of the office. Both Mike and Z received a very practical 
education that week, we even developed our own scientific law which statest 
"Everything that can go wrong will go wrong immediately after the lead 
engineer leaves town." During that week, our heat exchanger clogged and we 
lost approximately 2,000 dollars worth of PCM slurry. But by pooling our 
problem solving skills we cleaned out the heat exchanger, leveled off the 
system, and had the loop running again in no time, our test results never 
quite came out the same again because the concentration of the PCM* in the 
carrier fluid had been reduaed. I was disappointed that this had happened 
until I learned that discovering problems with experimental interactions is 
juet as important to research and development as making sure the product works 
perfectly. As Carl "The Man who Knows Everything" Williams, another 
technician in the lab, so elegantly put it, "If nothing goes wrong with an 
experiment then you probably haven’t learned anything." 

over the course of the past two months, I didn't just learn about 
developing technology and advanced computer systems. The most important 
lesson z learned was how to work with and get along with ell of my co-workers 
in a professional work-place. All of my colleagues were wonderful and deserve 
my deepest gratitude for the kindness and acceptance they hove shown me. The 
lessons that I will carry with me from this experience will not soon be 
forgotten. 

My summer job was supposed to be about scientific knowledge and 
research, and it was. But it was also so much mors. This program provided me 
not only with the opportunity to enhance ay scientific knowledge, but also 
provided me with an environment where I could grow as a person. This program 
accomplished its goals and many things far beyond them. I can only hope that 
the other HSAP students had as good of an experience as I have. X feel that 
there are a few events in each person's lifs which shapes his future and 
creates a lasting impact on that parson. This summer job has been one of 
those experiences for me. z wish your company continusd success in the future 
and that you provide many others with the opportunity you have provided me. 


12-2 



PROM 80 CENT THERMOCOUPLES TO MULTIMILLION 
DOLLAR AIRCRAFT 

DANIEL E. FEUCHT, high school apprentice under 
Joe. Pokorski 

West Carrollton High School 
5833 Student Street 
West Carrollton, Ohio 45418 
Final Report for: 

High School Apprenticeship Program 
Wright Laboratory/Flight Dynamics Directorate 
Structures Test Branch 
Sponsored by: 

Research and Development Laboratories 
Culver City, Ca. 

August 1993 


13-1 




FROM 80 CENT THERMOCOUPLES TO MULTIMILLION 


DOLLAR AIRCRAFT 

Daniel E. Feucht 

High School Apprentice 

West Carrollton High School 

Abstract 

During my summer experience with WL/FIBT I learned and experienced many 
new things. I also got to work with many interesting people during my time 
rfith WL/FIBT. Aircraft go through many vigorous testing environments and 
naneuvers in the course of one lifetime. At FIBT they test the fatigue of 
the aircraft while performing these maneuvers while at the same time applying 
Loads to them. During my time here I participated in many tests including: 
the "Lightly Loaded Splice Subcomponent," "Elevated Temperature Aluminum 
resting," burst tests, and F-15 and fuel tank fatigue testing. Most of my 
time was spent designing a Pressure Dump Manifold for tests to be run in the 
future. This was designed to release all pressure to the test specimen if 
something was to go wrong. The making of this manifold helped me to develop 
uy understanding of the Design-Cad 3d system better. 


13-2 




FROM 80 CENT THERMOCOUPLES TO MULTIMILLION 


DOLLAR AIRCRAFT 
Daniel E. Feucht 

uel Tank Test Methods 

The purpose of this project is to develop effective methods for sealing 
ntegral fuel tanks on aircraft today and in the future. This project is 
irected toward utilization of the test facility in support for a McDonnell 
ircraft Company program, a General Dynamics IRAD for quick seal damage 
epair methods, and a PRAM program with a spray sealant evaluation for 
arrier aircraft wings. 

During this summer I helped to prepare a specimen to be tested. I 
orked on a generic fuselage tank setup shown in figure 2.1. The first thing 
e had to do was put on thermocouples so that heat could be watched 
arefully. It took us a couple of days to hook up all the thermocouple 
dres, and after we hooked up the wires the tank was then put in a testing 
tructure were it would be heated and tested for leaks and to see if the 
ealing method worked. This tank was not being tested yet at the time of our 
ieparture. 

pightly Loaded Splice Subcomponent (LLSS) 

The McDonnell Douglas Corporation has made up a Titanium Matrix 
‘omposite (TMC) which replicates a typical X-30 fuselage portion. The Wright 
iabs are involved in the testing of this section to evaluate the fabrication 
,nd performance aspects of the thin and stiffened structures as in the X-30 
nd its program. This splice represents the splice of two 80 inch radius 
•ortions of fuselage at the ring of the frame. 


13-3 





FIGURE 2-1 

GENERIC INTEGRAL TANK CONFIGURATION DEFINITION 


D uring the summer I did not work on this part of the testing but I did 
get to watch a test be run on this splice, where they were going to fail the 
splice section with compression. This test was unsuccessful because the 
section could not be broken with the instruments attached to it at that time. 
When the plate would not break they stopped the test and waited to hear from 
the Douglas Corporation for further instructions. 

P-15 Wing and Wing Carry-Thru Structure Fatigue Test 

The current OSAF F-15 fleet usage is four times greater than expected. 

T' r * severity has caused questions of the ability of the F-15 structure to 

meet the original design life of 8000 flight hours. A new fatigue test using 

flight loads representative of actual fleet usage is currently in progress at 

the Structures Test Branch. The test spectrum is comprised of thirteen 

flight conditions varying from a positive 9.92 Gs to a negative 1.89 Gs. The 
test calls for 24000 flight hours to be applied. 

During my time at the lab this summer I never worked on the F-15 fatigue 
tests but I did watch them put 1100 flight hours on the aircraft. During my 
last week at WL they were stripping the aircraft for inspection at a later 
date. 

Elevated Temperature Aluminum Program (ETAP) 

The ETAP test component is an aft fuselage center keel beam. The object 
of the test is to verify by full scale testing that the keel beam meets the 
structural criteria used for its design. Additional objectives are to verify 
analysis, determine the failure modes, and assess the validity of the 
structural criteria for future ETAP designs. Static tests, durability, and 
damage tolerance will be accomplished while exposing the beam shear webs to 
an elevated temperature area. A picture on page 13-6 shows where the keel 
beam is located as compared to the rest of the aircraft. 


13-5 










SELECTEO . 

COMPONENT, 
CENTER KEEL BEAM 


SKirr covers - 

FOR ENGINE 
COMPARTMENT 







Conclusion 


During my time at Wright Labs I experienced many new and interesting 
things which will stay with me for my entire life. I feel that these things 
will help me to be a better person and will make my future a lot more 
j knowledgeable. I was involved in alot of projects that were fun and yet 
educational and I believe that this experience gives a lesson of the hard 
work involved in the field of engineering. This lesson is what I think makes 
the HSAP program a program to remember. 


13-7 





Richard Leon's report not available at time of publication 


14-1 




MAPPING COMPUTER 


NETWORKS 


Amy Martin 
High School Apprentice 
Computer Resources Team 


Kettering Fairmont High School 
3301 Shroyer Road 
Kettering, Ohio 45429 


Final Repent for 
Summer Research Program 
Wright Laboratory 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 


August 1993 


15-1 









MAPPING COMPUTER 
NETWORKS 


Amy Martin 
High School Apprentice 
Computer Resources Team 
Kettering Fairmont High School 

Abstract 

The mapping of the computer networks in building 450 and 24C was done. Previous maps were scanned 
in on Paintbrush, cleaned up, and corrected, or hand-drawn sketches were made into graphs. Harvard Graphics 
was used tot type in a repeat for a Technical Management Review. Familiarity was also gained with WordMarc 
word processing, die aircraft inventory data system, BTS Disspla, the UNIX and PRIME systems, and Silicon 
Graphics. 


15-2 





MAPPING COMPUTER 
NETWORKS 


Amy Martin 


As a high school 
apprentice I became familiar with 
many computers, computer 
languages, and computer 
p rogr a ms. This knowledge may 
be helpful in college and the 
career world. I also became 
familiar with how the c o mp u t e r 
n et w orking in buildings 450 and 
24C work by mapping them. 

The largest project I 
worked on while with the 
Co mpu ter Resources Team in 
Flight Dynamics was a network 
report The network report first 
explains how the system works 
through explanat i o ns 
oe muttons ot me atnettSK 
e l ement s in the network. Then 
the network is shown with 
graphs. These graphs show how 
the computer systems are 


connected. Broadband and 
Baseband Ethernet ThickNet 
and ThinNet connect the systems. 
Some graphs show where PC 
network taps are placed in tooms 


WXH NETWORK TOPOLOGY 
JOLT, 1993 



By tay Martin, Rick Clavangar, 
Dick Sal Us and Phyllis Saleh 


throughout both buildings. With 
these taps a computer can be 
connected to the network. By 
being hooked up to die network 
the user will actually work off of 

15*3 


server. In building 4S0, A Wing 
there are two servers which 
contain the Windows program. 

By working off of the network 
the user doesn't have to fill their 
C drive with programs but 
instead receives programs from a 
server. 

I was either given a 
previous graph or a hand-drawn 
graph of some part of the 
network. I then took the graph 
and scanned it into Paintbrush. 
During scanning lines become 
crooked and various extra dots 
appear on the graph. Then by 
looking at the original graph I 
cleaned up the drawing (see 
Be fore/After examples). This as 
often difficult Sometimes what 
had become a huge black dot 
after being scanned in had to be 








turned into a drawing of a computer 
terminal. After the graph was cleaned 
up my mentor sketched in the changes 
to the network and I updated the 
graphs. The graphs included maps of 
the Building 4S0 Ethernet and 
Asynchronous Local Area Network, 

Building 24C FIM Local Area Network, 

Building 24C FIM PC ThinNet and 
Terminal Service, Building 450 PC Net Configuration, Building 4S0 Dual Broadband Network, Building 450 "A* 
Wing hMwmnit, fint floor, and second floor PC network tap locations and ICU locations. Building 450 *E* Wing 
first floor PC network tap locations and ICU location. Building 450 Broadband ICU Room Number and Serial 
Number, and Building 24 C tap locations on the Mezzanine, First Floor, and Second Floor. The written 
explanations and graphs were compiled into a report for which a graph of the Network Topology was used as a 

title page. 

I also did various other jobs as an apprentice. On Harvard Graphics I laid out the text for the 
Aewnmachanica Technical M an agem e nt Review. This report inc lu ded the status, progress made in 
the past year, projected accomplishments, objective, approach, schedule, funding profile, and issues of the 

Computer Resources Team. I also 
gained experience with UNIX and 
PRIME systems. On the PRIME 
system I worked with WotdMarc 
word proces s ing to type in a 
S t at ement of Work for the 
Aeromechanics Division, and I 



rigurm 4: BLOG. 4S0 PC HIT C OU TX GP RATIOH BLOCK DIAGRAM 



15-4 






worked with the aircraft inventory data system. The BTS Disspta. These graphs dealt with testing he had 

aircraft inventory data system is a record of planes done on the T-38 Speed Brake. First I worked on the 


from the United States, Great Britain, Russia, and 
other foreign countries, spacecraft, and civilian 
aircraft. With this program my job was to input and 
correct data (see Aircraft Information example). 
These records include statistics about the planes found 
in an aircraft magazine. However, much of the 


PRIME system plotting graphs rod printing them (see 
T-38 Speed Brake graph). Then I was given a drawing 
to form on Paintbrush (see Speed Brake - Sketch 
From Drawing 2-16500, below). This drawing was 
revised several times until Mr. Fehl was satisfied with 
the results. 










T-38 SPEED BRAKE 

NUMBER 2,-4 B.L. 22.4 



X RXlb 











AIRCRAFT 


INFORMATION 


1 SYSTEM NAME - 

2 TYPE - 

3 PRIMARY MISSION - 

4 NICKNAME - 

5 DOD SERVICE - 

6 MANUFACTURER - 

7 STATUS - 

8 FIRST FLIGHT - 

9 CREW - 

10 OVERALL LENGTH - 

11 WING SPAN - 

12 WING AREA - 

13 MAX HEIGHT 

14 TAKE OFF GROSS WEIGHT (LBS) - 

15 ECM - 

16 OFFENSIVE RADAR TYPE - 

17 DEFFENSIVE AVIONICS - 

18 WT. ARMAMENT / FUEL - 

19 MAX SPEED - 

20 RANGE (SPECIFY NM OR M) - 

21 SERVICE CEILING (FT) - 

22 POWER PLANT - 

23 POWBR PLANT TYPE - 

24 POWER PLANT MFGR. - 

25 ENGINE DIMENSIONS - 

26 ENGINE THRUST (LBS) - 

27 RADAR WARNING RECEIVER - 

28 ARMAMENT - 

29 GUN TYPE - 

30 GUN ROUNDS - 

31 COST PER AIR CRAFT - 

32 WING LOADING - 

33 . . .TAKEOFF (LB/FT2) - 

34 . . .COMBAT (LB/FT2) - 

35 THRUST-TO-WEIGHT RATIO - 

36 . . .TAKEOFF - 

37 . . .COMBAT - 

38 SUSTAINED TURNING RATE - 

39 . . .(MACH 0.9 

40 INSTANTANEOUS TURNING RATE - 

41 . . .(MACH 0.9 k 15000 FT) 

42 RATE OF CLIMB - 

43 . . .(SEA LEVEL TO 36000 FT) 


C-141 

Cargo Aircraft 
Air-drop/troop carrier 
Starlifter 
USAF 

Lockheed-Georgia Co. 

Operational 

’B’ Model Delivered December 1979 
3-8 

168.3 ft 
159.9 ft 
3228 sq ft 

39.3 ft 
344900 lbs 

70000 lb. payload (A) / 94508 lb. payload (B) 
N/ 

N/A 

Unknown 
566 mph 

1970 mi. (A) / 2550 n. mi. (B) 

Unknown 

4 X PGE TF39-1C (A) / TF-33-P-7 (B) 

Turbofan 

General Electric (A) / Pratt k Whitney (B) 

Unknown 

21000 lb. (B) 

Unknown 

N/A 

N/A 

N/A 

Unknown 

Unknown 

Unknown 

Unknown 

Unknown 

Unknown 

Unknown 

Unknown 

Unknown 


15-7 








ELECTRONIC DOCUMENTATION AND DESIGN 


Benjamin J. Merrill 


Bellbrook High School 
3491 Upper Bellbrook Rd. 
Bellbrook, OH 45305 


Final Report fort 
High School Apprentice Program 
Wright Laboratories 
Flight Control Division 


Sponsored byt 

Research and Developement Laboratories 
Culver City, California 


August 1993 







ELECTRONIC DOCUMENTATION 


AND DESIGN 

Benjamin J. Merrill 
Bellbrook High School 

Abstract 

My apprenticeship was hosted by tho Flight Dynamic* 
Control Integration and Aaaaaamont Branch of Wright 
Laboratories* 1 wee involved with a few of the many different 
kinde of work that go on here. My main focua was on the 
documentation of electronic devices end the components that 
make up these devices. I also did acme designing of digital 
circuits and the initial testing of these circuits. Overall, 

I feel I've learned s great deal about what it's like working 
in an office environment, and have a good feel for what I can 
expect in the fields of engineering and science. 


16-2 




ELECTRONIC DOCUMENTATION AND DE8Z0N 


Benjamin J. Merrill 

lattafaiatlaa 

My summer apprenticeship At Wright laboratories took place 
in tho Control Integration And Assessment Branch of tha Plight 
Dynamics Diractorata. Thair focua hara ia on ground-based 
aircraft simulations for integrating and assessing aaroapaca 
tachnologiaa. Through simulation runs and post-simulation data 
analyais thay ara abla to inersasa aircraft performance* 
survivability* and strike affactlvanass. My mentor vas Ron 
Ewart who is tha Principal Scientist at this facility. 
PlKvitlm 

My main focus for this eight weeks has been with tha 
Simulator Network Analysis Project (SNAP). This progrsm is 
working to develop a tool that will test tha fidelity and 
effectiveness of simulations at the network level. It measures 
the time delays from one network node to another* correlating 
these measured delays as a function of network loading. 

My main task in this projact has been the documentation of 
the hardware that makes up each SNAP computer. I worked with 
both EA8YCAD2 and TAHGOSCH software packages in creating and 
updating drawings of the different computer boards and 
schematics of each one. These drawings will be used to 
produce* debug* and maintanance the SNAP computers. I started 


16-3 

































Flfurs 2 
16-5 






























with drawing eh* computer board* eo *how whtr* *ach microchip 
i* located on eh* v*riou* board*. Figure* 1 and 2 ar* eh* 
drawing* for eh* «w«*p card. Flgur* 1 *how* eh* different 
chip* and h*ad«r« on eh* card and Figur* 2 *how* eh* msks-up of 
•aeh th* h*ad*r* which conaise of r**l*tor* or capacitor*. 1 
also did tome work updating th* schematic* of th**« card*. Th* 
•eh*m*ticc ar* eh* actual designs that ar* laid out bafor* th* 
componant 1* built. A* each card va* debugged , variou* change* 
war* mad* and I va* responsible for shoving th*** change* on 
eh* schematic* and producing a new copy. 

Aside from documentation, £ also did a small amount of 
digital design. 1 was Involved with the Initial designing 
seep* for a high *p*«d data switch. This switch, when 
completed, would allow th* graphic* genaraeor eo be controlled 
from four different driver*. The problem was to design a 
switch which would control which of th* four separate drivers 
would be connected by pressing their corresponding button. 

There was also to be an automatic channel which would detect 
which driver va* feeding th* switch information and 
automatically switch to that channel. I started with a simple 
design ueing TTL microchips and buile a design up step by step 
until th* desired outcome was reached. 1 tested th* design on 
a breadboard driving LEDs as the outpues. Once it was 
determined th* logic would work, I implemented th* design onto 
a larger programmable microchip. This reduced th* number of 
chip* in the meehine from ten to one and made debugging much 


16 -t 






Flfurt 3 

































quicker end eeeiar. Figure 3 shows the final echemetie of the 
logic on the programmable chip* 

Conclusion 

Overall, 1 felt 07 experience vith the High School 
Apprenticeship program was a vary beneficial experience. 1 had 
a chance to learn a lot about electronics and flight 
simulators. More importantly I got a ehanca to experience many 
different aspects of the science and engineering field and 
develop a feel for which of these I like the best. I also got 
a feel for what a full-time job is like. Both of these will 
help me to determine what I have the most interest in doing 
later in my life and were the moet beneficial parts of this 
program. 


16-8 





AN EXPERIENCE IN STRUCTURES TEST IN6 


Charles J. Middleton 
Carroll High School 


Final Report tor: 

High School Apprenticeship Program 
Wright Laboratory 


Sponsored by: 

Air Force Office of Scientific Research 


August 1993 


17-1 







An Experience in Structures Testing 
Charles J. Middleton 

ABSTRACT 

I was assigned to Wright Laboratory, Flight Dynamics Directorate, 
Structures Test Branch, under Aaar Bhungalia, project engineer, at 
Wright Patterson Air Force Base. I worked with my mentor along with 
various other engineers and technicians on a diverse group of projects 
during ey stay there. I assisted on the setup of the Fuel Tank Test 
Methods and the Elevated Teeperature Aluminum Program (ETAP). I 
designed, edited, and redesigned a hydraulic manifold Tor use on the 
ETAP using DesignCAD-3d on a Unisys PW a Advantage computer. I helped to 
create a Macro prograa Tor the analysis of data for the F-16 
Transparency Evaluation Test. I assisted in the design of the 
structural framework for the Structural Assesaent Vulnerability 
Evaluation. I assisted in running the Leading Edge Flux Meter 
Calibration Test and analyzing data for it. I worked in preparing and 
constructing graphite composites in the Composites Lab. 1 was also able 
to view the Lightly Loaded Splice Subcoaponent Fatigue Testing, the 
Conductive Shield Heat Exchanger Sub-Eleaents X-30 Nozzle Active Cooled 
Structure Fatigue Test, and the F—15 Wing and Wing Carry-Through 
Structure Fatigue Test. 


17-2 





An Experience in Structures Testing 
Charles J. Middleton 

I was assigned to Amar Bhungalia, a project engineer, at Wright 
Laboratory, Flight Dynamics Directorate, Structures Test Branch, at 
Wright Patterson Air Force Base. I worked not only with Amar but also 
with many different engineers and technicians in my building. I was 
responsible for a variety of minor works on various projects and for the 
complete design of a hydraulic manifold for use as the dump system in 
the ETAP test. 

I was involved in wiring, electronic assembly of thermocouple 
plugs, attachment of thermocouples, and other preparatory measures for 
the Fuel Tank Test Methods. The objective of this effort is to develop 
effective methods for certification and evaluation of the fuel tank 
containment integrity of current and future aircraft integral fuel 
tanks. The effort is currently directed toward utilization of the test 
facility in support of a Me Do ' Aircraft Co. IRAD program, a General 
Dynamics IRAD for rapid batt mage repair methods, and a PRAM program 

involving spray sealant evaluation for transport aircraft wings. (See 
diagram page 17-4.) 

I was also involved in wiring and setup for the Elevated 
Temperature Aluminum Program (ETAP) test. The ETAP test component is an 
aft fuselage center keel beam. The objective of the test program is to 
verify by full scale testing that the center keel beam meets the 
structural criteria used for its design. Additional objectives are to 
verify analysis, determine the critical failure modes, and assess the 


17-3 





FIGURE 2-1 

GENERIC INTEGRAL TANK CONFIGURATION DEFINITION 


17-4 



validity of the structural criteria for future elevated temperature 
aluminum designs. Durability, damage tolerance, and static tests Mill 
be accomplished while exposing the keel beam shear webs to an elevated 
temperature environment. (See diagram page 17-6.) 

More importantly, I designed, edited, and redesigned a hydraulic 
manifold for use on the ETAP. I used the DesignCAD-3d program on the 
Unisys PM 2 Advantage computer. After being given an initial geometrical 
data and rough preliminary design and familiarizing myself with the 
program, I designed a three-dimensional model of the manifold, which was 
to be used as a hydraulic dump system. After pointing out a few 
problems and trying to fix them, I was permitted to redesign the entire 
manifold using my own ideas. The manifold is a ten inch by seven inch 
by five inch block of aluminum with various cavities in it for valves 
and fluid lines. After completing my new design, I had tD make 
different views and cross-sections of it and then put dimensions on them 
before they were finally sent to the machine shop for manufacturing. 

The diagrams on pages 17-7 through 17-9 detail the design of this 
hydraulic dump system. 

I also used the DesignCAD-3d to alter the design of the structural 
framework of the Structural Assessment Vulnerability Evaluation. This 
test involves firing a powerful laser at a plate of carbon composite 
material as it is bent under high amounts of load. It will demonstrate 
the internal strength of the material and its resistance to 
concentreated laser heating. The diagram on page 17-10 shows the setup 
for that test. 


-5 







17-6 














17-7 












































■■■■■■■■ 





























































































































































•T#T» 



-9 
























































17-10 


























I helped to create a Macro program tor the F-16 Transparency 
Evaluation Test. This test simulates the fatigue that a typical F-lfc 
canopy would undergo during various missions during its lifetime. The 
program that I worked on was used in the analysis of data for the past 
four years of the test. It was also done on a Unisys computer using the 
programming language that was required. 

I assisted in running the Leading Edge Flux Meter Calibration Test. 
This test involved finding the consistency of a plate of graphite 
composite material with respect to the aspect of high intensity heatino. 
Quartz lamps were used to heat the specimen as high as 1500 degrees 
Fahrenheit, and instruments measured heat and temperature at various 
spots throughout the material. I also helped in the data analysis of 
this test, smoothing and averaging data results. 

I worked briefly with the Composites Laboratory which is separate 
from the Structures Test Branch but is in the same building. There I 
was involved in preparing and constructing sheets of graphite 
composites. These carbon composites are meticulously assembled and 
sealed. Then they are "cooked" in an Autoclave at high temperature and 
pressure in order to make them the strong, light and durable materials 
that they are. 

Additionally, I was able to view and discuss in detail the workings 
of three other tests. These tests taught me just as much through 
observation as my work with the others did. 

I viewed the F-15 Wing and Wing Carry-Thru Structure Fatigue Test. 
Current USAF F — 15 fleet usage is appprox i raately four times as severe as 
anticipated. This severity has caused the F — 15 SPO to question the 


17-11 







ability of the F-15 aircraft structure to meet the original design life 
of 8000 flight hours. A new fatigue test lncorporatlng flight loads 
representative of actual fleet usage is currently in progress at the 
Structures Test Branch. The test spectrum is comprised of thirteen 
flight conditions varying from a positive 9.92 Gs to a negative 1.B9 6s. 
The test plan calls for 24,000 flight hours to be applied. 

I saw the Lightly Loaded Splice Subcomponent Test. McDonnell 
Douglas Corporation has fabricated a TMC subcomponent which represents a 
typical X-30 fuselage section. WL/FIBT is involved in the testing of 
this subcomponent ot evaluate the fabrication and performance aspects of 
thin, stiffened structures as related to the X-30 program. This article 
simulates the splice of two 80 inch radius sections of fuselage at a 
ring frame. This test as well as the "Burst Test" is being undertaken 
as part of the highly esteemed National Aero-Space Plane Program (NASP). 

I also viewed this "burst test", or the Conductive Shield Heat 
Exchanger Sub-Elements X-30 Nozzle Active Cooled Structure Test. The 
purpose of this program is to develop, demonstrate, and validate 
critical technologies required to satisfy the X-30 phase 2 technical 
objectives for actively cooled airframe nozzle structures. This is a 
joint development program between airframe contractors (General 
Dynamics, Rockwell, and McDonnell Douglas) augmented with government 
participation to develop actively cooled nozzle panels. The primary 
objective of the sub-element demonstration is to provide "building 
block" data to support development of large scale nozzle cooled panels. 
WL/FIBT is performing a series of tests to support this program. One of 


17-12 







these tests is a burst test to demonstrate pressure level at which the 
specimen may burst. 


17-13 




CONCLUSIONS: 


Being able to involve myself in so many high technology and 
important tests at a research facility of the stature of the one 1 was 
fortunate enough to find myself at has been an experience of inestimable 
value to me. I was able to work with so many extremely professional 
people in such a productive environment on a consecquential group of 
projects. I learned much about engineering as a career and saw the kind 
of work that I may someday perform. I learned about what government 
work is like, and I must say that I had a very positive experience on 
the Base. I was able to talk to both military and civilian personnel 
concerning work with the Air Force. My knowledge of Structures Testing 
was vastly increased. In addition, I picked up tidbits of wisdom about 
life in general. I am extremely glad that I was chosen for this program 
and decided to accept. 


ACKNOWLEDGEMENTS: 

I wou1d like to 
instruction during my 

Amar Bhunqalia 
Ray Fisher 
Larry Marcum 
John Pappas 


hank the following peopl 
apprenticeship: 

Jim Eichenlaub 
Scott Hamilton 
Doug Dolvin 
Fred Hussong 


for their guidance and 

Tim Si kora 
Larry Kretz 
Ron Dittmer 
Joe Pokorski 


17-14 








THE EFFECTS OF MOISTURE ON THE 


TENSILE STRENGTH OF CONCRETE 


Melissa A. Page 

William S. Strickland - Mentor 


Final Report for: 

AFOSR Summer Research Program 
Wright Laboratory 

WLFIVCS Tyndall AFB, FI 32403-5323 


June 14 - August 6, 1993 


18-1 







THE EFFECTS OF MOISTURE ON THE 
TENSILE STRENGTH OF CONCRETE 

Melissa A. Page 

Student, AFOSR Summer Research Program 
Wright Laboratory 
WLFIYCS, Tyndall AFB FL 32403-5323 

ABSTRACT 

The study measured the effect of moisture on the tensile strength 
of concrete. Forty-six samples were cut from the same J-Mix concrete. 
The samples were divided into three groups; each group was exposed to 
different moisture levels. Sixteen "wet* samples were tested after 
remaining in water for three days. Fifteen 'half-dry" samples were 
tested after being dried in an oven until their moisture was decreased 
by 5X. Fifteen "dry" samples were tested after drying reduced their 
moisture by 10X. Ten samples from each group were tested using the 
Split-Hopkinson Pressure Bar. The results were recorded on an 
oscilloscope; the transmitted wave was used for the calculations. The 
remaining samples from each group were tested on the Forney Machine for 
quasi-static load rates. The data showed that the "dry" samples showed 
higher tensile strength during quasi-static tests while the "wet" 
samples were more resilient at higher load rates in dynamic testing. 


18-2 







THE EFFECTS OF MOISTURE ON THE 


TENSILE STRENGTH OF CONCRETE 
Melissa A. Page 

Introduction 

While a participant in the High School Apprenticeship Program at 
Tyndall Air Force Base, the authoor conducted tests on the effects of 
moisture on the tensile strength of concrete. Previous tests had 
charted the effects of moisture and strain rate on the compressive 
strength of concrete. (Reference 1) Other tests were performed 
concerning the effects of moisture on direct tension. (Reference 2) 

But no experiments had been conducted regarding the effects of moisture 
on the tensile strength of concrete. 

Objective 

How the pressence of moisture will affect a structure has always 
been an important factor in construction. The knowledge of how a 
concrete specimen exposed to moisture will react to both high and low- 
load rates, when placed on a larger scale, can be very useful. The data 
gathered from this experiment can provide insights into how a structure 
exposed to different levels of moisture would hold up under the pressure 
of direct attack. The data can also be applied to domestic issues, such 
as bridges and roads. 


18-3 






Approach 


The concrete samples used in the experiment were cut, sawed, and 
polished -from the same J-Mix concrete. All samples were two inches in 
length and diameter. Forty-six samples in all were tested, thirty in 
dynamic tests, sixteen in quasi-static tests. Sixteen samples were 
tested after remaining in water for three days. For simplicity's sake, 
these are called the "wet" samples. The remaining thirty samples were 
removed from water and dried for different amounts of time in an oven 
until their moisture dropped by a certain percentage. Three samples 
from each group of fifteen were weighed every hour; the measurement was 
then divided by the initial weight to determine the new percentage of 
moisture. Fifteen samples were dried until their moisture was reduced 
by 5K (resulting moisture - 95%); these samples are called 'half-dry." 
The remaining samples had their moisture decreased by 10X (resulting 
moisture - 90 '/.); these samples are called "dry." 

Methodolooy 

The Forney Machine was used to perform the quasi-static tests. Six 
wet samples, five half-dry samples, and five dry samples were divided 
between splitting tensile tests and direct compression tests in the 
following manner: 

Splitting Tensile Direct Compression 

Met 4 2 

Half-dry 3 2 

Dry 3 2 


18-4 






For the spitting tensile tests, the samples went through other 
preparations. Each sample was marked on its top surface across its 
diameter. On either side of the sample two aluminum rods were attached 
using adhesive spray. (Figure 1 and la) These rods were placed on 
either side of the line in order to evenly distribute the load during 
tests. 

The prepared sample was then placed in the Forney Machine so that 
the pressure was first impressed to the aforementioned aluminum rods. 
(Figure 2) Pressure was applied until the sample failed, or reached its 
maximum load. The maximum load was then converted into the load rate 
(pounds/minute). The load was then divided by the static modulus to 
find the strain rate (1/s). The strain rate was then converted into 
pounds per square inch (psi). The individual psi measurements for each 
group were then averaged together. The resulting number, the tensile 
static strength (f'ts), was used later on for other calculations. 

Direct compression tests were run mainly because the information 
gathered is often valued by other researchers. These tests to find the 
compressive strength did offer additional data to compare the samples 
with. (Figure 3) 

The Split-Hopkinson Pressure Bar (SHPB) was used for the dynamic 
tests. It consists of a gas gun, a striker, an incident bar, and a 
tranmitter bar. (Figure 4 and 4a) A release of built-up gas pressure 
from the gas gun causes the striker to hit the incident bar, generating 
a stress wave. This wave travels through the incident bar, 


18-5 





5 







18-7 






| 8 - 8 















the sample, and the transmitter bar. Data is collected from strain 
guages placed equidistance from the sample. The data is registered on 
an oscilloscope, and measurements are taken. (Reference 3) 

The SHP8 was used primarily because of its ability to achieve high 
as well as low-loading rates. The striker can also be changed. For the 
tests, the fifty-inch striker was used for the low range rates (8, 9, 
and 13 psi), while the twenty-six-inch striker was used for higher rates 
<50, 125 psi). 

Ten samples were tested from each group in the SHPB. Every group 
underwent the tests under the same conditions; the only difference was 
their moisture content. This asssured a certain uniformity in the data. 
Each sample was prepared in the same way as the splitting tensile tests 
on the Forney were, and then placed between the incident bar and the 
transmitter bar. (Figure 5) The oscilloscope was set to trigger at 300 
microseconds (uS), and the gas gun was set to the pressure at which the 
test would be conducted (e.g. 15 psi). The gas gun was then fired. (At 
a high pressure, a protective box was placed over the sample.) The 
reading from the oscilloscope was stored, and the sample fragments put 
into bags and labeled. 

After the signal from the SHPB was stored on a disk, the 

oscilloscope was switched to Memory Channel Q3, which displays the part 
of th. stress wav. transmitted through* the sample. Th. transmitted 

M ,u< was then us.d to determine th. stress rate. (Figure 4) 

Stress Rat. - * ” »•* U ° m! ” * 2058 


18-10 



















This number was then divided by two to determine the dynamic 
tensile stress. 

Dynamic Tensile Stress = F'td = C^/2 

The dynamic tensile strength was then divided by the static tensile 
strength (F'ts) -found earlier on the Forney Machine. This set o-f 
numbers became the y-axis on the data charts. 

F'td/F'ts - Dynamic Tensile Strength/Static Tensile Strength 

Other measurements were made using the Q3 Memory Channel. The load 
rate was determined by First Finding the slope oF the line. The two 
points chosen to be the slope were then subtracted From one another, 
leaving a voltage reading and a time reading. The load rate was then 
calculated in the Following manner: 

Load Rate = Moltage Reading x 2058/ 2<Time Reading) 

Other calculations were made to Find the strain rate. 


Data 


Table 1 presents the numerical results From the SHPB and Fornay 
Machine tests. Chart 1 presents the same data in graphical Form. 


18-13 




STRAIN RATE EFFECTS ON CONCRETE STRENGTH 

WET/DRY SPLIT TEN TESTS, J-MIX 








1 


M 


unner> coj 


f <■ % u,\A<> 



4 csA-s 


WET HALF DRY DRY 


-7.13 

093 



-7.4 

0.98 



4U 

1.1 



1 

3.65 



0.87 

2.74 



0.78 

3.36 



0.7S 

2.98 



0.44 

2.98 



0.42 

2.94 



0.81 

4.02 



•0.33 

2.51 



•0.21 

2.65 . 



0.22 

3.2 



-0.17 


1.89 


-0.2S 


2.24 


-0.27 


2.31 


0.75 


359 


0.37 


2.53 


0.81 


3.06 


0.75 


308 


0.81 


3.65 


a73 


3.54 


0.8 


3.46 


-7.3 


0.99 


-7.2 


0.93 


-7.28 


1.07 


-7.3 



099 

-7.2 



0.93 

-7.28 



1.07 

-0.07 



1.85 

•0.42 



1.43 

-0.47 



1.47 

075 



2.77 

05 



1.89 

0.54 



1.92 

0.83 



2.99 

088 



2.76 

081 



2.55 

0.84 



2.84 

-7 




03 




0812 




-7 




03 




1.02 




-7 




03 




1.415 





1 

2.7 

4 


WET 


18-15 



e \ ( Co*"vC.Wd« cJ. \ 


X Oat* HALF ORY ORY 


-7.13 * 
-7.4 

408 

1 

a07 

a78 

a7s 

0.44 

0.42 

0.01 

•0.33 

421 

a22 

417 

425 

427 

0.75 

0.37 

aei 

a75 

0.01 

0.73 

as 

-7.3 

-7.2 

•7.28 


-7.3 

-7.2 

-7.28 

407 

442 

447 

0.75 

as 

a54 

0.93 

aso 

aoi 

0.94 

-7 

0.3 

0.812 

-7 

as 

1.02 

-7 

as 

1.415 


1 

2.3 

4 


1 

1.7 

4 

18-1 6 


•Mat 




Conclusions 


When the SHPB and Forney Machine test results were plotted, the 
graph showed that the dry samples had a higher strength in static tests, 
and the wet samples were stronger during dynamic tests. (Chart 1 and 
Table 1) It also appeared that more aggregate broke during the dry and 
hal-f-dry tests than during the wet tests. The direct compression tests 
conducted on the Forney Machine showed that the dry samples had a 
greater compressive strength than did the wet and hal-f-dry samples. 

Based on the data, the -following conclusions on the e-f-fects o-f 
moisture on the tensile strength o-f concrete can be made: 

1. High moisture levels generate a high strain rate sensitivity in 
dynamic testing. 

2. Low moisture levels yield a higher compressive strength. 


18-1 T 




REFERENCES 


1. Ross, C.A., Tedesco, J.W., and Kuennen, S.T., "Effects of Strain Rate 
on Concrete Strength,* accepted for publiation in AC! Materials Journal. 

2. Reinhardt, H.W., Rossi, P., andMierran, J.6.M., "Joint Investigation 
of Concrete at High Rates of Loading,* Materials and Structures 23, 

1990, pp 213-216. 

3. Ross, C.A., *Split-Hopkinson Pressure Bar Test,* HQ Air FOrce 
Engineering and Services Center, Tyndall AFB, FI, 1989. 


18-16 





MODIFICATION OF M18 FOR EMAA EXTINGUISHING PURPOSES 


Jefferey R. Strickland 
High School Research Student 


Final Report for: 
Summer Research Program 
Wright Laboratory 


Sponsored by: 

Air Force Office of Scientific Research 
Wright Laboratory, Panama City, FL 


August 1993 


19-1 





MODIFICATION OF M18 FOR EMAA EXTINGUISHING PURPOSES 


Jefferey R. Strickland 
High School Research Student 

Abstract 

A new class of fires suppressants, known as Encapsulated 
Micron Aerosol Agents (EMAA), having superior volumetric 
efficiency, low initial and life cycle costs, low toxicity, no 
known global atmospheric environmental impacts, and with the 
potential for a wide variety of applications, is being 
developed through a joint program between the private sector 
and the U. S. Air Force. Through the modification of the M18 
smoke grenade an evaluation of a portable method for the 
delivery of EMAA can be accomplished. This innovative 
application is one of the many possible with this promising new 
agent. 


19-2 




MODIFICATION OF M18 FOR EMAA EXTINGUISHING PURPOSES 


Jefferey R. Strickland 


The search for replacements and alternatives for the halon 
family of chemical fire suppressants has coincided with the 
development of novel materials and techniques that provide new 
options for fire protection. One class of materials that has 
good potential for filling several roles formerly performed by 
halons is aerosols. The U.S. Air Force has entered into a 
Cooperative Research and Development Agreement with Spectrex, 
Inc. to further the development of aerosol technology for a 
number of fire suppression applications. Encapsulated Micron 
Aerosol Agents (EMAA) is the title given by the U.S. Air Force 
to this research program. By implementing old technology such 
as the M18 smoke grenade unique and applicable apparatuses for 
safety enhanced and portable use are available. 


MEIBQ P QL OQ X 

Aerosol science or particle mechanics draws from several 
scientific disciplines to formulate the science that underlies 
its principle areas of research. Aerosol refers to a system of 
liquid or solid particles suspended in a gaseous medium. 

Several common aerosols are fumes, smoke, mists, fog, and 
haze. Based on the state of the suspended substance, liquid or 


19-3 




solid, dispersion and condensation aerosols are 
differentiated. Dispersion aerosols are formed by the 
atomization of solids and liquids while condensation aerosols 
are formed through the condensation of superheated vapors or 
chemical reaction in the gaseous state. EMAA is a dispersion 
aerosol that is delivered to the protected space through the 
combustion of the solid tablet. Prior to EMAA, dispersion 
aerosols have been created through crushing, grinding, 
blasting, or drilling of solid matter. The dynamics of 
aerosols are important considerations for two reasons. First, 
the ability ot the particles to the particle size and the 
residence time of the fire suppressant. Second, the aerosol, 
if it is to replace gases in certain applications, must be able 
to flow around obstacles. 

EMAA is initially a solid material that can originate in a 
variety of forms: solid, powder, or gel. The active component, 
an oxidizer, and a reducer are combined with a filler. These 
components are ground into a fine powder and mixed with an 
epoxy resin binder. Upon ignition of the material, the 
combustion products are ejected as a dispersion aerosol, with 
the solid particles floating in the air and the gaseous 
combustion products. 

Successful fire suppression requires that one or more of 
the four factors that tend to propagate a fire be interrupted. 
Theses factors together with their suppression mechanisms are 
listed on the following table. 


19-4 




Factor 


Method 


Fuel 

Oxygen 

Heat 

Chain reaction 


Suppression Mechanism 

Removal 

Exclusion 

Absorbance 

Inhibition 


Vapor seal 
Smothering 
Cooling 
Stop Reaction 


EMAA aerosols are hypothesized to function through several 
mechanisms to suppress fire, the most prominent of which is 
chemical inhibition of the chain reaction. Other mechanisms 
such as heat absorption are also possible while oxygen 
exclusion is not a path for aerosol fire suppression. 


APPARATUS 

The M18 Smoke Hand Grenade is the apparatus one of the 
innovative applications used in EMAA fire extinguishing. This 
grenade will emit red smoke for 50 to 90 seconds. Safety clips 
are not required with these grenades. 

The grenade body, composed of thin sheet metal, is filled 
with red smoke composition. The filler is topped with a 
starter mixture. 

The Hand Grenade Fuze, M201A1 is a pyrotechnic 
delay-igniting fuze. The body contains a primer, first-fire 
mixture, pyrotechnic delay column, and ignition mixture. 
Assembled to the body are a striker, striker spring, safety 
lever, and safety pin with pull ring. The split end of the 


19-5 







safety pin has a angular spread. 

Removal of the M201A1 Fuze safety pin permits release of 
the safety lever. When the safety lever is released, it is 
forced away from the grenade body by a striker acting under the 
force of a striker spring. The striker rotates on its own axis 
and strikes the percussion primer. The primer initiates the 
first-fire mixture. The fuze delay element, ignition mixture, 
and grenade starter mixture and filler are initiated in turn by 
the preceding component. The pressure sensitive tape is blown 
off the emission holes and the colored smoke emits from these 
holes. 

With these specified modifications the M18 can be 
implemented as portable and safe addition to fire fighting 
equipment. See (Figure 1). 

1. Replace smoke composition, filler, with Encapsulate 
Micron Aerosol Agent (EMAA). This replacement allows 
for affective fire suppression. 

2. Expand canister to compensate for specified dimension 
of EMAA 16 x 7.4 cm. To extinguish 20 cubic meters of 
fire one kilogram or 630 cubic centimeters of EMAA is 
required. 

3. Replace current starter mixture with a mixture capable 
of, at minimum, a 2000 K combustion temperature to 
compensate for EMAA ignition temperature. 


19-6 





Employ alternating buffers for control of EMAA fire 
emissions. If fire emissions are not controlled it is 
possible to reignite extinguished material. This is 
a simple but necessary advancement for safety as well as 
mechanical improvement. 






SECTION 



















EXPERIENCE IN THE METHODOLOGY SECTION AT 
AT WRIGHT-PATTERSON AIR FORCE BASE 


Miranda H. T. Tseng 
High School Apprentice 
Methodology Section 


Wright-Patterson Air Force Base 
WL/FIVS 

1901 Tenth Street 
WPAFB, Ohio 45433-7605 


Final Report for: 

Air Force Office of Scientific Research 
Sumner Research Program 


Sponsored by: 

Air Force Office of Scientific Research 
Wright-Patterson Air Force Base, Dayton, Ohio 


20-1 




EXPERIENCE IN THE METHODOLOGY SECTION AT 
WRIGHT-PATTERSON AIR FORCE BASE 


Miranda H.T. Tseng 
High School Apprentice 
Methodology Section 
Wright-Patterson Air Force Base 

Abstract 

In the Survivability Enhancement Branch of the Vehicle Subsystem 
Division, information was obtained about the survivability and vulnerability 
of composites through basic research and testing conducted in a gunnery range. 
Different types of data were collected from the tested composite panel and 
from associated reports. Accumulated results were put into a data base that 
contained raw data from previous tests. This data base was then updated and 
completed. The findings were in turn reduced and analyzed for further use. 
Plans for future tests to support methodology development were also made. 

These plans included the determination of data that will be needed to support 
the thesis of these developmental tests. Besides research settings, a general 
work atmosphere was observed and the ability to fit into the office 
environment was gained. 


20-2 






EXPERIENCE IN THE METHODOLOGY SECTION 
AT WRIGHT-PATTERSON AIR FORCE BASE 


Miranda H. T. Tseng 

The vulnerability of aircraft and other military vehicles is inportant 
in the survival rate of its operators. The extent of this vulnerability is 
tested by the Survivability Enhancement Branch of the Vehicle Subsystems 
Division. This department of the Flight Dynamics Directorate specifically 
tests composite panels from military vehicles by simulating possible 
threatening engagements with enemy projectiles. The damage is then 
evaluated and possible improvements are tried. Throughout this cycle, 
military armorment is gradually improved. 

Tests are run at one of four ranges at Wright-Patterson Air Force Base 
in Dayton, Ohio. At the range, a gas or blade powder gun is set up to fire 
either missile warhead fragment simulators or armor-piercing incendiary 
rounds. At the front of the muzzle of the gun, the panel being tested is 
set up with sensors at the front or back of the it. Possible sensors used 
cure break papers, electromagnetic coils, light screens, and wire triggers. 
These sensors time the projectile so that impact and residual velocities 
along with other variables may be calculated. Cameras triggered to open 
their shutters before the projectile is shot are set up to capture the 
firing of the round. The cameras can also record any fireballs that ignite 
when the projectile hits the composite panel. If a fireball starts and is 
caught on film, it is then classified into five possible categories that 
describe its incendiary ignition. 

Besides fireballs and projectile timing, other data are collected or 
calculated. Before testing, the impact and residual weights of the panel 
and projectile are recorded. Details from the panel such as lay-up and 
material type are also documented. Furthermore, various facts from the test 
set-up, including obliquity angle and projectile size, are noted. After the 
panel has been damaged, data are taken off the panel and a C-scan taken off 
it. Fran the panel itself, maximum deep delamination is collected along 


20-3 





with the distance of the damage along the x- and y-axis. The same 
information is taken off of the perforation in the panel. From the C-scan, 
total area is determined in addition to the three things mentioned above. 
Another type of data is taken from the witness panel, which is set further 
down the range. It is used to record the amount of yaw the projectile 
gains. These data are used to determine what the target will do to the 
projectile. It is also used to predict information relative to a second 
panel. Plans for a future test to verify these predictions are being made. 

After the data are collected, they are either analyzed or used in 
collaboration with other tests. The purpose of evaluating data is to find 
relationships between the different variables using graphs. Variables are 
especially noted when the display of the results show multiple graphs to be 
identical. This information is used in future tests to see if the same 
relationships are repeated. If it is, a theory is usually developed. The 
data are also used for predicting algorithms. When patterns in the data are 
observed, the data are then used to derive equations to predict the terminal 
ballistics of the projectile. In turn, the data is then used in 
vulnerability models to predict the levels of vulnerability of military 
vehicles. 

As the armorment of military vehicles advances, future testing of it 
must be planned. The planning involves making up a test plan that includes 
the configuration of the range. Arrangements must also be made for the 
collection and organization of data. This process involves establishing the 
type of data needed along with the method in which the data will be 
collected. Planning also entails signing up for the range and making sure 
that people are available to work at the test site. These arrangements take 
variable amounts of time but often become quite tedious and time- consuming. 

This apprenticeship gave insight to the testing process from the 
beginning to the end. A key tool used in the testing process was the 
personal computer. Wright-Patterson Air Force Base offered many 
opportunities to learn various computer programs. Throughout the summer, 
the student mastered programs such as "Excel" and "Harvard Graphics". 
Proficiency in such programs will help open future opportunities. 


20-4 





Series 1 


3 — Series2 
— Series3 
5 — Serie$4 
■— Serie$5 
‘— Series6 
’— Series7 


Series8 












Experience in the computer system "Silicon Graphics" was also gained. 

Simple experimentation allowed further knowledge in basic computer use. 
Knowledge in non-electronic tools was also acquired. Understanding 
planimeters and calipers will be useful when electronic devices such as 
calculators and computers are not available. Since comprehension of 
non-electronic tools is rare, this learning experience was most worthwhile. 
In addition general observation of the work atmosphere provided experience 
in fitting into an office environment. Sitting at a desk for a long time was 
a new experience. A work attitude was also developed since the behavior 
called for in an office is quite different from the behavior required in 
school. It was also necessary to learn how to deal with people in person 
and on the phone. These skills will be a great benefit in any office or 
job. 


This sunnier job sponsored by Research and Development Laboratories 
provided many benefits for students interested in jobs heavily based on math 
and science. By spending time in an engineer's environment, a student can 
learn a great deal about what an engineer actually does. He or she sees not 
only the finished product the engineer produces but also all the hard work 
that goes into generating the result. Advantageous as this job is, a few 
improvements could be made. The first suggestion is to find a way to 
standardize the jobs since the work the students do varies greatly. Sane 
have jobs that confine then to desks and computers in a closed room all day 
while others have hands-on, active positions. Blending the two types of 
work would give a more realistic demonstration of a permanent job since work 
generally does not consist of only desk work or laboratory work, but is a 
combination of both. Patterning the jobs would also give mentors ideas on 
what the students can do. Since the position is only for a summer, the time 
should be spent wisely. However much time is wasted when the mentor must 
discover the interests and talents of the student. If a survey of interest 
is given along with the application, the mentor may have a better idea of 
the student's capabilities and be able to create a work plan for the summer. 

As this position is reaching an end, the testing procedure begins to 
repeat its cycle. It will not reiterate merely through a scientist's 
procedure but through a student's unique learning experience. As stated 


20-6 




before, the high school apprenticeship program sponsored by Research and 
Development Laboratories was very beneficial. It allowed a student to 
continue their learning process through the summer. The apprentice not only 
learned skills useful in college but abilities that can be used throughout 
the future. This job provided a good transition from the educational 
environment to the working world. 


20-7 





COLOR NEUTRAL RUGATE RLTERS 


Jessica M. Behm 


Final Report for: 

AFOSR Summer Research Program 
Wright Laboratory 


Sponsored by: 

Air Force Office of Scientific Research 
Wright Patterson Air Force Base, Dayton, Oh. 


August 1993 


21-1 





COLOR NEUTRAL RUGATE FILTERS 


Jessica M. Behm 


AB STR ACT 

The description of a transmissive rugate filter designed to reflect a portion of the 
visible spectrum and yet not appear to have a dominant color. The filter design criteria 
were chosen so that the filter rejects portions of the visible spectrum from 0.38 to 
0.78 microns. Observing a scene through this type of optical filter one perceives it to 
be color neutral, although it is somewhat darker. The design constraints require a solar 
light source. The eye bounds the wavelength range over which perceived coloration is 
affected. For this work the spectral characteristics of both the incident light and the 
standard human eye determine the spectral tailoring of the reflection bands. Rugate 
filter design and fabrication technology permits a very wide variety in the number, 
location, bandwidth, and peak height of all reflection bands. The result is a color neutral 
rugate filter having reflection bands tailored to provide the human user with maximum 
color discrimination capability. 


21-2 





COLOR NEUTRAL RUGATE FILTERS 


JESSICA M. BEHM 
INTRODUCTION 

There exists a wealth of research in the field of color science such as documented by textbooks 
devoted to the topic . 1 >2 a methodology has been developed to calculate the color qualities of a 
transmissive filter illuminated by a variety of standard sources as viewed by the human eye. This 
paper presents results obtained from applying this methodology to rugate filter designs. The spectral 
performance of rugate filters can be tailored by altering the refractive index profile of the filter. 
The goal is to design color neutral rugate filters through refractive index profile tailoring. It shall 
proceed from a rugate filter having a single reflection band located in the visible portion of the 
spectrum. All subsequent rugate filter designs shall maintain the same coating and optical thickness. 
In addition, the changes to the spectral location and bandwidth of the original reflection band shall be 
minimized. The following calculations assume that light has a normal angle of incidence for all the 
rugate filter designs presented. 


SINGLE-BAND RUGATE FILTER 


This paper shall start with a rugate coating having a simple sine wave refractive index profile 
deposited onto a glass substrate with a refractive index of 1.52 (Fig. la). The five parameters that 
define this sine wave refractive index profile are: 


• n a (average refractive index ) 

• n pv (peak-to-valley refractive index excursion) 

• n a P (period of the sine wave in optical thickness) 

• q (phase at the substrate in radians) 

• N (number of sine wave cycles) 


2.0 
0.15 
0.22 pm 
0 

100.5 


x 

UJ 

o 

z 


UJ 

> 

►- 

o 

< 

E 


UJ 

E 


2.1 *j 



A 

T 

E 


1.5 f r » i i | i i i i |-< i i !■ | i i i i | i i -'i i | i i i i | 

0 5 10 15 20 25 

OPTICAL THICKNESS (microns) 

Fig. 1 a. Sine wave refractive index profile. 


This optical coating has an optical thickness of 22.11 pm .vhich produces a single reflection band in 
the visible spectrum. The reflection band located at 0.44 pm and has a full width at half maximum 
(FWHM) bandwidth of approximately 17 nm (Fig. 1b). Note that the spectral performance of this 
rugate filter is shown over the visible wavelength range, defined to be from 0.38 to 0.78 pm. 


21-3 






The Commission International de I'Eclairage (CIE) or International Commission on Illumination 
recommends standards for describing the source of illumination. Solar light shall be used for the 
illuminant. Although there are others, one CIE standard source is recommended for general use. This is 
standard source D 65 which simulates average natural daylight (Fig. 2). Therefore, this shall be the only 
source of illumination modeled in this paper. 



WAVELENGTH (microns) 


Fig. 2. CIE standard source D 65 which simulates average natural daylight 

A standard observer having color vision which represents the average human with normal color 
vision has also been identified by the CIE. It is the intent to use the recommended CIE standard observer 
rather than attempt to discuss how one arrives at such a standard. However, a simple understanding of 
the CIE standard observer is useful. The intensities of three narrow bands of red, green and blue light 
are combined and adjusted to match an adjacent test color. This results in three values (tristimulus 
values) describing the test color. However, one of the three narrow bands of light may be combined with 
the test color rather than the other two remaining bands to achieve the match. For this case, the narrow 
band of light that is combined with the test color has a negative value. However, all negative tristimulus 
values disappear by performing a mathematical transformation. The resulting positive values are 

21-4 











identified as the CIE color matching functions x , y , and z (Fig. 3). Note that z has a non zero 
value over the 0.38 to 0.55 pm range in wavelength. 

2.5 

(A 

S3 2 

< 

> 1.5 

</) 

3 

3 1 

3 

22 0.5 

s 

H 

0 

0.3 0.4 0.5 0.6 0.7 0.8 

WAVELENGTH (microns) 

Fig. 3. CIE color matching functions x (—), y (-), and z ( -) 

A spectrum of color is plotted from the ratio of the three CIE color matching functions to their sum as 
a function of wavelength. The three resulting values are commonly referred to as chromaticity 
coordinates x. y, and z and sum to unity. Therefore, it is sufficient to work with any two of the three 
chromaticity coordinates. Accepted practice results in a plot of the x versus y chromaticity coordinate 
and are termed a chromaticity diagram (Fig. 4). The spectrum of color as a function of wavelength 
bounds the chromaticity diagram by the well known horseshoe-shape curve. All real colors are 
contained within this closed horseshoe-shape curve. 

i 

0.8 

0.6 

>» 

0.4 

0.2 
0 

0 0.2 0.4 0.6 0.8 

x 

Fig. 4. 1931 chromaticity diagram. Wavelengths in microns are identified along the horseshoe-shaped 
spectrum. Standard source 065 is located at (x) and the filter of Fig. 1 is located at (+). 

CIE standard source 065 has coordinates of x = 0.313 and y = 0.329 (z = 0.358) on the CIE 1931 
chromaticity diagram. The location of this point is denoted by the (x) in Fig. 4. This point on the 
chromaticity diagram is assumed to be where one perceives no coloration, i.e., the color is neutral. 
Therefore, an optical filter having chromaticity coordinates matching those of CIE standard source D 65 is 
defined to be color neutral. 

The method of calculating the chromaticity coordinates (rounded to the nearest thousandth's place) 
for the optical filters modeled in this paper account for the illuminant, filters, and observer which are 
the CIE standard source D 65 , rugate filters, and CIE standard observer respectively. Billmeyer and 
Saltzman present a clear description and schematic of the methodology for this calculation. 2 However, 
simply stated the products of the illuminant, rugate filter, and three CIE color matching functions are 
divided by their sum from 0.38 to 0.78 pm. The resulting three values are the chromaticity coordinates 

21-5 











for a standard observer viewing light transmitted through the filter. The single reflection band 
rugate filter described in Fig. 1 has chromaticity coordinates of x = 0.343 and y = 0.389 (z = 0.268) 
which is located by the cross symbol (+) in Fig. 4. The total throughput of this single band rugate filter 
is 0.79 over the 0.38 to 0.78 pm wavelength range. 

An additional measure of rugate filter performance viewed by the human eye is determined by 
daytime (photopic) and nighttime (scotopic) vision. A spectral shift in performance to shorter 
wavelengths occurs between the CIE standard photopic and scotopic observer (Fig. 5). The numeric 
values for photopic and scotopic vision are the total intensity of light viewed through a rugate filter by 
the human eye relative to the intensity viewed by the human eye with the rugate filter. The photopic and 
scotopic values for the single band of Fig. 1 are 0.83 and 0.77 respectively. 



Fig. 5. Luminous efficiency of the eye for photopic (-) and scotopic (--) vision. 

THE COLOR NEUTRAL RUGATE FILTER 

In order to produce a color neutral rugate filter a second sine wave can be tailored and superimposed 
to the original sine wave refractive index profile described in Fig. la. This second sine wave is defined 
by npy = 0.212, n a P = 0.284 pm, q = 1.2761... n, and N = 77.852 cycles. The optical thickness of the 
resulting rugate coating is still 22.11 pm (Fig. 6a). 


x 

LU 

o 


LU 

> 

O 

< 

QC 

U. 

UJ 

CC 


0 5 10 15 20 25 

OPTICAL THICKNESS (microns) 

Fig. 6a. Refractive index profile resulting from the superposition of two sine waves. 



21-6 








The presence of this second sine wave yields an additional reflection band having a FWHM bandwidth 
of approximately 32 nm located at 0.568 pm (Fig. 6b). Note that this reflection band is located in the 
wavelength region where z is effectively zero. This tailored two band rugate filter has chromaticity 
coordinates of x = 0.313 and y = 0.329 (z = 0.358). The photopic and scotopic values for this filter are 
0.54 and 0.69 respectively. While the total throughput of this filter from 0.38 to 0.78 pm is 0.70. 



WAVELENGTH (microns) 

Fig. 6b. Transmittance spectrum of a color neutral rugate filter (x = 0.313, y = 0.329, and z = 0.358) 
having two reflection bands. 

Overall performance of this rugate filter can be improved with the inclusion of refractive index 
matching at the substrate/coating and coating/air interfaces and refractive index profile apodization. 
The original refractive index sine wave described in Fig 1 is superimposed with a sine wave defined by 
n pv = 0.16, n a P = 0.2835 pm, q = 1.75 n, and N = 77.989 cycles (Fig. 7a). 



0 5 10 15 20 25 

OPTICAL THICKNESS (microns) 


Fig. 7a. Refractive index profile resulting from the superposition of two sine waves which are apodized 
and index matched at the substrate and air interfaces. 

The chromaticity coordinates for this color neutral rugate filter are x = 0.313 and y = 0.329 (z = 
0.358). Thereby, resulting in a two band color neutral rugate filter having enhanced throughput (Fig. 
7b). The FWHM bandwidth of the reflection band at 0.567 pm is approximately 27 nm. The photopic 
and scotopic values for this filter are 0.71 and 0.82 with a total throughput of 0.82 over the 0.38 to 
0.78 pm wavelength region. Note that this filter has performance values that compare well with the 
original single band filter except for the photopic vision which suffers from the presence of the 
reflection band at 0.567 pm. 


21-7 






Fig. 7b. Transmittance spectrum of a color neutral rugate filter (x = 0.313, y = 0.329, and z = 
0.358)having two reflection bands with enhanced throughput. 


There are other designs that can yield results similar to the previous filter design. For example, a 
sine wave having n pv = 0.28, n a P = 0.284 pm, q = 1.23... n, and N = 8.5 cycles can be superpositioned 
with the original sine wave refractive index profile of Fig. la. This filter design also includes refractive 
index matching and refractive index profile apodization (Fig. 8a). Note from Fig. 8a that this second sine 
wave starts at the air/coating interface and continues only a short distance into the coating. 


APODIZATION 


APODIZATION 


X 

Ul 

o 


Ul 

> 

o 

< 

a 

u. 

Ul 

a 


2.1-3 



0 5 10 15 20 25 

OPTICAL THICKNESS (microns) 


Fig. 8a. Refractive index proi ’e resulting from the superposition of two sine waves (one extending only 
from 19.7 pm to the air interface-; which are apodized and index matched at the substrate and air 
interfaces. 


This second sine wave produces a shallow broad reflection band at 0.568 pm. This band has a 
transmittance minimum of 0.53 and a FWHM bandwidth of approximately 75 nm (Fig. 8b). However, 
this filter design transmits light at the wavelengths used in changing chromaticity coordinates. The 
chromaticity coordinates for this filter are x = 0.313 and y = 0.329 (z = 0.358). The photopic and 
scotopic values for this filter are 0.71 and 0.78 with a total throughput of 0.82 over the 0.38 to 0.78 
pm wavelength range. These three performance values agree with those of the two band filter described 
in Fig. 7 except for scotopic vision which decreased by 0.04 (i.e., from 0.82 to 0.78). 


21-8 









Fig. 8b. Transmittance spectrum of a color neutral rugate filter (x = 0.313, y = 0.329, and z = 0.358) 
having one deep narrow and one shallow broad reflection band with enhanced throughput. 




-U S t Q N 


This paper demonstrates that color neutral rugate filters can be designed by refractive index profile 
tailoring. It assumes a rugate filter having a single reflection band is located at 0.44 pm, however, this 
band could of easily have been located anywhere in the visible. This would result in a different starting 
point on the chromaticity diagram (i.e., different x and y values). Additional refractive index sine waves 
and profile tailoring the chromaticity coordinates of the resulting filter can be reasonably controlled. 
More than one rugate filter design can be used for changing the chromaticity coordinates to obtain the 
location of choice on the chromaticity diagram. This paper assumes that the chromaticity coordinates of 
CIE standard source D65 is the optimum location in obtaining a color neutral rugate filter. However, this 
may not be the ultimate desired location. This could be especially true when accounting for the spectral 
shift to shorter wavelengths exhibited by rugate filters that encounter light having non normal angles of 
incidence. 


R EBE RTCE S 

1 . G. Wyszecki and W. S. Stiles, Color science concepts and methods, quantitative data and formulas. 
John Wiley and Sons, New York, 1957. 

2. F. W. Billmeyer and M. Saltzman, Principles of color technology, pages 45 and 46, John Wiley and 
Sons, New York, 1981. 


21-9 




A STUDY OF METHODS OF DEVELOPING A 
CARBON-CARBON STRUCTURAL FOAM 


Jeremy D. Focht 

Beavercreek High School Graduate 


Final Report for: 

AFOSR Summer Research Program 
Wright Laboratories 
Nonmetallic Structural Materials Branch 
Wright Patterson Air Force Base 
Beavercreek, Ohio 45432 


Sponsored by: 

Air Force Office of Scientific Research 


August 1993 


22-1 





A STUDY OF METHODS OF DEVELOPING A 
CARBON-CARBON STRUCTURAL FOAM 

Jeremy D. Focht 

Beavercreek High School Graduate 

Abstract 

The formation of a carbon-carbon foam that would possess all the characteristics of a solid 
composite material was desired in this experiment. A carbon mesophase pitch was used to form the actual 
carbon-carbon material itself. In order to create the kind of foam desired, two methods were attempted. 
The first was the mixing of the pitch with a nitrogen-releasing blowing agent. The second was the 
saturating of the pitch with highly pressurized toluene. Experimental results show that the use of the 
blowing agent creates foams with ceils too big to retain any practical portion of the strength of the 
composite material itself. Pressurizing the pitch with toluene showed much better results with cell sizes 
within acceptable limits. However, foe foams created by this method were shown to contain closed cell 
structures, which indicates incomplete penetration of the pitch and consequently, incomplete foaming in 
certain parts of the sample. 





A STUDY OF METHODS OF DEVELOPING A 
CARBON-CARBON STRUCTURAL FOAM 


Jeremy D. Focht 

Introduction 

In the past ten years, much research has been done in the field of composite materials. This has 
been because of their much superior strength and much lighter weight than metals and other materials. 
Recently, carbon-carbon has been researched for use in the aerospace field because of its extremely light 
weight, superior tensile strength, and its almost nonexistent expansion when exposed to heat. The 
strength of carbon-carbon composites comes from the alignment of carbon molecules in long strands. 

The belief is that when a carbon-carbon sample is foamed, the carbon strands will still retain their alignment 
through the ligaments of the foam created. By retaining this alignment, the strength of the material will be 
preserved as long as the cell size of the foam remains consistent throughout the sample. The cells cannot 
be larger than 30 microns to 1 mm in diameter if the strength of the material is to be preserved (the 
theoretical size varies, depending upon the source consulted). By using a carbon-carbon composite 
foam instead of a solid composite, weight could be saved without sacrificing the strength of the material 
which holds much interest with the aerospace field. Also by using a foam, much of the volume taken up by 
the structure is actually empty space, which will save on the amount of material used to create the 
composite. Since the materials to create carbon-carbon are still not cheap, this would lead to the saving of 
money in manufacturing. 

Methodology 

The first method used to create carbon-carbon foams is the mixing of the 
carbon-carbon medium with a blowing agent. The foam is created by heating the medium to its softening 
point. At this point, the blowing agent releases gas through a chemical decomposition reaction and 
creates multitudes of spherical cells evenly spread out throughout the medium of roughly the same size. 
By adjusting the ratio of the amount of blowing agent to the amount of medium, the size of the cells can 
theoretically be controlled. This method relies heavily on the complete mixing of the two media, as well as 
the even distribution of the blowing agent throughout the entire medium. It also requires a blowing agent 


22-3 





that releases a gas at or slightly below the medium 's softening point Also since this is a carbon-carbon 
material being foamed, the gas released should not be oxygen. Oxygen gas would immediately oxygen 
stabilize the medium and make it rigid upon the first exposure to 02 gas. preventing further expansion in 
the foam cell structure. Another variable in the formation of the foam using a blowing agent is the duration 
at the medium's softening temperature. If not enough time is spent at the medium softening point, the 
blowing agent will not release enough gas to foam the medium to the desired porosity. If too much time is 
spent at the softening point, large cell structures will be formed that cause the medium to be brittle and 
weak instead of displaying the desired composite strength. 

Another method used in developing a carbon-carbon foam is that of saturating the medium with a 
highly pressurized liquid. The foam is created by first saturating the medium with this liquid at a high 
pressure. Then the medium is heated to its softening point, expanding the liquid trapped inside the solid 
and creating a uniform foam cell structure. The size of the foam cells can be controlled by adjusting the 
pressure at which the medium is saturated. Again, the duration at the softening point should affect the 
degree of porosity and size of the foam cells. 

Experimental Setup and Procedure 

The medium used for methods of producing foam is AR 760 Mesophase Pitch (obtained from 
Mitsubishi). In the first method, the blowing agent used was EXPANDEX 175™ It releases nitrogen gas at 
anywhere from 225 °C to 250 °C. The pitch's softening point was 240 °C. The pitch was heated to a liquid 
state and the blowing agent was added in a 2.5 grams of pitch to 1 gram of blowing agent ratio. The 
mixture was very thoroughly mixed and then allowed to cool. Samples were then created of approximately 
2.5 grams each. Each sample was placed in a programmable ashing furnace and heated in the presence 
of nitrogen to 240 °C at varying rates of heating from 5 °C per minute to 25 °C per minute. All samples were 
allowed to cool by letting the heat escape by opening the furnace to the outside environment. 

The second method was carried out by placing an approximately 2.5 gram sample of the pure 
pitch (without blowing agent) in a glass beaker submerged in 

80 mL of toluene. This glass beaker was then transferred to a pressure furnace and nitrogen was then 
pumped into the furnace until the desired pressure was reached. The furnace was kept at room 


22-4 




temperature and at a constant pressure for 24 to 72 hours (depending on the test) The samples were 
pressurized from 600 psi to 1600 psi (again depending on the test) The sample was then removed from 
the toluene, weighed (after letting the excess toluene evaporate), and placed in an aluminum foti 
container to be heated in a programmable ashing furnace. It was heated in the presence of nitrogen gas 
to prevent oxygen stabilization. The sample was then heated at variable rates (from 5 °C per minute to 50 
°C per minute) to 240 °C or 250 °C. 

Results 

The first method of mixing the pitch with a blowing agent produced foams with a wide range of cell 
sizes. The cells were far too large to be useful in any sort of structural use (2 mm to 1 cm in diameter) The 
cell size could be controlled somewhat by the heating rate, but the gas was released too much all at once, 
no matter how quickly it was heated. The different sizes of cells in the foam were believed to be caused by 
the blowing agent’s concentration being more in one area than the other. This ratio was determined to be 
impractical. Other ratios of blowing agent to pitch might yield better foams, but the tests are not yet 
completed. Instead, another method was pursued because of the difficulty of cooling it quickly enough to 
release only part of the gas in the blowing agent. 

The second method yielded much better results. The foams created had cells that were much 
more uniform in size. The foam ceil size was within the practical limits for structural use listed earlier in this 
paper. It was found that the heating rate of the saturated pitch had little to no effect on the foams. The 
length of the pressurization also had little effect, as long as it was pressurized for 24 hours (any less was 
not within the scope of this experiment). The samples pressurized for 72 hours did not show any more 
weight gain than those pressurized at the same pressure for only 24 hours. However, the pressure at 
which the pitch was saturated proved to be a control for the size of the cells formed. The greater the 
pressure, the larger the cells. While the foams made up until now have many of the characteristics 
desired, they contained some closed cells, which may indicate incomplete saturation of the toluene 
throughout the pitch. The pitch might be heated to its softening point and then pressurized while it is 
cooling to saturate it evenly throughout. But all in all, this method seems much more conducive to the 
manufacture of carbon-carbon foams. 





Related Project 

The discussion of the foams has been of the size of ceils and not the strength or structure One 
pursuit at the Wright Laboratories is to understand what structures in foams are strong and what are not 
They examine this by plotting the X and Y coordinates of a cross-section of foam, cutting away a thin layer 
of foam (30 to 40 microns at most), and plotting the next cross-section of foam. In this way. they can place 
these cross sections on top of one another using a CAO (Computer Aided Design) program and view the 
ligament structure of the foam at a microscopic level. 

One of the problems run across was that the plotting instrument exported only X and Y 
coordinates. In order to view them on top of one another, a Z coordinate must be assigned to each set of 
X and Y coordinates. Each layer consists of up to a dozen different shapes and each shape consists of 
hundreds of X and Y pairs. While the adding of a Z coordinate to each pair could be done by manipulating 
the file with different word processing and spreadsheet programs, the process took hours to accomplish 
for just one layer. Once this was accomplished, the data had to be inserted with some code so that the 
CAD program that they used to view the data could plot them in a three-dimensional layout. In addition, 
they also wanted to thicken each cross section in order that they could be viewed more clearly. However, 
this was another process that would take a considerable amount of time. 

This was given to me as a process to automate using a computer program. By using THINK 
Pascal™ as a programming tool. I wrote a computer program to take the X and Y coordinate files from the 
plotter program and convert them into code that the CAD program could understand. I also enabled each 
set of data created to give the user an option for the thickness of each layer displayed. This computer 
program could then process a dozen layers in less than one hour, saving days of human labor. It also 
will enable them to see the layers more clearly using the thickening option installed with every set of data 
given to the CAD program. 

This took a considerable amount of time to program at the laboratories, and it was my main pursuit 
while I was there. Hopefully, it will be useful to the research done on many different types of foams at 
Wright Laboratories. 


22-6 





AN INTRODUCTION TO BASIC ORGANIC 
LABORATORY TECHNIQUES 


David Ginger 

High School Summer Apprentice 


Final Report for: AFOSR Summer Research Program 
Wright Labs, Materials, Polymer Branch 


Sponsored by: 

Air Force Office of Scientific Research 


23-1 





AN INTRODUCTION TO BASIC ORGANIC 
LABORATORY TECHNIQUES 


David Ginger 

High School Summer Apprentice 


Abstract 

Basic techniques of organic chemistry were studied and applied. Such techniques 
included vacuum and fractional distillation, purification and recrystallization, thin layer 
chromatography, column chromatography, and the collection of infrared spectrum. In 
addition, several reactions were carried out under the direction of M. H. Dotrong. 


23-2 







AN INTRODUCTION TO BASIC ORGANIC 
LABORATORY TECHNIQUES 


David Ginger 

Introduction 

During the course of the eight week high school summer apprenticeship, basic 
laboratory techniques of organic chemistry were studied and applied in a variety of 
situations. Such techniques included vacuum and fractional distillation, purification and 
recrystallization, thin layer chromatography, column chromatography, and the collection of 
infrared spectra. The general procedures used are outlined below, sometimes with 
information regarding specific instances of their application. In addition, several reactions 
were carried out under the direction of M. H. Dotrong. These too are reported below, 
along with relevant information regarding the compounds synthesized. 


Thin Laver Chromatography (TLQ 

Thin layer chromatography (or TLC) is generally used as a quick test to check the 
purity or identity of a product. It relies on the differential migration of compounds along 
an adsorbent test strip as they are carried by an eluting solvent. This differential migration 
is caused by differences in the attractive forces the compounds experience towards the 
adsorbent layer and the solvent. Compounds which are attracted more to the adsorbent 
layer, will migrate more slowly than those which are attracted less. Likewise, those which 
are attracted more towards the eluting solvent will migrate up the TLC test strip faster than 
those which are not attracted strongly to it. Thus, the selection of the adsorbent layer and 
the eluting solvent can be important factors in determining the success of the TLC 
separation. TLC test strips consist of a thin layer of adsorbent (commonly silica gel) 
backed by a layer of plastic, are commercially available, and are often treated with an 
ultraviolet dye to make the separated spots more visible. This means that it is probably 


23-3 






more convenient to vary the eluting solvent to enhance a separation rather than to vary the 
adsorbent layer. 


General Procedure: One half of a TLC test strip is marked with a light, horizontal 
pencil line approximately one centimeter from the bottom of the strip. The compounds to 
be analyzed are dissolved in a solvent and a small amount of each mixture is placed on the 
pencil line with a capillary tube, and each spot is then labeled with pencil. The spots 
should be kept as small as possible in order to enhance resolution. Thus, when creating the 
spots, it is advantageous to dissolve the compounds in a rapidly evaporating solvent, to 
keep them from spreading. The TLC strip is then placed in a chamber containing the 
eluting solvent. The level of the solvent in the chamber should reach just below the line of 
spots on the test strip. The chamber may be packed with filter paper in order to ensure that 
the air in the chamber is saturated with solvent. This prevents rapid evaporation of the 
solvent from the edges of the test strip which could result in uneven solvent migration and 
a poor separation. The strip should be removed from the chamber before the solvent 
reaches its top. If the adsorbent layer has been treated with a UV dye, the finished test 
strip may then be analyzed under a UV light source to make the spots more visible. 


Column Chromatography 

Column chromatography relies on the same principles as thin layer chromatography 
to achieve a separation, however, in column chromatography, the solvent flows down 
through a large adsorbent layer rather than up a thin one. Column chromatography is used 
more for the isolation of a product than for the analysis of one because it is much more 
time consuming than TLC. 

Specific Procedure: Under the direction of M.H. Dotrong, a chromatography column 
with stopcock was first packed with a small wad of glass wool. The silica gel that was to 
be packed into the column was then stirred in the solvent (hexanes and acetone, 1:1) to 
remove air pockets which could disrupt an even flow rate through the column. Two pieces 

23-4 





of filter paper were placed over the glass wool and the silica gel/solvent mixture was 
poured into the column and allowed to settle. Another piece of filter paper, followed by a 
layer of sand, was then placed on top of the settled silica gel to form an even surface and 
the solvent was allowed to run out past the top of the sand. The mixture to be separated 
was then dripped onto the sand so as not to disturb the even surface. After the mixture 
layer had dropped below the top of the sand, more solvent was continuously added to elute 
the compounds. Fractions were collected at 15ml intervals and TLC was used to check the 
contents of each fraction (see figure 1 below). Like fractions were combined. 


Results: Based on the TLC’s results, the separation was not wholly successful. 
Fractions 0-6 included a mixture of starting products, fractions 7-10 included all four 
compounds, fractions 11-14 contained the desired product and what was likely the product 
of a side reaction, and fraction 15 contained contamination from the rubber stopper that was 
used to keep the column from drying out over night. 









Recrvstallization 

Recrystallizations are commonly used to purify a solid product. In a 
recrystallization, the goal is to completely dissolve the product in a boiling solvent, and 
then to reform the crystals by allowing the solvent to cool. Colored impurities may be 
removed by boiling with activated charcoal, and insoluble impurities are removed by hot 
filtration. It is obviously advantageous to use a solvent in which the impurities are 
insoluble even at high temperatures, and in which the product is much more soluble at high 
temperatures than at low temperatures. 

General Procedure: The compound to be purified is refluxed in an amount of solvent 
sufficient to dissolve it completely. If colored impurities are present, a small amount of 
activated charcoal may be added after the compound has completely dissolved, but not 
while the solvent is boiling. Note that some product, as well as the colored impurity, will 
adsorb to the carbon. Hence, charcoaling should only be used when necessary. While the 
solution is still hot, it should be filtered through folded filter paper. The solution should be 
allowed to cool to room temperature or below, then filtered again to recover the 
recrystallized solid, and washed with cold solvent to remove any impurities clinging to the 
surface of the crystals. 


Fractional Distillation Under Vacuum 

Fractional distillation separates two or more substances on the basis of boiling point 
differences. The fractioning column typically contains protrusions along its interior surface, 
which create a series of levels at which the solution is boiling, thus enabling a more precise 
separation. Carrying out the distillation under vacuum allows one to boil compounds that 
would normally oxidize or decompose at temperatures below their boiling points at 
atmospheric pressure, and also allows the distillation of compounds whose boiling points 
are so high as to prevent distillation at atmospheric pressure. 

General Procedure: The solution is heated by a heating mantle, and stirred with a 


23-6 







magnetic stir bar in a round bottomed flask. The fractioning column is attached to the neck 
of the flask and a thermometer is inserted in the proper inlet in the column. A multi-flask 
"cow" adapter is connected to the condenser end of the fractioning column, and appropriate 
sized round bottomed flasks are connected to the adapter. The water in and out lines, as 
well as the vacuum line, are then connected to the appropriate nozzles on the column. 

From the nozzle on the column, the vacuum line is then connected to a 4 T’ valve, from 
which one line is connected to a manometer and another line is connected to a trap 
immersed in liquid nitrogen to prevent solvent vapors from damaging the vacuum pump. A 
line is then connected from the trap to the vacuum pump itself. The ‘T’ valve should be 
turned so as to connect only the manometer and vacuum pump. The vacuum pump should 
then be activated and the pressure allowed to stabilize. Once the pressure has stabilized, 
and it has been determined that no significant leaks are present in pump/trap and 
manometer branches, the ‘T’ valve may be opened to include the distillation glassware 
assembly into the vacuum circuit. By this stage, a failure of the pressure to reduce to 
adequate levels can be traced directly to the glassware setup. Usually, rotating the glass- 
glass joints to spread the vacuum grease evenly will solve this problem. Once the pressure 
has equilibrated, the power may be applied to the heating mantle by means of a variable 
transformer. The "cow" adapter is then rotated so as to collected each fraction in a seperate 
flask. 

Results: 3-dodecylthiophene was distilled in the above manner, yielding three 
fractions. One at 40-41 °C (1mm), the second at 50-52°C (1mm), and the third at 130- 
135°C (,1mm). From its known boiling point, the third fraction was determined to be the 
product. 


Infrared spectroscopy provides information regarding the functional groups of an 
organic molecule. Additionally, by comparison with known spectra, it can be used to 
confirm the identity of a compound. The characteristic spectra of an organic compound is 
caused by its absorbance of infrared radiation at energies corresponding to those at which 

23-7 






the bonds in its component functional groups rotate, bend and stretch. The spectrum of 
each compound is so unique that two samples with identical spectra must, in fact, be the 
same compound. Two common means for preparing samples for scanning are the KBr 
pellet method, and the liquid film Method. The KBr method is used with solid samples, 
and the liquid film method is used with either liquid samples, or samples in a solution 

General Procedure, KBr Pellet Method: A small amount of the sample and a larger 
amount of spectral grade KBr powder are placed in a shaker vial. The vial is then 
mechanically shaken with a glass bead inside it until a homogeneous mixture is obtained. 

A small portion of this mixture is transferred to a small hollow cylinder. Steel heads fitted 
to the interior diameter of the cylinder are positioned on either side of the sample and the 
sample is distributed evenly between them so as to form a thin disk. The cylinder assembly 
is then placed into a press at 20,000+ psi for around 30 seconds. The high pressure causes 
the mixture to fuse into a transparent glassy disk. After the background scan is taken, the 
sample is placed into the scanning chamber and, after the chamber has been purged with 
nitrogen to eliminate carbon dioxide which might cause a signal, the spectrum is collected. 
Afterwards the cylinder and heads should be cleaned. 

General Procedure, Liquid Film Method: An amount of the liquid sample just 
sufficient to completely cover the surface is placed on an KBr plate. A second KBr plate is 
placed on top of the first, and the two are mounted into a carriage assembly. After a 
background scan is taken, the assembly is placed into the scanning chamber and, after the 
chamber has been purged with nitrogen, the spectrum is collected. The plates should be 
cleaned with methylene chloride or another anhydrous solvent, as KBr is hygroscopic and 
any absorbed water will interfere with future spectra. Likewise the plates should be stored 
in a moisture free environment. 

Results: Example spectra are included as relevant information following the 
reactions discussed below. 


23-8 






l,5-Diannilino-2.4-dinitrobenzene 

Under the direction of M.H. Dotrong, 50g of l,3-dichloro-4,6-dinitrobenzene and 
106ml of fresh distilled aniline were mixed with a magnetic stir bar in a 250ml flask and 
heated to 140°C in an oil bath for 4 hours. The resulting dark orange mass was mixed with 
150ml of ethanol to form a slurry and then dissolved in approximately 700ml of hot ethyl 


acetate and mixed with an equal volume of hot methanol to precipitate the product. The 
infrared spectrum may be found below. 















1.3- Diammino-4.6-diannilobenzene 


Under the direction of M. H. Dotrong, 50g of l,5-diannilo-2,4-dinitrobenzene, 2.5g 
of palladium on carbon, and 650ml of degassed THF were shaken in a Parr bottle for 20 
hours under hydrogen at 55psi. The product was then precipitated with the addition of 
1000ml of hexanes. The solid was collected, dried under nitrogen and recrystallized from 
benzene. The melting point was not determined because the compound decomposes before 
melting. 



Under the direction of M.H. Dotrong, 30.20g of 3-dodecylthiophene were stirred in 
200ml of DMF at room temperature. A solution of 42.72g of NBS in 250ml of DMF were 
dropped slowly into the mixture in the absence of light. The mixture was then stirred for 3 
hours at 40°C. After checking the clear, yellow product with the GC to determine if the 
starting material had completely reacted, the mixture was poured onto ice. It was then 
extracted with approximately 1000ml of diethyl ether, washed with water, dried over 
sodium sulfate and distilled twice at 170-173°C at ,1mm pressure. The infrared spectrum 
may be found below. 



23-10 








IR Spectrum of 2.5-dibromo-3-dodecvlthiophene 



4000 


3500 


3000 


1500 


1000 






ELECTRICAL ANALYSIS OF YBa 2 Cu3C>7. x SUPERCONDUCTING 
THIN FILMS AND BULK SAMPLES 


Peter G. Kozlowski 


Centerville High School 
500 East Franklin Street 
Centerville, Ohio 


Final Report for: 

AFOSR Summer Research Program 
Materials Directorate 
Wright Laboratory 

Wright Patterson Air Force Base, Ohio 


Sponsored by: 

Air Force Office of Scientific Research 
Boiling Air Force Base, Washington, D.C. 


August 1993 


24-1 




ELECTRICAL ANALYSIS OF YBa 2 Cu 3 07 x SUPERCONDUCTING 
THIN FILMS AND BULK SAMPLES 


Peter G. Kozlowski 
Centerville High School 


Abstract 


High temperature superconducting thin films and bulk samples of the Y-Ba-Cu-0 system 
were studied in order to characterize their electrical properties. The preparation of high critical 
temperature YBa 2 Cu 3 0 7 . x films on single crystalline SrTi 03 and AI 2 O 3 was done by laser 
ablation. In all cases, c-axis oriented films with critical temperature of about 90 K were obtained. 
On patterned films we obtained a critical current density of 10 6 A/cm 2 . Bulk samples, having a 
much larger cross-sectional area, exhibit lower critical current densities, approximately 103 A/cm 2 . 
Both thin films and bulk samples were measured by a four point technique and were tested through 
a range of temperature from 77 K (liquid nitrogen) to 300 K (room temperature). 


24-2 




ELECTRICAL ANALYSIS OF YBa 2 Cu 3 0 7 x SUPERCONDUCTING 
THIN FILMS AND BULK SAMPLES 


Peter G. Kozlowski 


INTRODUCTION 

The electrical resistance or high temperature superconducting materials falls to zero when 
they are cooled to temperatures below critical which are still above the temperature of liquid 
nitrogen (77 K). This makes the superconductive transport properties of high critical temperature 
superconductors an important aspect for future applications. The electrical properties of these 
materials in both bulk and thin film form were measured by using a four-point technique capable of 
measuring very low values of resistance. The properties which were measured include the normal 
state resistivity as a function of temperature, the temperature at which the transition to 
superconductive properties occurs (critical temperature), and the maximum current (critical current) 
which can be carried by the material in the superconducting state before the material begins to show 
resistance. 

METHODOLOGY 

Superconducting thin films were grown on SrTiOa substrates by using an ArF excimer 
laser operating at a wavelength of 193 nanometers, pulse duration of 15 nanoseconds, and a 
repetition rate of 20 Hertz. The focussed laser beam was rastered across a rotating, stoichiometric 
YBa2Cu 3 0 7 . x target with an energy density at the target of approximately 1.5 J/cm*. The surface 
of the target was cleaned, prior to film growth, by ablating the target in situ for 5 minutes with 
excimer radiation. The target to substrate distance was 6 centimeters [1]. 

The single crystal SrTiC >3 substrates were cleaned by rinsing in triochloroethylene, acetone, 
and methanol, after which they were subjected to a 30 minute ultraviolet-ozone treatment. After 
being loaded into the growth chamber, the substrates were heated to approximately 850°C in O 2 
(100 mTorr) for 30 minutes. Film growth was carried out at approximately 750°C in O 2 (100 
mTorr) and required 30 minutes [2]. The deposition rate was typically 0.4 nm/s, and the resultant 
film thickness ranged from 0.6 to 0.9 pm, as measured with a stylus profilometer. Immediately 
after deposition, the growth chamber was backfilled with O 2 to a final pressure of 1 atmosphere. 
The resulting films were superconducting as grown and were wet-chemically patterned into a 4- 
probe structure which allows measurements of resistivity (p) and critical current densities (J c ). 
Then, four gold wires were attached to the sample with silver paint and it was annealed for 1 hour 


24-3 




at 400°C. The transport properties were then measured by this 4-point technique with the current 
leads and voltage leads being attached to the thin film as shown in Figure 1. 


VOLTAGE LEAD 


CURRENT 

LEAD 



VOLTAGE LEAD 


Figure 1. Scbemetic layout of a thin film prepared for 
transport measurements. 


Bulk samples made of yttrium barium copper oxide were first prepared in a tube furnace 
under flowing oxygen. This procedure varies according to how long the sample is treated and at 
what temperature. The particular samples that we measured were made of 1-2-3 (YBa 2 Cu 3 07 . x ) + 
5 weight percent of Y 2 BaCu 07 _ x (2-1-1), and silver. The pelletized sample underwent a melt- 
process procedure consisting of many steps at different parameters. First, the sample was heated to 
1055°C and remained at this temperature for approximately 30 minutes. Second, the temperature 
was lowered to 1015°C and remained there for 20 minutes. Then at the rate of 2°C/hr, temperature 
was rapidly lowered to 960°C. Finally, at a rate of 240°C/hr the temperature was ramped to room 
temperature. Annealing of sample took place for two days at 450°C under flowing oxygen. The 
pellet was then cut into a rectangular small bar using a diamond saw. On average, a bulk sample 
has the dimensions of approximately 5 mm (length) by 2 mm (width) by 1 mm (thickness). The 
four leads attached to the bulk sample may have different configurations as shown in Figure 2. 


VOLTAGE LEAD VOLTAGE LEAD 

VOLTAGE LEAD CURRENT LEAD 

1 1 


■ ■ 

CURRENT LEAD CURRENT LEAD 

1 1 ! 

CURRENT LEAD VOLTAGE LEAD 


Figure 2. Scbemetic layout of bulk sample prepared for transport measurements. 


24-4 









The transport measurements were carried out by using the four probe technique on both 
the bulk samples and thin films. In both cases, a voltmeter showed the readings between the two 
voltage leads and a current source of 10 mA was applied across the superconductor with an 
ammeter which took readings through the current leads. A thin film connected in this way is shown 
in Figure 3. 



Figure 3. Four-probe technique for transport measurements. 


This four wire measurement technique is capable of measuring very low values of 
resistance [3]. All experiments are completely automated with computer control to set the sample 
temperature, set the sample current, and collect all current and voltage readings. The properties 
which are measured include the normal state resistivity, the critical temperature, and the critical 
current density. 

RESUL TS 

The normal state resistivity were measured as a function of temperature from room 
temperature down to the temperature at which the transition to superconductive behavior occurs 


24-5 









(the critical temperature). High-quality YBa 2 Cu 3 0 7 . x thin films grown in-house by pulsed laser 
ablation with transition widths of about 1 K have been measured. The resistivity curve (Figure 4) 
of one of the measured samples shows a critical temperature of 90 K. 


RESISTIVITY 



TEMPERATURE (K) 


Figure 4. Resistivity versus temperature dependence. 


To calculate the resistivity of a thin Film or bulk sample, the formula: 

„ . R • A 

Resistivity = - 1 — 

1 

was used where R is the resistance of the sample in ml), A is the cross-sectional area, and / is the 
distance between the voltage leads [4], Again, the resistivity curve is used for indicating the critical 
temperature of the sample. 

The voltage as a function of current was measured at different temperatures to determine the 
maximum current (the critical current) which can be carried by the material in the superconductivity 
state before the material begins to show resistance. High quality thin films grown in-house have 
critical current densities of 5 • 10 6 A/cm2 at 77 K (Figure 5) shows a typical voltage versus current 
graph of a sample. Keeping the temperature constant at 20 K, 40 K, 60 K, 70 K, 77 K, 85 K, and 


24-6 






89.5 K, the current value is increased and voltage is measured. In superconducting state, the 
material has no voltage and thus explains the primary superconducting principle that the material’s 
resistance also equals zero. According to Ohm’s Law, V=IR and thus R=V/I. When different 
current values are run through the sample at and below its critical temperature, the voltage is zero. 
And so, mathematically, when voltage equals zero in the equation R=V/I (I>0), any value of 
current will give a resistance of zero [5]. At temperatures above the critical temperature, the voltage 
becomes greater than zero and thus the material has resistance. 


CRITICAL CURRENT DENSITY (J c ) 



O 5.0 10® 1.0 10 7 1.5 10 7 2.0 10 7 2.5 10 ? 3.0 10 7 

CURRENT DENSITY (A/cm 2 ) 

Figure 5. Voltage versus current density. 

The critical current density of a sample is calculated as follows: 

Ic 

Critical Current Density = — 

A 

where Ic is the critical current and A is the cross-sectional area of the sample. 

Two other methods were used to determine the quality of the films. One is x-ray diffraction 
which is used to determine the crystallographic orientation of the films. No phases other than the 
YBa 2 Cu 3 07 _ x were observed in any of the films. The x-ray data for the films with high critical 
current densities show strong peaks of reflections indicating a preferential orientation of the c-axis 







perpendicular to the surface of the film. Another method used was scanning electron microscopy 
which was used to examine the film surface microstructure. Films with high critical current 
densities have relatively smooth and continuous surfaces with no visible granular features. They 
also contain surface features which appear to be inclusions about l^m in diameter. Further 
investigations of these surface features and methods of avoiding their formation are in progress. 

CONCLUSION 

The expanding field of superconductivity has accomplished numerous successes since the 
first high temperature ceramic superconductor was discovered and developed by Miller. Currently, 
more and more pressure is placed upon the study of the properties of superconductors in order to 
learn how they differ and compare to ones made earlier and to be able to apply them to 
technological uses which will certainly benefit society in the years to come. The WL/MLPO 
laboratory has expanded into one of the leaders in the field of superconductivity and has by far had 
some of the best results in its research. 

In conclusion, the thin film and bulk sample superconductors produced here at Wright- 
Patterson Air Force Base have excellent transport properties with an enormous potential for future 
applications. 

REFERENCES 

[1] D.W. Chung, I. Maartense, T.L. Peterson, P.M. Hemenger, J. Appl. Phys. 68, 3772 
(1990). 

[2] I. Bransky, J. Bransky, I. Maartense, T.L. Peterson, J. Appl. Phys. 66, 5510 (1989). 

[3] H. Adrian, C. Tome-Rosa, G. Jacob, A Walkenhorst, M. Maul, M. Paulson, M. Schmitt, 

P. Przyslupski, G. Adrian, M. Huth, T. Becherer, Supercond. Sci. Technol. 4,166 
(1991). 

[4] G. Adrian, W. Wilkens, H. Adrian, M. Maul, Supercon. Sci. Technol. 4, 169 (1991). 

[5] G. Deutscher, K.A. Muller, Phys. Rev. Lett. 59, 1745 (1987). 








A STUDY OF THE MINIMUM DEVIATION METHOD 
FOR REFRACTIVE INDEX MEASUREMENTS 


Sandra R. McPherson 
High School Apprentice 
Bishop Brossart High School 


Final Report for: 

Summer Research Program 
Wright Laboratory, Materials Directorate (WL/MLPO) 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington,D.C. 

August 1993 

25-1 




A STUDY OF THE MINIMUM DEVIATION METHOD 
FOR REFRACTIVE INDEX MEASUREMENTS 


Sandra R. McPherson 
High School Apprentice 
Bishop Brossart High School 

Abstract 

The index of refraction, which is a measure of how light travels through a 

substance, is a basic characterization done by scientists after a new material is 

created. We discuss three types of crystalline material and how to measure each 

one's refractive index. A spectrometer is used to record the data and we discuss how 
the data is taken and analyzed. Indices at several wavelengths need to be done 
because of dispersion. Five runs are done on each prism and a standard deviation 

(STD) is calulated on the computer. Quartz was used as a practice sample until our STD 
was below 10*3. when that was accomplished new materials from Crystal Associates, 
RTA and KTP:Na were measured. The data taken proved to be accurate enough for us 
and the index measurements were sent to Crystal Associates to be used. 


25-2 



A STUDY OF THE MINIMUM DEVIATION METHOD 
FOR REFRACTIVE INDEX MEASUREMENTS 

Sandra R. McPherson 

Introduction 

Index of refraction is one of the basic characterizations for all types of 
materials. In fact, as new materials are developed, one of the first characterizations 
performed is the index of refraction. The index defines how fast light travels 
through the material and the 'bend* (refraction) the light incurs upon passing from 
a material of one index to a second material with a different index. The Materials 
Directorate here at Wright-Patterson Air Force Base is in the business of creating 
new materials to meet the Air Force's needs. In the Electronic and Optical Materials 
Branch, Nonlinear Optics group, new crystalline materials are developed on a daily 
basis and knowing the index is very important. In the lab is an experiment that 

allows us to measure the index on a prism-shaped sample of the material to be tested. 

Disciimaa 

To discuss the orientation of the material, we first point out that there are 

three different types of crystalline material we can study: isotropic, bi-axial and 
uniaxial 1 . Isotropic materials are the simplest type of crystalline materials. In every 

direction inside the crystal, the index is the same. One prism is cut in any orientation 

and the index is measured with unpolarized light. ( If we think of light as a bunch of 
arrows, all in different directions, coming out of the light source as follows, this is 
unpolarized light, as seen in FIGURE 1.) 


25-3 






Light source representing unpolarized light 

Uniaxial materials have two indices of refraction. To measure both the indices, we 
have a prism of the material made like this with axes as seen in FIGURE 2. 



3D drawing showing prism axes 

In this way, if we can use light polarized in the vertical direction, we can get 
one index and if we use light polarized horizontally, we can get the second index. 
(From the definition of unpolarized light, if we now take that same light source and 
place a polarized sheet in front of it, the arrows come out all pointing in one 
direction as FIGURE 3 depicts.) 


25-4 








FIGURE 3 

Light source and polarizer showing polarized light (in this case vertical) 


The third type of crystalline material is called bi-axial. This material has 
three indices of refraction. We use basically the same procedure for recording index 
on this material, however since there are three indices, we require two prisms of the 
material cut as shown in FIGURE 4. 



3D drawing showing prism axes for bi-axial materials 
Once again, using polarized light gives us all three indices. As is evident, one 

of the axis will be repeated, however it will give us a check on the other measured 
values so it is worthwhile doing. 


25-5 





Having determined the type of material and how the samples should be cut, we 
can then determine the index of refraction. The following derivation proves how we 
calculate the index from experimental data 2 : 

Theta (6) is the angle the input beam makes with the perpendicular to the 
prism face and phi (<p) is the prism angle. The incident light is refracted inside the 
prism (shown here as a horizontal line) and then again upon exiting the prism. If 
we draw a line that cuts the prism angle and is perpendicular to the refracted beam 
inside the prism, the angle that line makes with the prism face is called alpha (a). 

Using the fact that the sum of the three angles of a triangle equals 180°, we can solve 
for a as follows in FIGURE 5. 


<P 



FIGURE 5 
Derivation for a 


25-6 



By symmetry 



FIGURE 6 

Derivation for a continued 

Again, using the fact that the sum of the three angles of a triangle is 180°, we can 
write 

(90° - a) + (90° - a) + q> = 180° 

Solving for a a = tp/2 

Now, if we look at a close-up of the prism face at the input ray (FIGURE 7), we 
want to solve for the angle between the rays before and after refraction. We use the 
fact that opposite angles made by two intersecting lines are equal, and the fact that if 
we have a right angle (90°) and we know the value of part of it as x°, the other 
section is 90° - x° (same procedure is used for 180°). 


25-7 



FIGURE 7 
Derivation for x 


(90° - a) - (90° - 0) = x 


Solving for x x = 0 - a 

Next, we sum the angles of the following triangle to solve for 6 using FIGURE 



25-8 





(0 - a) + (6 - a) + (180° - \|/) = 180° 


Solving for 0 0 = — 

Finally, we have the information we need to solve for the index using a 
Physics equation called Snell's Law 2 . Snell's Law relates the angles inside and outside 
any boundary with different indices on each side: 

ni Sin (0 1 ) = n 2 Sin (02) 

Where the subscripts mean on each side of the boundary. In this case, T means air 
and '2' means prism. Therefore, knowing the index of air to be 1.0, 0i in this case is 
0, n 2 is n and 02 is a and we can write 

s m[£(y + <p)] = nsin(i-tp) 



sin[l(y + <p) 
^ /1 \ 

or 

sin (t <P) 


This is the formula that will be used by the Mathematica program to calculate 
the index from the recorded data. 

We measure the index of refraction by minimum deviation method with a 
Gaertner LI24 spectrometer. An Oriel Hg-Xe high intensity white light is used as a 
source of light and it is directed through a Digikrom 240 monochromator to separate 
the colors using a diffraction grating. A polarizer is placed at the output of the 
monochromator. All wavelengths in the visible spectrum can be used, and near 


25-9 



infrared (NIR) measurements are possible with an Electrophysics handheld 
electroviewer which converts NIR radiation to a visible image. A specific 
wavelength enters the collimator which spreads out the input light so it is larger 
than the sample. The sample, cut into the shape of a prism so that light can pass 
through it and be refracted, is sitting on a table that swivels at the base to allow it to 

move and has an adjustable height. The light is incident of one face of the prism and 

is refracted out the other face. 

On the other side of the prism table is a telescope used to focus the light 
coming out of the prism. At the end of the telescope is an eyepiece that we look 

through to see the light. The prism table is then slowly twisted toward the inside 

until the light reverses its direction. This is the called minimum deviation and is 
shown in FIGURE 9. 



9 (degrees) 

FIGURE 9 

Minimum deviation angle versus input angle 
for a prism of index 1.5 and angle of 60° 


25-10 





FIGURE 9 depicts deviation (in our case (p) versus incident angle (a). The 
angle will go no smaller once it has reached the bottom of the plot. This is the point 
where the light has reached it furthest inside position, as it was described earlier. 



FIGURE 10 

Gaertner LI24 spectrometer 

FIGURE 10 is the spectrometer used in this study to measure index using 
minimum deviation. To the right is the telescope with the eyepiece. The flat 
platform in the middle is the moveable prism table. To the right of the prism table is 
the collimator. The black box on the far left is the monochromator. Two of the 


25-11 



















microscopes used to measure the angle as described below are seen, one below the 

collimator, and the other to the left of the telescope. 

To record data, the prism table is left at the point where the light is at its 

furthest position to the inside and the telescope is locked into place. It is important 
that the table be left at the point of minimum deviation, or the measurements will be 
incorrect. Also, the telescope is attached to a divided circle that angle measurements 
are taken from, so it is imperative that the telescope does not move. A fine adjust 

helps to center the beam on the crosshairs located inside the eyepiece. (These are 

merely a constant spot to place the beam for measuring.) Four microscopes are 
situated 90° apart around the bottom of the prism table so the graduated circle can be 
read down to half second arc. These numbers, which are in degrees, minutes and 
seconds, are used to determine the angle of the beam as it comes out. 

For each wavelength, both the right and left side of the prism must be 
measured. This is explained in FIGURE 11. 



Schematic of spectrometer 


25-12 







When the light enters the left prism face, it is refracted. However, we want to 
measure the light's straight path through the prism, which you can’t see (shown by 
dashed line in FIGURES 11 and 12). We accomplish this by measuring the light as it is 
refracted on the left and then the right sides. The left is subtracted from the right. 
Since it is known that the straight line is 180°, then we can subtract the difference 
between the left and right from 180 to get 4*. 



FIGURE 12 

Diagram showing how \|/ is found 

The wavelength is changed after both sides are measured and minimum 
deviation is found again. Each wavelength bends more at different wavelength, 
meaning the index varies with wavelength. This is known as dispersion 4 and is the 
reason for the "rainbow" formed when white light goes through a prism. 

This process is repeated over a wavelength range, called a run, that varied 
between prisms. The standard number of runs was 5 for each axis. 


25-13 




All numbers recorded were entered into a program written by my mentor for 

the Software package Mathematica version 2.2. designed to calculate the index. All 5 
runs on each sample are compared and a standard of deviation (STD) is calculated. 
(STD is how our index numbers varied between runs.) How small the STD is 

determines the accuracy of the final index number. No number is accepted unless its 

STD is smaller than 10'^. 

Rcsuiia 

Before actually beginning on new materials, samples of known indices were 
studied so as to ensure we could repeat the measurements while the STD remained 

smaller than 10‘ 3 . We began with Quartz and only did the visible spectrum of 430-670 
nm (where 1 nm= 10‘^m) in increments of 20 nm, 700-1000 nm in steps of 50 nm and 
1064 nm in steps of 20 nm. This material was known to be an isotropic material so 
unpolarized light was used and only one index was measured. The results are shown 
in FIGURE 13. 


25-14 





Fused Quartz Refractive Index 

1.468 

1.466 

5 1.464 

S 

9 

I 

* 1.462 

X 

I 

1.46 

1.458 

1.456 

0.4 0.45 0.5 0.55 0.6 0.65 0.7 

Wavetength (tun) 

FIGURE 13 

Refractive in>i?x data for quartz 

Then, we moved on to a new material, RTA. This sample was sent to Wright Labs from 
Crystal Associates, in Waldwick, New Jersey, to be measured. Crystal Associates 
determined this material to be bi-axial so two prisms of different orientation were 
sent to be measured. This meant that polarized light needed to be used on both of the 
prisms. Each prism was measured first doing 5 runs on vertically polarized light, and 
then 5 on the horizontally polarized light. However, on one of the prisms only 3 runs 
on the horizontal polarization were done because it was determined that this was one 
of the axes that were equal. We recorded measurements for both the visible and NIR 



25-15 








range of 430-670 nm in increments of 20 nm, 700-1000 nm in steps of 50 nm and 1064 
om. The results, as sent to Crystal Associates, are seen in FIGURE 14. 


RTA Refractive Indices 



Wavelength (nm) 

FIGURE 14 

Refractive index data for RTA 


A second new material, KTP:Na was begun, also from Crystal Associates. Like 
the RTA, this material is also bi-axial. The same series of wavelengths used for RTA 
were recorded. As time had run out on this study, not all five runs for each 
polarization were completed. For the first prism 4 runs were completed for the 
horizontally polarized light and 3 were done for vertical. Only 4 runs were completed 
on the second prism for vertically polarized light and none on horizontal because 


25-16 








there was aot enough time. Measurements will be resumed later on by someone else 
at Wright Laboratory. The measurements taken are shown FIGURE 15. 

KTP:Na Refractive Indices 

I .95 - 1 - 1 - 1 - 1 - 1 - 1 - 

- • 

• n 

1.9 - * . * 

I 1 - 85 ' ’*’*• • • . . : 

I * * 

s ; 

S i g _ • n 

I • . • y 

... 

• • • 

1.7 C- 1 - 1 -J_l_l_l_] 

0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 

Wavaiangth (jun) 

FIGURE 15 

Refractive index data for KTP:Na 

Conciusiflu 

The index of refraction measurements recorded by us in this study proved to 
be accurate enough for use by the scientists who developed the new materials. The 
final numbers and the plots for RTA and KTP:Na shown in this study were sent to 
Crystal Associates. There they will be used to help in the characterization of those 
materials. 


KTP:Na Refractive Indices 


-1- 

- • 

-1-1— 

-1— 

-1-1— 

- 

• 





• n 
• * 

• 

• 

• 

• , 



- 

■ 




• 

- 


• a 

* • • . 

■ 

• n 

a y 




- 



• 

• 

* • • . 

• 

i 

■ 1 1 

1 

_ 1 _ 1 

■ 

.4 0.5 

0.6 0.7 

0.8 

0.9 1 

1. 


Wavatongth (urn) 

FIGURE 15 

Refractive index data for KTP:Na 


25-17 








Referents 

1. Saleh, Bahaa E. A., and Malvin Carl Teich, Fundamentals of Photonics, John Wiley 
and Sons, New York. 1991. p.211. 

2. Halliday, David and Robert Resnick, Fundamentals of Physics, Second Edition, John 
Wiley and Sons, New York. 1986. p.748. 

3. Klein, Miles V. and Thomas E. Furtak, Optics, Second Edition, John Wiley and Sons, 
New York. 1986. p. 97. 

4. Sears, Francis W„ Mark W. Zemansky and Hugh D Young, University Physics, Sixth 
Edition, Addison-Wesley Publishing Company, Reading. 1982. p. 72S. 


25-18 



ORGANIZATION AND EVALUATION OF FRAGMENT SHATTER TEST 
DATA RESULTS USING THE MICROSOFT EXCEL SPREADSHEET 


Jennifer R. Bautista 
High School Apprentice 
Warheads Branch 


Wright Laboratory Armament Directorate 
WL/MNMW 

Eglin AFB, FL 32542-5434 


Final Report For: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored By: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington D.C. 


August 1993 


26-1 







ORGANIZATION AND EVALUATION OF FRAGMENT SHATTER TEST 
DATA RESULTS USING THE MICROSOFT EXCEL SPREADSHEET 


Jennifer R. Bautista 
High School Apprentice 

Computational Mechanics Section, Warheads Branch 
Wright Laboratory Armament Directorate 
Eglin Air Force Base, Florida 

Abstract 


During my second summer in the HSAP program, I organized and evaluated fragment 
shatter test data for over 800 experiments done by NSWC. In order to organize the data, make 
xy plots, and evaluate discrepancies in the information, I utilized the Microsoft EXCEL 
spreadsheet The information was sorted according to specific criteria and put into small groups 
of similar information, so that engineers in my section will be able to pinpoint particular data 
points. Once the organization was complete, xy plots of impact velocity versus residual mass and 
residual velocity were charted, and test shots which appeared to be questionable were identified. 


26-2 







ORGANIZATION AND EVALUATION OF FRAGMENT SHATTER TEST DATA 
RESULTS USING THE MICROSOFT EXCEL SPREADSHEET 

Jennifer R. Bautista 

INTRODUCTION 

My project for the summer was based on the fragment shatter test, performed by the 
Naval Surface Warfare Center (NSWC), Applied Research Associates (ARA), and the Denver 
Research Institute (DRI). The fragment shatter test involves cubical, metal fragments which are 
fired at a metal plate at different velocities, angles, and orientations, shown in Figure 1. One 
purpose of the fragment shatter test is to develop penetration equations for use in vulnerability 
assessment With information about these penetrations, predictions can be made about lethality 
and survivability of certain targets. Currently my mentor, Mr. Michael E. Nixon, is exploring the 
use of hydrocodes to model these tests. The Computational Mechanics Section works closely 
with hydrocodes, which are complex computer codes that simulate warhead formation, 
penetration, and response. If accurate, they can eliminate some range tests and facilitate design of 
new models. 

METHODOLOGY 

The original fragment shatter test database consisted of approximately 800 test shots, with 
information pertaining to projectile material, shape, mass, and Brinell hardness; plate material and 
thickness; angle of obliquity; impact angle; impact velocity; residual velocity and residual mass; 
number of behind-armor fragments from the projectile and mass of those fragments; number of 
behind-armor fragments from the plate and the mass of those fragments; and whether or not the 
main projectile fractured in the test (see Figure 2). I eliminated some of these categories, 
rearranged the other columns, and revised some column titles for clarity. Subsequently I grouped 
the data into small groups of test shots which were specifically related to each other according to 


26-3 





particular criteria: projectile mass and material, plate thickness and material, and impact velocity. 
The result was a series of small, specialized groups of data, shown in Figure 3. 

The next step was charting a series of xy plots of residual mass and velocity. The 
Microsoft EXCEL spreadsheet program provides users with various capabilities for creating clear 
and concise charts. We chose to plot our data series using an xy line plot For each group of 
data, two charts were created: residual mass and velocity. The percentage of remaining mass or 
velocity was plotted versus the striking velocity of the projectile; thus, for each specific group of 
data, there is a visual representation of how much of the projectile remains and how fast it is 
going. Figure 4 shows an example of each. 

For the larger groups of information, we noticed a few discrepancies in some of the xy 
plots: for example, in a group of data where the projectiles travel from 4000 to 8000 fps and 
pierce the plate, most retain around 60 or 70% of their original mass. We found some data points 
which indicated that a test shot in this series retained 98% of its original mass, which of course is 
questionable when viewed in the light of die other tests. An example of these questionable points 
is shown in Figure 5. There is every possibility that this data could be bonafide information, but 
for accurate development of penetration equations, the information must be correct We will 
consult NSWC, ARA, and DRI to review these questionable points in order to clarify our 
information. 

The final step was placing the information in a format that will be easy to use in the future. 
I formatted the data and charts so that the engineers in my section will have an easy-to-use 
reference for prospective use. 

RESULTS 

The organization and evaluation were completed, and the final product will be used in the 
future for development of penetration equations. The questionable data points have also been 
singled out and their authenticity will be checked. Not only have I produced a product that will 
be used in the future by the engineers of the Computational Mechanics Section, but 1 have gained 
valuable knowledge of the fragment shatter test, its applications in vulnerability assessment, and 





as a result of my other "mini-projects" during the summer, a working knowledge of Silicon 
Graphics Iris Indigo workstations, the UNIX operating system, personal computers, the Microsoft 
Windows environment The result of this summer is not merely what I have produced for the 
Wright Laboratory Armament Directorate, but the many things I have learned about the technical 
fields and the working world. 

ACKNOWLEDGMENTS 

First I would like to thank the Lord for providing this opportunity to me. 

Of course, I owe many thanks to my mentor, Mr. Michael Nixon. His guidance has been 
invaluable throughout my two summers here, and I look forward to another wonderful summer. I 
would like to thank my fellow apprentice and "partner in crime" Elliot Moore II for two 
wonderful years of companionship, help, humor, guidance. I must thank the Sverdrup contractors 
who work in our section, Ms. Pam Cortner. Mr. Randy Anderson, and Mr. Bizhan Aref, for 
helping me correct some of my problems. I would like to acknowledge the other members of our 
section. Dr. Bill Cook, Dr. Harbans Sidhu, Mr. Ed Bradley, Mr. John Collins, and our section 
chief, Maj. Howard Gans, for their help and support during the summer. I could not have asked 
for a more receptive or helpful environment in which to work and learn, and I owe everyone my 
thanks. 


26-5 




BEFORE IMPACT AFTER IMPACT 



> 


A 



26-6 


Figure 1: Parameter Identification for the Breakup Study 












NSWC/ARA/DRI FRAGMENT SHATTER TEST RESULTS 


e 0\f~ ; r^OOop v£: !<Nf^oqs>r^vqrrrn\ooooor^'!ts < «v rr lo; 
2 a> 000 — 000^—' — O — — O — fSOOf^f^fS o — 


""-nOOOn-NN^NM-nNOOlSaOiflXX 


00 ' 0 'A t O'PQ°^cnO®'®! 0 < ^*^'^ 0 ^ 0 \ 0 '^^‘. ^ 4; 

ddN^N2212«o , f2o22no23'o2S2£N« 


©©mmvvoroiricN© — voo<s , »^'Or~'<rOf^'^r'VOf» , >ro 


3 | qqpw ino «!’lP9P« e !' , .8>»'lo 0 l !9' { !^ftwa^ 

> 1 X X X X X X g g X X X « X X X X X g X X XX £ g § | 

g, «n \o ^ ^ 

Q /-v 

s c 


g.’^S? ppqppQq«qpp#qpp2qqPqqq^-|»)2 

S 4 _• i{ QoptSvOOQcnvO^tSP'OvC-OtSmcorooocooovjOfSiri'tf' 

S-«5 8 #SM?'OQ»inpON5tt-OOM»MpSNt : IO-'ONi?l 

5* •> -S '^•ovoJn^Koo — vOdSdhoopOooooo^S—■r'rS'-ooc^oorjTi- 


oooooooooooooooooooooooooo 


----..---nNNMMinmmmwMinininifiinioin 

§S§g§§S§SooS8SSS8SS^222222 

oooooooooooooooooooooooooo 




iiiiiiiiiiiiiiiiiiiiiiiiii 


nmnnnmi'Knwrti'KncKommnrtmMndmnnf'i 


|jj||£jJj|J|j§l£Jj 


minifliftininiflio»iinin*iinifiinift>nininiflV'inioininirt 

mcntntnfnrofnpofnmtnpncnmrnfomcnmc»imtncocof*ic^ 

NNMNNNNNNNNNNMNNNNNNINNNNNO 


N(SM«NNNNNNMNNNN(SNNNN« 


HHHHHHHHHHE-'HHHf-'HHHHHHHE-'f-'t-H 

0000009000000000000000000000000000000000000000000000 

^4 «■* —4 4-4 *—4 4*4 ^4 *>4 v“4 ^4 —4 ^4 *-4 -4 —4 4-4 1-4 --4 4— —4 4-4 ^4 —4 —4 w-H —4 

OOOOOOOOOOOOOOOOOOOOOOOOOO 

—4 p4 4-4 --4 ^4 —-4 —4 ^4 -4 ^4 —H —4 *4 —-4 1*4 ^4 —1 ^4 4—4 4^ —4 1—4 4«4 4-4 ^4 4-4 


*r«? f ?T , ?'? r r 0 ?'7 f ?'?TT < ? , ?T‘?'9 r r’7<? c ?T w ? v 9 r 7 


26-7 


Figure 2: Original Data in Unorganized Format 










NSWC/ARA/DRI Fragment Shatter Test Data: 4140 Steel Against 2024 Aluminum 




Figure 3: Organized Fragment Shatter Test Data 



































Figure 4: XY Plots of Residual Mass and Velocity 














26-10 


Figure 5: Example of Questionable Data Points 










William Blanchard's report not available at time of publication. 


27-1 






FROM MINDS TO MISSILES: 

THE DESIGN, DEVELOPMENT, AND TESTING OF 
AERODYNAMIC MODELS 


Theresa J. Cook 
High School Apprentice 
Aerodynamics Branch 


Wright Laboratory Armament Directorate 
WIVMNAA 

Eglin Air Force Base, FL 32542 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington D.C. 


August 1993 


28-1 








FROM MINDS TO MISSILES: 

THE DESIGN, DEVELOPMENT, AND TESTING OF 
AERODYNAMIC MODELS 


Theresa J. Cook 
High School Apprentice 
Aerodynamics Branch 
Wright Laboratory Armament Directorate 

Abstract 

The aerodynamic qualities of weapon airframes and munitions can be examined 
through the use of scale models and analysis of free-flight test data. The different stages 
of aerodynamic modeling were studied in three different programs: the GBU-28, a 
precision guided bomb, HAVE DASH II, an air-to-air missile, and the PGU31-B, an 
armor-piercing munition. The GBU-28 was studied with the Projectile Design Analysis 
System, while the properties of the HAVE DASH II and the PGU31-B were investigated 
in the Aeroballisdc Research Facility and the Ballistic Experimentation Facility. The data 
obtained from the finished tests was determined to be accurate, while the work done on 
ongoing testing has yet to be evaluated. All the results obtained will be used to provide 
control data for the programs. 


28-2 







FROM MINDS TO MISSILES: 

THE DESIGN, DEVELOPMENT, AND TESTING 
OF AERODYNAMIC MODELS 

Theresa J. Cook 

Introduction 

For several years, scientists and engineers have utilized the opportunities provided 
through the analysis of free-flight test data to investigate the aerodynamic properties of 
missiles, munitions, and other projectiles. By measuring the aerodynamics and calculating 
the aerodynamic coefficients of these models, scientists can get an idea of how the full- 
scale missile will fly and can input this data into their fire and flight control programs. 
Free-flight analysis has a decided advantage over tests conducted in wind tunnels because 
of the absence of the effects caused by the sting on which the models are mounted. Free- 
flight analysis can provide more accurate drag measurements and other aerodynamic terms 
than a wind tunnel, but is less efficient in measuring pressure on the body and can not 
accurately test at high angles of attack. However, free-flight analysis gives a more 
comprehensive view of the missile's trajectory because the model’s actual flight path can be 
followed. By the use of two orthogonal camera stations, the engineer can determine the 
model’s spatial position as well as its angular orientation. The spatial position and angular 
orientation of the model provide six pieces of information, which are also known as the 
Six Degrees of Freedom (6DOF). By entering the Six Degrees of Freedom into equations 
of motion, the aerodynamic terms of the projectile can be determined. These equations, 
convolutions of Newton's second law, F=ma, are derived to fit the terms specified by the 
engineer and are taken to a certain degree dependent upon the accuracy desired. 


28-3 






The engineer's tasks during the aeroballistic free-flight testing are usually done in 
three phases. The first of these is to analyze the projectile’s design to ensure its feasibility. 
This part of the process was studied with the GBU-28 using PRODAS. The second step 
in the aerodynamic testing procedure is to actually shoot the models in the range and 
collect the data from these shots. This was practiced with the HAVE DASH II program 
in the Aeroballistic Research Facility (ARF) and the Ballistic Experimentation Facility 
(BEF). The third and final phase of the process is for the engineer to examine the data 
collected from the tests and report on the results. The PGU-31/B was used for this part of 
the procedure. 

GBU-28 

The GBU-28, or "Bunker Buster," was a precision guided bomb developed and used 
by the United States Air Force during the Persian Gulf War. When the war started, the 
Air Force realized it did not have the type of bomb needed to effectively travel through 
one hundred feet of dirt and rock and then through twenty feet of reinforced concrete, or 
to get into the Iraqi command bunkers. In seventeen days, the engineers at Wright 
Laboratories and Eglin Air Force Base designed and produced the GBU-28. It was then 
shipped to Saudi Arabia, where it was used to hit the main Iraqi command bunker on the 
last day of the war. In that somewhat rushed situation, the engineers did not have much 
time to investigate the aerodynamics and flight pattern of the GBU-28’s configuration. 

Now the Air Force is interested in how and why the GBU-28 flies, so tests have been 
planned with scale models to be analyzed in the free-flight range. The data from these 
tests will be used to corroborate earlier wind tunnel results and add to the bomb's 
aerodynamic database. 

Before the models are actually made, however, the design is run through a 


28-4 







preliminary program, the PROjectile Design Analysis System (PRODAS). This is a 
comprehensive program for ballistic analysis written in the FORTRAN language by Arrow 
Tech. It is capable of projectile modeling, calculating physical properties, conducting 
aerodynamic, stability, and trajectory analyses, and estimating interior and exterior 
ballistics. To start, the projectile is modeled on the computer in elements. Each element 
must be defined by its left diameter, right diameter, length, length in reference to the whole 
body, density (the material of which that particular element is made), element code, radius, 
and color if desired. The element code specifies what part of the body the element is, such 
as boattail, fin, ogive, flare, or body. PRODAS then takes these elements and creates a 
composite model that is similar to the original aerodynamically, using a database of 
projectile parts and pieces that have known terms and coefficients. 

The GBU-28 is a fairly complex configuration, with several changes in the 
diameter throughout the length of the body and three distinct fin groups (Fig. 1). A 
problem was encountered when the fins were modeled. The aft wing, composed of two 
fin groups, had a small protrusion at the front that could not be drawn correctly by 
PRODAS. This piece was determined to be inconsequential and omitted from the model, 
but the remaining fin proved still too complex for PRODAS to handle. The composite 
model created by the program was imprecise and the results of the tests run on that model 
were unfavorable. The aft wings of the PRODAS model were then changed to a shape 
that was easier for the program to correctly emulate, but that had the same area as the 
prototype fins and were expected to have similar properties (Fig 2.). The model produced 
by PRODAS after these changes had been made was much more accurate and the results 
of these tests more favorable. The GBU-28 tests from PRODAS affirmed the results 
obtained from earlier tests in that the model is stable and has the aerodynamic terms that 
were expected of it. To proceed with the program, fifteen models have been made and 


28-5 






will be tested in the ARF. The PRODAS analyses showed the configuration to be stable 
and ready for testing in the ARF. 

HAYEJ2ASH-II 

The HAVE DASH II is an air-to-air missile designed by the Aerovehicles Branch 
of the Wright Laboratories Armament Directorate. It is a conformal carriage weapon 
airframe, meaning that it is specially designed to fit snugly up against the body of aircraft 
Conventional weapons pylons present several problems. The configuration of the pylon 
creates large amounts of drag that the a craft must overcome. Pylons also produce a 
distinctive radar signature, making the aircraft more easily detected and placing the pilot in 
greater danger. A conformal carriage weapon airframe reduces both of these effects. The 
HAVE DASH II is made of composite materials engineered to lower drag and has been 
shaped to produce the lowest amount of drag possible. The fins on the HAVE DASH II 
also fold to allow the missile to be carried internally and to be stored with greater ease. 

The missile has one flat side which rotates to the bottom when launched The combination 
of this flat surface and specially designed fins provides enough lift to allow the HAVE 
DASH II to use the bank-to-tum method, the same method as used in conventional 
aircraft 

Over the past several months, the HAVE DASH II configuration has been tested in 
the ARF and the BEF. These are both facilities where free-flight analyses are conducted. 
The ARF is the inside range, consisting of a concrete tunnel approximately six hundred 
feet long lined with the orthogonal camera stations used to photograph the models as they 
fly down the range. The BEF is the outdoor range, where the models are shot prior to 
being shot inside to ensure their reliability. Some of the bsonic shots are also conducted 
outside at the BEF. The HAVE DASH II program has involved several shots in both the 


28-6 









ARF and the BEF. Eight models were shot in the ARF using the High Performance Gun, 
a powder gun with a three inch diameter barrel. The velocities of these shot ranged from 
Mach 0.98 to Mach 1.91. The data from these shots is currently being reduced. The 
physical properties of the models - mass, distance of the center of gravity from the nose, 
length, height, width, moments of inertia about the X, Y, and Z axes, and speed - as well 
as the atmospheric conditions of the range at the time of the shot were entered in a 
database in a format that could easily be used for the Aeroballistic Research Facility Data 
Analysis System (ARFDAS) data reduction program. Engineers use ARFDAS to extract 
the aerodynamic information from the experimentally measured trajectories. In essence, 
the engineer uses ARFDAS to develop a theoretical trajectory that matches the 
experimentally measured trajectory. The terms used to develop the theoretical trajectory 
then provide the aerodynamic results. 

Some preparatory shots for subsonic tests at the BEF were conducted using the 
medium chamber, single barrel, compressed air gun. In order to attain the desired 
velocities, a pressure curve was needed to ascertain the correct air pressure to charge the 
gun up to. This curve was generated using the following formula: 

llPV (1-X (1_Y) ) 
p (Y-l )M P 

where 

K =s + Vb 

v c 

and 

Vp = Muzzle velocity of the projectile 

Pc = Chamber pressure 


28-7 



Y = Ratio of gas specific heats 

Mp = Projectile mass 

= Launch tube cross sectional area 
Lb ~ Launch tube length 

Lf = Efficiency factor. 

All variables except for Pc, chamber pressure, were held constant. Some unit conversions 
were necessary to acquire the proper output. By altering the amount of pressure (psi) 
entered in the equation, the curve was generated and graphically displayed through the use 
of Axum. Two curves were calculated, one at one hundred percent efficiency and one at 
eighty five percent efficiency (Fig. 3). The eighty-five percent efficiency curve was then 
used in the planning of the shot and proved to be accurate. 

PQU-31/B 

The PGU-31/B is an armor-piercing penetrator mounted in a forty millimeter round 
ammunition. The Special Operations Forces, sponsors of this program, were interested in 
the penetrator’s aerodynamic qualities and testing has been conducted in the ARF. A 
problem arose, however, when the project engineers tried to find a way to measure the 
models’ roll rate. Normally, a small pin is moi ed on the back of the model. This spin 
pin is then read off the film and its position in space is calculated by using the two 
orthogonal views taken of the model as it flies downrange. By observing the change in the 
position of the spin pin from one station to the next, the roll rate of the projectile is 
measured. Without a spin pin, it is nearly impossible to determine the roll rates off the 
film. Spin pins could not be installed on the PGU-31/B models because they were already 
mounted in the ammunition and were too small for the pins to be added correctly. 


28-8 







The solution to this problem was found to be quite simple, although somewhat 
archaic. Three plywood boxes were constructed and placed in the rar 'e. One was 
positioned immediately out of the blast chamber, one at approximately sixty meters 
downrange, and one at one hundred and fifty meters from the gun. Each of these boxes 
had three pieces of photographic paper (because of its strength) that were eight inches 
apart stretched across the opening. As the PGU-31/B flew through the paper, the fins cut 
it, leaving a clear view of the model's angular orientation at the time it passed through the 
paper. When the shots were finished, the paper was taken down and the angles of each 
model as it flew through each piece of paper were measured manually with a plumb line 
and a protractor. The eight inch spacing between each sheet of paper was calculated to 
ensure the model would not rotate over ninety degrees in that space, which would have 
made the angles confusing to measure and would have resulted in incorrect roll data. 

After the raw angular data had been extracted from the roll sheets, the actual roll rates 
were calculated for each shot. This data was entered in a spreadsheet and plotted 
separately for each of the sixteen shots (Fig. 4). These plots were sized so that they could 
be visually compared quantitatively with experimental plots from ARFDAS. The data was 
determined to be accurate and the data reduction for the PGU-31/B program is nearly 
completed. 

Conclusions 

The three projects worked on this summer: the GBU-28, the HAVE DASH II, and 
the PGU-31/B demonstrated the various processes involved in the testing and evaluation 
of aerodynamic models. The PRODAS tests done for the GBU-28 program confirmed 
initial estimates and prepared the program for further work. The results gained from this 


28-9 



work also helped to validate the PRODAS program. The HAVE DASH II is an ongoing 
project, and work is continuing with the configuration. Although the actual testing is 
finished, the data that is received must be organized and analyzed. The PGU-31/B tests 
are coming to a close now that the roll rates have been determined and the data is 
evaluated. The High School Apprenticeship Program is an invaluable experience for 
students to learn what the "real world" of engineering is like and should be encouraged 
and promoted extensively. I have had a great time for the past two years and look 
forward to coming back again. 


28-10 






Fig. 1 PRODAS Model of the GBU-28 with Complex Fins 

28-11 






Fig. 2 PRODAS Model of the GBU-28 with Simplified Fins 

28 - 1 '. 









HAVE DASH Pressu 
Medium 



1 00 200 300 400 500 6 








28-14 








CHARACTERIZATION OF SOILS FROM RANGE 22 
TO DETERMINE CONTAMINATION 


Nancy H. Deibler 
High School Apprentice 
Environics Branch 


Wright Laboratory Armament Directorate 
WL/MNOE 

Eglin AFB, Florida 32542-5434 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base,Washington D.C. 


August 1993 


29-1 





CHARACTERIZATION OF SOILS FROM RANGE 22 
TO DETERMINE CONTAMINATION 


Nancy Deibler 
ABSTRACT 

Metal contamination in soil is a hazard to the environment. Since munitions testing using a large range 
of metals have taken place at Range 22 since the early 1940's, there is a possibility of metal contamination in the 
soils on Range 22. If the level of metal contamination in the soils at Range 22 is known then modification of 
testing to prevent any further contamination and/or the precise treatment of the soils can be executed to 
decontaminate the soil. Metal contamination in the soils at Range 22 was investigated. A total of 16 soil samples 
was taken in selective areas at Range 22. The soil samples were characterized in the chemical laboratory. The soil 
samples' pH and metal content were determined using instruments in the laboratory. The element of metal, 
amount of each metal, and the pH of each soil sample was found and compared to controls of similar soil texture 
for their contamination level. The results show that there is possible iron and aluminum contamination in some 
soils at Range 22. Further testing, such as, contour analysis and core soil testing will make it possible for 
conclusions to be drawn on metal contamination of soils at Range 22. 


29-2 





CONTENTS 


INTRODUCTION 4 

BACKGROUND.4 

PROCEDURES.4 

RESULTS.7 

CONCLUSION.10 

MISCELLANEOUS.11 

ACKNOWLEDGMENTS.11 

BIBLIOGRAPHY.13 


29-3 











INTRODUCTION 

The characterization of the soils at Range 22 has not been accomplished. The purpose of my summer 
project was to accomplish this survey. Range 22 is the range located on the main base site. My project's results 
started a record o/ the metal content of the soils from Range 22. With these results and the results of further 
testing, the impact on the environment from the munitions testing will be known. With this informataion, efforts 
can be made to decrease human impact and decontaminating the range. 

BACKGROUND 

A broad range of metals has been used in testing at Range 22. Aluminum, brass, steel, tantalum, lead, 
and copper are some of the most commonly used metals. Many different metals were tested at the range during 
World War II. Almost any metal that has been used in weapons has been used at Range 22. A wide range of tests 
has taken place at Range 22. A wind tunnel, air guns, powder guns, light gas guns, nets to clear mines, BS2 tests, 
fuse testing, and explosive sensitivity tests are some of the tests that have been conducted at Range 22. Since 
Range 22 has been exposed to so many different types of tests involving weapon systems and materials that may 
cause metal contamination, the chance of contamination is high. 

"Life is vital to soil and soil is vital to life."(Louis M. Thompson and Frederick R. Troeh). Soil is 
essential for plant life in a natural environment. From the soil, plant roots get mechanical support, essential 
elements, water, and oxygen. Plants provide oxygen for animals and oxygen is critical to animals, consequently; 
soil is an essential part of natural life on this earth. Since soil is so important, it is necessary that the soil is 
maintained as a permanent, useful resource. 

The military is taking part in trying to take care of this important resource. The characterization of the 
soils at a range was to find any contamination. If contamination is found proper actions will be taken toward 
solving the problem. 

PROCEDURES 

Range 22 was mapped out for soil sampling areas. The location from which each soil sample was taken is 
shown on the map of Range 22 in Figure 1. Each flag represents the location that the soil samples were taken 
from. Samples were taken randomly from target butt 413 and the concrete target butt next to the Ballistics 
Experimentation Fadlity(BEF). The target butts are where metal would first be deposited. Soil samples were also 


29-4 




taken from the wetlands approximately 120 ft north of target butt 413. Since shots are fired into the 
Choctawhatchee Bay, samples were taken along the shore of Choctawhatchee Bay at the range. Three samples 
were taken with a dredge from the creek located in the middle of the range. These samples were taken to analyze 
the migration of metal contaminates due to weather, wind, and other things that might disturb the soil. Two soil 
samples were taken from an undisturbed area that has similar soil texture to the soil from Range 22. One sample 
was taken from an undisturbed creek that had similar soil content as the creek located in the middle of Range 22. 
These controls were used to compare the normal pH level and metal content of this particular type of soil to the 
samples collected to determine contamination. Using a sterilized spatula for each sample, a 12 inch by 12 inch by 
1 inch hole was dug. The soil was sifted with a USA standard testing sieve with an opening of 2 millimeters. All 
particles 2 millimeters or larger were not collected for sampling. The soil was then poured into plastic bags. We 
then labeled the soil sample bags by the location from which each soil sample was collected. The samples were 
then brought to the chemical laboratory for characterization. 



FIGURE 1 

The samples collected at the creek and the wetlands northwest of target butt 413 had a large amount of 
moisture absorbed in the soil. These samples were dried in the sun so they would be ready for processing. All 


29-5 




samples were then evenly homogenized with a ball jar roller mill. Next 5 grams of each soil sample were weighed 
The measured soil was poured into small spectra cups. 

The first analysis was done using the Portable X-ray Fluorescence Spectrometer. The soil samples, as 
well as, 10 metal fragments that were randomly collected at Range 22 were analyzed using the Portable X-ray. 

This instrument uses two radioisotope sources, cadmium 109 and iron 55. These sources excite the electrons 
causing an election displacement. The X-ray reads the displacement energy levels of each element peak. The 
Portable X-ray has the capability to simultaneously measures and analyzes 21 of the common alloying elements in 
a full range of Fe, Ni. Co. Cu, and A1 based alloys in any size or shape material. Depending on the type of surface 
on the sample, the instrument will read the sample using two different settings: universal, for samples with rough 
or ridged surface, and special flat, for samples with a flat surface. Then the instrument prints out the percentage of 
each element present in the sample placed on the probe, with the exception of calcium, silicon, sulfur, potassium, 
carbon, hydrogen, and oxygen that can not be detected by the Portable X-ray. Although the instrument can not 
detect the element Aluminum, it will give you the nearest Aluminum alloy. 

For the second test conducted, 10 grains of each soil sample was mixed with 10 milliliters of distilled 
water. Samples 17 and 18 absorbed the added distilled water. An extra 10 milliliters distilled water was added to 
these two samples. These mixtures were used for pH testing. The Expandable Ionanalyzer 940 in its pH mode was 
used to find the pH level of each soil sample. The pH meter is calibrated with a standard that has a pH level of 7 
and a standard with a pH of 10. The pH meter then automatically calculates the pH level of the samples. 

To process the samples for analysis with the Inductively Coupled Plasma Spectrophotometer (ICP), 50 
grams of each soil sample was weighed. Then 25 milliliters of distilled water and 25 milliliters nitric acid was 
added to the measured soil. An extra 50 milliliters, half distilled water and half nitric acid, were added to soil 
sample 16. The soil sample had absorbed most of the first 50 milliliters added to the soil. The samples and the 
nitric acid solutions digested for 42 days. Then the samples were filtered with number 2 qualitative filter paper. 
The filtered liquid was used for the ICP analysis. 

The ICP works by comparing the wave length of light of a standard or known substance to the wave 
length of light of your sample. Using a standard or known substance you calibrate the ICP so it reads the 


29-6 



standard's element parts per million (ppm). The 1CP uses the calibrated peak for reference to read the ppm of your 
sample. 

RESULTS 

The results from the Portable X-ray analysis of the fragments that were randomly collected from Range 22 
are shown on Figure 2. As you can see there is a high percentage of copper (Cu) in fragments number 1,4, 5, 7, 
and 8. There is almost SO percent copper in fragment number 9. There is a high percentage of iron (Fe) in 
samples 2 and 3. There is a high percentage of lead (Pb) in sample 6. These metal fragments are where potential 
co ntamina tion begins. These results are helpful in telling what metals are in the fragments. The results would be 
more informative if the location from which each fragment was collected had been recorded and more fragments 
had been collected. 


X-ray Analysis Of Fiagments from Range 22 









Fragment 

Cu (%) 

Fe {%) 

Mo (%) 


Ti (%) 

Zn (%) 








1 

94.81 

0.41 



0.2 

4.58 

2 

1.66 

96.36 

1.08 


0.31 


3 


97.02 

1.36 


0.24 


4 

92.14 





7.86 

5 

72.02 

11.22 


1.31 


4.67 

6 

2.05 



97.95 



7 

74.01 

0.34 




25.25 

8 

94.53 

0.41 



0.22 

4.84 

9 

49.87 

0.56 


2.68 


4.16 

10 




99.05 

0.95 



FIGURE 2 


29-7 




















































The results from the Portable X-ray analysis of the soil samples are shown on Figure 3. There is a higher 
percentage of iron in samples 16. 17. and. 18 than in the 3 control samples 19. 20. and 21. Samples 16. 17. and 18 
were collected from the creek in the middle of Range 22. The Portable X-ray analysis results show possible iron 
contamination in the creek. 



X-ray Analysis Of Soils from Range 22 


SAMPLE 

Fe (%) 

Sn (%) 

Ti (%) 

Zr (%) 


E 


0.19 


1.84 


0.57 


3.07 


1.63 


.35 


0.33 


1.08 


1.11 




0.11 


0.25 


0.73 


0.13 


0.28 


0.26 


0.17 


0.18 


0.63 


0.23 


0.28 


0.13 


0.25 


0.28 


0.02 


0.07 


0.03 


0.02 


0.02 












































The results from the Inductively Coupled Plasma Spectrophotometer analysis is shown in Figure 4. 

There is a notably larger amount of aluminum and iron in samples 6.9,13. 16. 17, and 18. Samples 16, 17, and 
18; which were collected from the creek in the middle of Range 22; and 9; which was collected from shore of 
Choctawhatehee Bay; have an extremely high aluminum and iron content compared to the controls. These results 
suggest possible iron contamination as does the Portable X-ray results. Due to the fact that Portable X-ray does not 
detect the element Aluminum, the ICP results for aluminum content can not be compared to the Portable X-ray 
results. The ICP results show possible aluminum contamination in some of the soils from Range 22. 


ICP Analysis of Soils from Range 22 





SAMPLE 

Al (ppm) 

Fe (ppm) 






6 

1580 

140 


7 

80 

- 


8 

600 

- 


9 

4700 

2060 


10 

680 

- 


11 

- 

- 


12 

840 

- 


13 

1280 

40 


14 

- 

- 


15 

- 

- 


16 

11760 

2660 


17 

4480 

8720 


18 

4620 

3540 


19 

- 

- 


20 

380 

- 


21 

840 

- 



FIGURE 4 


29-9 








































■ 


The results from the pH testing are shown on Figure S. The controls have an average pH level of 5. 
Samples 6, 10, and 11 tested to be basic. All the other samples tested to be acetic, like the controls, but are more 
neutral than the normal average pH of these soils. Some possible reasons for the soils being more alkaline than the 
controls could be the introduction of new metals and their alkaline salts into the soil. The presence of salts from 
elements, such as, calcium, magnesium, and sodium carbonates would change the pH level of the soil to an 
alkaline level. These results are significant due to the fact that the more alkaline the soil is the more contaminates 
can leach further into the soil. 



FIGURE 5 
CONCLUSION 

Further testing is needed before complete conclusions can be drawn on the contamination of the soils on 
Range 22. The results from the project show possible aluminum and iron contamination in the soils. To decide 
whether these metals are being introduced into the soils as a result of munitions firing, some other man made 
disturbance, or a natural disturbance still needs to be investigated further. Some other testing at Range 22 may 
include contour analysis and core soil testing. Contour analysis will show the boundaries of the contamination. 
This is done by collecting and analyzing soil samples collected over a large area surrounding the target butts. Core 
soil testing will show how deep into the earth the contamination has leached. This is done using a core soil 
sampler to take samples from layers in the earth. That the samples are analyzed in the laboratory. 


29-10 










MISCELLANEOUS 


In addition to conducting my project I became familiar with the equipment in the chemical laboratory. 

I was involved in the reorganization of the chemical laboratory. The equipment and mechanical parts were 
organized into drawers according to similarity for their use. For proper organization of the supplies. I learned how 
each piece of equipment is used. 

I was involved in an inventory of the chemicals in the chemical storage room and the chemistry 
laboratory. This job revealed the dangers of various chemicals and the proper disposable of hazardous chemicals. 
It was a great way to become familiar with the large amount of chemicals a chemical laboratory needs for 
experiments. 


ACKNOWLEDGMENTS 

Of course, none of this would have been possible without the help of others. I would like to send many 
thanks to my mentor, Mr. Luis Santana. He was always ready to help me in any way. I thank Mr. Ric Crews for 
all of bis help and time. He was a great substitute mentor for the time Mr. Santana was on vacation. Lt. Noland 
conducted the ICP analysis and helped with other miscellaneous information. Along with everyone else in the 
High School Apprenticeship Program. I would like to thank Mr. Don Harrison. I really appreciate all the work and 
effort he put into this program to make it work. Mr. Michael Deiler was also very valuable in the running of this 
program. One person not to be forgotten is Mrs. Glenda Apel for all the typing and organizing she did for the 
program Another person I would like to show gratitude to is Melissa Griffiths. It was great working with 
someone willing and ready to work. I was lucky enough to work with two great apprentices. I'd like to thank Mary 
Pletcher for her help as an fellow apprentice. I thank all of my branch. Everyone in the Environics Branch was 
ready to answer a question or lend a helping hand. I'd like to thank everyone for use of their computer. I would 
also like to thank the photo lab for their speed in which they developed our veiwgraphs and for their full 
cooperation with photographs. I will again thank everyone who helped in anyway. Thanks for all the knowledge 
and good laughs. 


29-11 




BIBLIOGRAPHY 


Thompson, L. M. and F. R. Troeh, "Soils and Soil Fertility," Fourth Edition, McGraw-Hill, Inc., 1978. 

Lyon. T. L„ H. O. Buckman, and N. C. Brady. "The Nature and Properties of Soils," The Macmillian Company, 
New York, 1950. 

Donahue. R. L., J. C. Shickluna. and L. S. Robertson, "Soils: An Introduction to Soils and Plant Growth," 
Prentice-Hall, Inc., Englewood Cliffs, New Jersey, 1971,1965,1958. 


29-12 






AUTOMATED HISTOGRAM STATISTICS FOR SENSOR 

FUSION ANALYSIS 


Christie W. Gooden 
High School Apprentice 
Seeker Technology Evaluation Branch 


Wright Laboratory Armament Directorate 
WL/MNGI 

Eglin AFB, FL 32542-5434 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington D.C. 


August 1993 


30-1 





AUTOMATED HISTOGRAM FUNCTONS FOR SENSOR 

FUSION ANALYSIS 


Christie W. Gooden 
High School Apprentice 
Seeker Technology Evaluation Branch 
WL/MNGI 

ABSTRACT 

Currently, the data fusion work being done by WL/MNGI utilizes LADAR data 
obtained from the Submunition Guidance program. This display and image processing 
software provided with the data proved to be inadequate. The goal of this project was to 
provide histogram statistic capability for use with the data and display software. Initial 
software adaptions produced approximately forty pages of output that then required 1-2 
hours of manual analysis to extract the desired range of information per image. The 
output has reduced to 1-2 pages, and requires no perusal. To achieve this reduction of 
output, Pascal programming was reviewed and then used to modify an existing program. 
This improvement enables the user to receive the necessary data quickly. 


30-2 




AUTOMATED HISTOGRAM STATISTICS FOR SENSOR FUSION 

ANALYSIS 

Christie W. Gooden 

Introduction 

To prepare for the future, the United States is taking its "smart” weapons, 
improving their capabilities and making them into "brilliant" weapons for the next 
generation of tactical warfare. These brilliant weapons will be completely automated so 
that the possibility of injury will be reduced for the aircraft operator and maximized for 
the target. To assist in this directive, research is being done to fuse the data of LADAR 
channels. Consequently, this process will upgrade existing target detection, and enhance 
the image processing ability of LADAR sensors. 

Background 

LADAR, or Laser RADAR, displays multiple channels of data. These channels 
are known as range, reflectivity (intensity), and doppler. Each one works independently 
of each other and as a result can be used without each other. This is due to the fact that 
each channel has its own primary purpose. The data of range is essentially used for the 
detection of ground targets that are of a considerable size. Intensity's data works to weed 
out the non-camouflaged, man-made items. Lastly, the data from the doppler channel 
displays kinetic information. The channels that the LADAR data fusion team are trying 
to bring together are the former two. 

The fusion of range and reflectivity will bring about an enhanced image. Range 
shows the width, height and length of an object, and reflectivity can reveal the material 
of which an object is made. This aspect makes targets more discernible. Especially in 
the case of reflectivity. The intensity of the wave that bounces off an object changes 
with the composition of the object For example, an unpainted tank would give off a 






completely different wavelength then trees or grass would. The only problem with 
reflectivity data is that when it is used by itself recognizing and pointing out camouflaged 
targets is difficult. This is where range comes in. Because the data of range helps to 
detect the dimensions of an element, a camouflaged target can be flushed out. 
Consequently, this union would be most suited for ground mobile targets. 

Procedure 

Ideally, to write computer programs, one must know the language. The same is 
true for modifying a program. Thus, the first step taken in this venture proved to be 
research. The Pascal programming language is seen by many to be a stepping stone in 
learning other languages. It can be used as a learning tool or as an actual programming 
device. To begin this project research was done on Pascal and C, another programming 
language. C, which was not used initially, is the programming language used in the 
display program and the prime directive for the next step in this ongoing project. Due to 
Pascal's dual use, this language was incorporated first. Linked lists, pointers, and other 
program necessities were studied to give an overview of programming and finally a basic 
knowledge of the language. 

The program's purpose was to produce numerical data of an image, giving the 
maximum and minimum range, maximum and minimum intensity and a list of the 
distribution of pixels. The only problem with this code was that the data was 
overwhelming in its size and required too much manual analysis to receive useful data. 
The data given could sometimes equal to forty pages, most of which was useless. The 
depth of the information warranted a compounding and simplification of the output. 

Since the code was written in Pascal, alterations were not needed and perusal of 
the code began immediately. A week of research was given to leam the program and 
detail. In Pascal man-made variables and definitions are allowed, thus the language can 
be slightly altered. This causes difficulties, when one is modifying a program that is not 


30-4 





their own. After the study, remodeling began. 

To remodel the code, two windows were run simultaneously, on Turbo Pascal 
6.0. One window had the actual program, and the other contained tentative adjustments. 
The latter window was compiled and tested for errors, thoroughly before tried in the 
actual program. Running the code with the modifications, was the next step. Copies of 
the original program and changes made along the way were made for reference sake. 

The automated program was changed into a manual form in order to monitor the progress 
of the code. After two weeks of the trial and error process, the program was completely 
compiled, error-free, and most importantly, achieved the objective. 


Result 

The display of output shows only the usable data. The modifications of the 
program have lowered the number of allowable zeroes to three, therefore alleviating the 
problem of too many pages of unusable data. Instead of approximately forty pages of 
data per image, the reduced output is one to two pages. Also the man-in-the-loop 
analysis has been reduced from one hour to approximately two minutes. From an 
environmental standpoint, hundreds of trees were conserved. 

Conclusions 

The primary objective has been conquered. The next step will automate the entire 
process of obtaining and displaying the needed information. The cut on man hours will 
allow more time for actual research and less time for unnecessary analysis. C 
programming will need to be reviewed to carry on this process of automation. 


30-5 





ACKNOWLEDGEMENTS 

Primarily, I would like to thank Emily Martinez, my mentor and friend. Without 
her, I would never have gotten through this summer or Pascal programming. Good luck 
on your thesis, Em. I know that your infinite wisdom will not go unnoticed. I would also 
like to acknowledge Emily's husband, Otto. Thanks for the laughs (you do a great 
impression of a fish). I will also like to thank the other apprentices for their 
companionship, and the good times that we had. I enjoyed everyone one of you. 
Christina, Jenny, Barry, Kyle, Nancy, Mary, Jon, Melissa, Jack, everyone: THANKS! I 
learned so much this summer and I am glad to have obtained this experience. I must 
thank God and my family for keeping me on the straight and narrow all these years. I 
am also very thankful for William A. Washington, Jr., who has given me strength, love, 
devotion and a hard time about everything. I can't wait until next year. 





FACTORS INFLUENCING THE DEPOSITION OF POLYPYRROLE FILM 


Deanna Harrison 
High School Apprentice 
Fuzes Branch 


Wright Laboratory Armament Directorate 

WL/MNMF 

Eglin AFB, FL 32542-5434 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 


August 1993 


31- 1 





rACTORS INFLUENCING THE DEPOSITION OF POLPYRROLE FILMS 


Deanna Harrison 
High School Apprentice 
Fuzes Branch 

Wright Laboratory Armament Directorate 


Abstract 

Most polymers are insulators, such as plastics, but a few that have 
been recently developed are conductive. There has been a great deal of 
experimentation done with conductive polymers, and in many cases important 
factors have been ignored or left unreported. Since this is a fairly new 
field of study, many factors that affect the quality of the films are still 
unknown. If all of the factors that influence film deposition are not 
analyzed, data gathered in experimentation can be deceiving. Each factor 
must be identified and studied in order to attain accurate and consistent 
results. The purpose of this project was to determine what factors 
influence the quality of polypyrrole films and how each factor affects 
them. 


31-2 



rACTORS INFLUEHCZN6 THE QUALITY OF POLYPYRROLE FILMS 


Deanna Harrison 

JTRODUCTIQN 

Most polymers are insulators, such as plastics, but a few that have 
sen recently developed are conductive. In the past few years there has 
sen much research done on conductive polymers because of their many 
ractical applications. They can be used to make rechargeable batteries 
tat do not work by chemical reactions. This process eliminates the 
roblem of corrosion. Smart windows are another recent development in which 
mductive polymers are used to block out sunlight and heat by turning 
>aque. This reaction is triggered as light or heat passed through the 
.ndows. Double layer capacitors can also be made using conductive polymer 
.lms as electrodes [1]. These films can be used with nonaqueous 
.ectrolytes and can sustain higher voltages than other materials commonly 
ted in capacitors [2]. 

There are some significant advantages to using conductive polymers 
tstead of metals. They are very, light weight and relatively easy to make. 

: is also possible to alter their physical and electrical properties by 
tanging the conditions at which the polymer films are deposited [3-5]. 


:scussiQN QE PROBLEM 

There has been a great deal of experimentation done with conductive 
tlymers, and in many cases important factors have been ignored or left 
treported. Since this is a fairly new field of study, many factors that 


31-3 





affect the quality of the films are still unknown. If all of the factors 
that influence film deposition are not analyzed, data gathered in 
experimentation can be deceiving. Each factor must be identified and 
studied in order to attain accurate and consistent results. The purpose of 
this project was to determine what factors influence the quality -f 
polypyrrole films and how each factor affects them. 

METHQDQLQfiX 

By analyzing the results of previous research, eight factors were 
chosen to be tested for their effects on the quality of polypyrrole. There 
may be additional factors which have not yet been identified, and future 
research can be done to search for any other conditions which might affect 
the deposition of the films. The eight factors that were studied in my 
project were: dopant, molarity, current density, deposition time, 
substrate electrode material/quality, anode-to-cathode spacing, 
temperature, and agitation. 

The quality of electrodeposited polypyrrole film material depends 
greatly on what dopant is used. To analyze this factor, different dopants 
have been tested. Ammonium chloride was the main dopant that we compared 
to dodecylbezenesulfonate (DBS) . In the remainder of the experimentation, 
DBS was used as a controlled dopant. 

The molarity of the polymer and the dopant present in the deposition 
solution also could be a factor, but past research has not shown a 
noticeable difference in film quality as the molarities of the chemicals 
are varied. In all previous experiments, 0.05 M PPY and 0.05 M DBS were 
used. DBS molarities of 0.1, 0.15, and 0.2 were tested and no significant 
differences in film quality or conductivity resulted. 

From experience, as well as from other research, it has been 


31-4 





determined that current density plays a crucial role in determining the 
quality of polypyrrole films. To find the ideal situation, various current 
densities were tested. In solutions that contained different dopants, or 
in solutions that contained different molarities of a dopant, the voltage 
required to achieve a desired current density would vary. 

Another factor that influenced the quality of the films was the time 
period over which deposition occurred. Film thickness varies directly with 
the deposition time allowed. The deposition time allowed should depend at 
least partly upon what the films are to be used for. If a more durable 
film is desired, a longer deposition time would be beneficial. 

The type of electrodes used in the deposition of polypyrrole films can 
also affect their quality. In some chemicals, certain types of electrodes 
can corrode, causing the solution to be contaminated. If the electrodes 
are not polished, abrasions on the surface can cause flaws in the polymer 
film. Many types of electrodes were tested, including nickel, zirconium, 
and stainless steel. Different thicknesses of these metals were also 
tried. 

Not only is the type of electrode important, but the distance between 
electrodes is also a significant factor affecting the quality of 
polypyrrole films. Various anode-to-cathode spacings were tested, but more 
experimentation could be done in this area to determine the exact 
limitation on how small the spacing can be without degrading film quality, 
has also been found to affect film quality. There are two different ways 
of altering film texture and conductivity using temperature. The solution 
can be exposed to various temperatures during deposition, or the film can 
be aged at different temperatures to alter physical and electronic 
characteristics of the film. A temperature chamber was used to perform 


31-5 




both types of tests. The freezing point of the PPY/DBS solution had to be 
found before any tests were done at low temperatures. Films were deposited 
slightly above the freezing point at 7° C, and at temperatures of 23°C, 

39°C, 60°C, 80°C, and 100°C. 

Films that had been previously made were heated to temperatures of 180° C. 

The last factor that was determined to influence the quality of 
polypyrrole films was agitation. Depositions were done using a magnetic 
stirrer and an ultrasonic cleaner. 

RESULTS 

It was found that use of ammonium chloride as a dopant in a PPY 
deposition solution results in poor quality film and is not very useful 
alone. This process does enhance surface area, however, and can be used to 
deposit textured PPY material on a smooth PPY/DBS film which has been 
prepared beforehand. DBS is perhaps the best dopant for optimum film 
strength and flexibility. As the dopant molarities were altered within the 
range of 0.05 to 0.2, no significant differences in film quality or 
conductivity were observed. 

A current density of 1 mA/cm 2 was found to be ideal for creating a 
film with optimum qualities. Significantly higher current densities cause 
films to have increased surface area, but increased brittleness as well. 

There is no increase in conductivity, either, which previous research has 
shown [6] . Polypyrrole films made at significantly lower current densities 
are too thin and tear easily. Therefore, in the ideal setting, a current 
(in mA) used should be equal to the area of the surface being coated on the 
electrode. The thickness of deposited polypyrrole film material also 
depends on the time period of the deposition. A time period of one hour 
generally seems to produce good quality films. Of course, if a thicker 


31-6 




film is desired, the time period must be increased. 

In aqueous PPY/DBS deposition solution, nickel and stainless steel 
electrodes work well. Zirconium electrodes will allow a PPY/DBS film to be 
formed on them, but what results is of substandard quality. In a 
polypyrrole solution doped with ammonium chloride, any exposed areas of 
nickel or stainless steel substrate electrode material may corrode during 
deposition and contaminate the deposition bath. The only electrochemically 
stable electrode that we found suitable for this type of electrodeposition 
treatment is indium tin oxide coated glass. It is also beneficial that 
such substrate material is highly polished and much smoother than metal 
sheet material. Resulting film quality is generally better if polished 
electrodes are used, or if a new electrode is used for each deposition. 

New stainless steel sheet electrodes were used in all of the other 
experiments to help control the effects of substrate surface abnormalities. 

It was also found that anode-to-cathode spacing of 8 cm and 15 cm 
promote growth of good quality films. Electrode spacings of less than 4 cm 
inhibits the deposition of the film, significantly lowering film quality. 

The limitation on how far apart the electrodes can be is a topic for 
further research. 

It was found that increasing the temperature during deposition 
increased surface area, and in some cases slightly increased conductivity. 

The film grown at 40°C was slightly textured in certain areas. The films 
grown at 60°C and 100°C started to wrinkle as they detach themselves 
from the substrate. These films were relatively smooth, with the exception 
of the wrinkles. The film deposited at 80°C had a very fine, even texture. 

This film was examined using a scanning electron microscope (SEM), which 
revealed a cauliflower surface structure (see figures 1 & 2) . A film was 


31-7 







also made at 7°C, which is slightly higher than the freezing point of the 
PPY/DBS solution. This produced a good quality smooth film with a slightly 
higher conductivity than the film deposited at 23°C (room temperature) in 
the same controlled temperature chamber (see figure 3) . The conductivities 
of these films were good compared to reported conductivities of polypyrrole 
films (see figure 4) . Films were also placed in the temperature chamber 
after they had been made. This slightly increased conductivity of the 
films and seemed to make them more flexible. However, the films wrinkled 
up, especially when they had been removed from the electrodes prior to 
heating. 

Agitation was also found to affect the quality of polypyrrole films. 

The films subject to the turbulence of a magnetic stirrer had flaws which 
caused them to tear easily. In the first experiment with ultrasonic 
agitation, the ultrasonic device overheated after 30 minutes. This film 
was very textured, and fairly flexible. However, it was postulated that 
the heat alone could have been responsible for the marked increase in 
texture. This was proved by repeating the experiment using water to keep 
the ultrasonic bath from overheating and maintain the solution much closer 
to room temperature. The film was smooth and flexible. The experiment was 
repeated once more, using ice water to cool the solution. Although it 
began to wrinkle and detach itself from the substrate electrode, this film 
was also quite smooth and confirmed the hypothesis that temperature affects 
PPY film growth considerably more than ultrasonic vibration does. 

CONCLUSION 

If high surface area films can be made using higher temperatures, 
coating films with textured material deposited from PPY/ammonium chloride 
solution may be unnecessary. Since factors such as the temperature of the 


31-8 






room can affect the quality of the films produced, the researcher must be 
careful to keep a controlled environment. One must remain aware of all the 
factors that can influence the growth of polypyrrole films in order to keep 
the experimental environment constant. 


31-9 







REFERENCES 


1. M.G. Kanatzidis, "Conductive Polymers, " Chemical and Engineering News, 
Dec. 3, 1990. 

2. S. DiStefano, "Conducting Polymers for Thermoplastic Electrodes, " JPI 
Task Plan No. 80-3201, Air Force Armament Laboratory, August 1991. 

3. Same as above 

4. D. Harrison, "High Surface Area Conductive Polymers," High School 
Apprenticeship Program Final Report, August 1992 (Wright Laboratory 
Armament Directorate, Eglin AFB, FL) p.18-1 to 18-6. 

5. D. Finello, D. Harrison, R. K. Bunting, "High Surface Area Conductive 
Polymers." From: Procedings of the 2nd International Seminar on Double 
Layer Capacitors and Similar Energy Storage Devices, December 1992. 

6. R. C. D. Peres, J.M. Pernaut, and Marco-a. De Paoli, 
"Polypyrrole/Dodecylsulfonate: Effects of Different Synthesis Conditions, " 
Journal of Polymer Science: Part A: Polymer Chemistry, Vol. 29, 1991. 


31-10 




















Characteristics of PPY/DBS Films Deposited 
On Stainless Steel Electrodes 


Conductivity (S) 

67 

CO 

38 

89 

89 

00 

00 

Texture 

O 

o 

E 

CO 

smooth 

slightly textured 
in certain areas 

detached from 
electrode 

fine texture 

detached from 
electrode 

<d 

3 

G3 

o 

O 

O 

O 

O 

O 

i- 

<D 


00 

O 

o 

o 

O 

Q. 

E 


CM 


CO 

00 

o 

0) 








31-13 


figure 3 

























HIGH SURFACE AREA CONDUCTIVE POLYMER FILMS 

1 

USING AN AMMONIUM CHLORIDE AQUEOUS SOLUTION 


Laura Heraner 
High School Apprentice 
Fuzes Branch 


Wright Laboratory Armament Directorate 
WL/MNMF 

Eglin AFB, FL 32542-5434 


Final Report for: 

High School Apprentice Program 
Wright Laboratory Armament Directorate 


Sponsered by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 


August 1993 


32-1 






HIGH SURFACE AREA CONDUCTIVE POLYMER FILMS 
USING AN AMMONIUM CHLORIDE AQUEOUS SOLUTION 


Laura Hemmer 
High School Apprentice 
Fuzes Branch 

Wright Laboratory . Armament Directorate 


Abstract 

In recent years, researchers have found numerous applica¬ 
tions for conductive polymers. For example, polymer films can 
serve as electrodes in capacitors [1]. 

The goal of this project was to increase the surface area of 
conductive polymer electrodes thereby increasing their capaci¬ 
tance. The method tested was to increase surface area by coating 
polymer electrodes of dodecylbenzenesulfanate (DBS) doped poly¬ 
pyrrole (PPY) with chlorine doped podypyrrole electrodeposited 

from an ammonium chloride (NH Cl) aqueous solution (Figure 1.) 

4 

These textured polymer electodes were then used to make a double 
layer capacitor [2]. 






high surface area conductive polymer films 

USING A» 'AMMONIUM CHLORIDE AQUEOUS SOLUTION 

« 

Laura Hammer 4 


INTRODUCTION 

Conductive polymers have become a growing interest for 
scientists in recent years. Many experiments have been undertak¬ 
en to determine uses for these "organic metals." Past work has 
determined that conductive polymers have numerous applications. 

They can be fabricated into rechargeable batteries or serve as 

» 

electrodes in capacitors. Conductive polymers are even used in 
"smart windows," which are light sensitive devices. Their versa¬ 
tile nature has opened up many new possibilities [1]. 

DISCUSSION OF PROBLEM 

One way to increase the capacitance of a double layer capac¬ 
itor is to increase the surface arfea of the electrodes. This 
creates more interaction along the interface between the electo- 
lyte and the electrodes. The objective of this project was to 
make high surface area conductive polymer electrodes to be used 
in a double layer capacitor. This was done by coating base 
polymer films with a textured layer of chlorine doped PPY grown 
from an aqueous ammonium chloride solution. Other methods of 
making high surface area films, such as pulsing the deposition 
current, have been explored but none of the results have been 
quantified in terms of capacitor performance [3]. 


32-3 




METHODOLOGY 


The first step in this experiment was to make base polymer 

films. Two 8x15cm electrodes were placed 15cm apart in a 

17x12x8cm container (Figure 2.) Stainless steel electudes were 

used for the majority of the experimentation. However nickel, 

zirconium, and indium tin oxide coated glass were also tested. 

The indium tin oxi'de glass electrodes were made using standard 

microscope slides (7.5 x 2.5 cm.) One liter of a solution of 

0.05 M PPY mixed w^h 0.05 M DBS was used to electrochemically 

deposit a conductive PPY film onto the anode (Figure 1.) For the 

2 

deposition a current density of 1 mA/cm was used for one hour. 

These conditions were found to produce good quality PPY films. 

A solution of 0.05 M PPY and 0.05 M NH Cl could then be used 

4 

to deposit a textured layer of chlorine doped PPY onto the base 
polymer film (Figure 1.) Again, two 8x15cm electrodes were 
placed in a 17x12x8cm container 15cm apart (Figure 2.) Nickel, 
stainless steel, zirconium, and indium tin oxide glass electrodes 
were all tested with the ammonium chloride solution. Indium 
tin oxide glass electrodes produced the best quality films and 
were the only electrodes which showed no evidence of deteriora¬ 
tion in the ammonium chloride solution. One liter of the solu¬ 
tion was used for the depostion while the length of the deposi¬ 
tion was reduced to 45 minutes. The current used in this stage 
of the experiment was varied to determine the level of current 
which produced high surface area films with good quality. Agita- 


32-4 




tion was also experimented with to evaluate its effect on tex- 
tured film growth. 

' Following the ammonium chloride deposition, the films were 
then tested on the four point probe to measure resistance. Film 
sheet resistance was displayed by a Precision LCR Meter in order 
to calculate the resistivity and the conductivity of the PPY film 
for each experiment unless precluded by extreme surface rough¬ 
ness. 

Two double lay^r capacitors were then made, one using two 
textured PPY films as electrodes and the other using base polymer 
electrodes. The electolyte in these capacitors was .created using 
strips of Whatman filter paper soaked in a solution of nitrometh- 
ane and potassium iodide at approximately half the solubility 
limit [4,5]. Once the capacitors were constructed, they were 

charged for several minutes and the steady state voltage (V) was 

- • 

measured by an multimeter. A coulometer was then used to meas¬ 
ure the charge (Q) stored in the capacitors. Following voltage 
and charge measurements, the capacitance (C) of the capacitors 
could be calculated using the equation C=Q/V [6], Since surface 
area is directly proportional to capacitance, comparison of the 
calculations would be indicative of the increase in surface area 
[7]. 

RESULTS 

Optimum deposition parameters were determined for the depo- 


32-5 







I " 

sition of chlorine doped PPY from an ammonium chloride solution. 

f 

1 

Three main variables were tested: the effects of agitation, 
current density, and the type of electrode used. After much 
experimentation it was determined that agitation of the solution 
decreased the film quality. Agitation also caused uneven coating 
of the base PPY films;. 

In order to stjidy the effects of changing the current densi¬ 
ty, the size of the film was kept constant while the current was 

changed. The currents tested ranged from 5 - 45mA. However, the 

2 

the most textured films were made at 30 - 35mA (0.87 mA/cm .) 
High resolution (1500x) scanning electron micrographs of the most 
textured film material and the base film material are compared in 
Figures 3 and 4. 

Four different types of electrodes were tested in this 

project. Both nickel and zirconium electrodes could be used in 

the PPY/DBS depositions but they proved to be electrochemically 

unstable during the PPY/NH Cl depositions. Stainiess steel elec- 

4 

trodes also exhibited deterioration during deposition but to a 
much lesser extent. The indium tin oxide glass electrodes proved 
to be the best for this study. The films deposited on these 
electrodes were better quality and had more texture than the 
films made on the other electrodes. Also, the indium tin oxide 
glass showed no evidence of corrosion during PPY deposition in 
the ammonium chloride solution. These results provide the best 
known conditions for textured PPY film depositions. 


32-6 



The textured PPY films were then useu as electrodes to 

♦ 

1 

construct a double layer capacitor. The double layer capacitor 

with chlorine doped PPY electrodes was successfully charged to 

-2 

0.5 volt with 2.92 x 10 coulomb while the one using base PPY 

-7 

electrodes could only store 1.0 x 10 coulomb when given the 

same source voltage. The capacitance of the double layer capac- 

? -2 

itor using textured PPY electrodes was 5.84 x 10 farad while 

-7 

the capacitor made from base PPY electrodes had only 2.0 x 10 
farad. Because surface area is directly proportional to capaci¬ 
tance, it can be deduced that the surface area was much greater 
for the electrodes made of chlorine doped PPY films [7]. The 
large increase in measured capacitance indicates that the surface 

5 

area can readily be increased by a factor of almost 3 x 10 
through surface texture enhancement. 


CONCLUSION 

The goal of this project was to make high surface area 
conductive PPY films by coating base films with a layer of tex¬ 
tured PPY material from an aqueous mixture of ammonium chloride 
and PPY. Through experiment, improved conditions for making 
textured PPY films were determined. These conditions were used 
to produce high surface area conductive polymer films. After 
textured PPY films were made, they were used as electrodes to 
make a double layer capacitor. The measured capacitances were 
indicative of the surface area of the PPY films. The results 


32-7 



substantiated the hypothesis that coating base PPY films with 


chlorine doped PPY will increase the surface area thereby in¬ 
creasing the capacitance. 



REFERENCES 


i'. M.G. Kanatzidis, "Conductive Polymers," Chemical and Engineer¬ 
ing News, Dec. 3, 1990. 

2. D. Finello, "Solid Laminated Double Layer Capacitor," AFATL- 
TR-90-72, Air Force Armament Laboratory, July 1990. 

3. D. Harrison, ?High Surface Area Conductive Polymers," High 
School Apprenticeship Program Final Report, August 1992 (Wright 
Laboratory Armament^Directorate, Eglin AFB, FL). 

4. D.J. Anderson, General Review of Electrolytes . J. Electrochem¬ 
ical Society, 1977. 124: pp. 401c-409c. 

5. H.B. Oakley and J.C. Philip, Conductivity and Ionization of 

Solutions of Potassium Iodide in Nitromethane . J. Chemical Socie¬ 
ty, 1924. 125: pp. 1189-1195. 

6. M.E. Van Valkenburg, Network Analysis . Prentice-Hall, Inc., 
Englewood Cliffs, New Jersey, 1974. pp. 7-19. 

7. V. Del Toro, Principles of Electrical Engineering . Prentice- 
Hall, Inc., Englewood Cliffs, New Jersey, 1972. pp.4l. 


32-9 






Chemicsi stucture of pdypyrrde PFY with Ammonium Chlorid 

dcdecyibenzsnesuifanste DBS counterion 



Proposed disordered stuciirsi model for pdypyrrde 



FIGURE I 
32-10 


DEPOSITION OF PPY 


Multimeter or 



FIGURE 2 


32-11 











\ 








r* 


(TT 


r 


nr 


r 



















FIGURE A 






USING NEURAL NETWORKS FOR THE DETECTION OF POTENTIAL 
TARGETS IN AN IMAGE SEGMENTED BY FRACTAL DIMENSION 


Mark E. Jeffcoat 
High School Apprentice 
Advanced Guidance Branch 

Wright Laboratory Armament Directorate 
WL/MNGA 

Eglin AFB, Florida 32542-5434 

Final Report for: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 

Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D. C. 





USING NEURAL NETWORKS FOR THE DETECTION OF POTENTIAL 
TARGETS IN AN IMAGE SEGMENTED BY FRACTAL DIMENSION 

Mark E. Jeffcoat 
High School Apprentice 
Advanced Guidance Branch 


Abstract 

This paper discusses the use of neural networks to detect potential targets within an image, specifically one from 
the Advanced Technology LAdar (Laser Detection and Ranging) System (ATLAS) program. The ATLAS sensors 
include a Ladar beam and an infrared sensor. This research included only the active (range) and passive (infrared) sensor 
data. A neural net removed defects from the active images, caused by atmospheric interference with the Ladar beam (not 
present in the passive images). A neural net separated an image into road, building, and natural pixels, given both the 
original and fractal dimension data, with very few unknowns or errors. 

Introduction 

A goals of image processing, especially for military applications, is to develop methods to identify points of 
interest within an image without human assistance. The use of computers to detect targets can result in improvements in 
speed and accuracy. The first objective of the research was tn evaluate the nsefolnes s of fractal dimension data in 
detecting targets within an image. If the fractal algorithm is sucessful, then the second objective was to find an auto¬ 
mated technique to apply the fractal data toward detecting potential targets. 

Apparatus 


A Sun SPARCstadon 1 workstation, wife occasional assistance (over an Ethernet network) from a SPARCServer 
MP690. provided fee processing power necessary to execute the fractal dimension algorithm. Khoros. an image process¬ 
ing package from the University of New Mexico, assisted in routine manipulation of fee sample images and provided 


useful image manipulation routines, which could be easily called from the C programming lan guage. 


Background 


Fractal geometry deals with shapes difficult to describe in traditional topological terms. Mathematical fractals 
often exhibit properties such as infinite surface area or perimeter. The no rmal topological concept of dime-nsion fails to 
property describe such a shape. The quantity of fractal dimension gives a more accurate description of a fractal shape, 
and will generally be a higher number than fee topological dimension. Natural shapes, often highly fractal, should have 


33-2 






a higher fractal dimension than a man-made shape. Ideally, this difference would allow the segmentation of images 
based solely oo fractal dimension. Reality is not quite as consistent. {1} 

The Hurst Dimensional Estimate finds an approximation of the fractal dimension (D) of an object by examining an 
image over various areas to find the degree of variation in an area: a fractal shape should vary to a much higher degree 
than a non-fractal. In equation (1). the function M is defined at each sub-area A. R is the resolution of the image 
inside the sub-area. K is a constant. 

M(AJD»K*R^ 1 (1) 

M(AJR) can be found by equation (2). The maximum and minimum pixel values from each sub-area are sub¬ 
tracted and summed. 

M(AJR) * £ (Maxi A,] - MinJA]) (2) 

Taking the natural log of each side in (1), and then taking the partial derivative of equation (3) with respect to In R, 
(D-l) appears; the constant disappears. (4) 

In M(AJi) ■ (D-l) * In R + In K (3) 

- s *SP L -°-•« 

A log-log plot of M versus R is a line. (D-l) is the slope. {2} 

A neural network is a method of arriving at a solution of a problem without standard algorithmic procedures. A 
neural network has at least three layers: input, processing (the "hidden" layer), and output. Each layer consists of nodes 
in which data is processed. Each intemode connection has a numerical weight associated with it. which adjusts to a 
specific set of problems during die training of the network. There may be multiple hidden layers. The tr aining can be 
supervised or unsupervised. The output from a supervised network is an evaluation of the conformity of the input to tire 
possible categories of output, defined by the training. La supervised training, the network receives both sample data from 
each solution category and the correct answer. The network weights are adjusted until its answers match the given 
solutions and it can predict in which category to place new input. {3} 

In both preprocessing and detection, a backward error propagation learning style was used, hi training, the 
comparison of the computed classification to the given classification determines tire error. The error is used to calculate 
the correction for the weights. The error backpropagates through the network, adjusting the weights. {3} 

The data used was from the Advanced Technology LAdar (Laser Detection and Ranging) System (ATLAS) 
program, which combines Ladar and infrared sensors. Ladar is an active sensor that uses short wavelength laser beams 
to determine range and intensity. This paper deals primarily with the range and infrared data. 

When the laser bean leaves die aircraft, it will usually travel down to the ground and reflect back toward the 
Ladar. Some of the beam's intensity is lost through atmospheric attenuation, but, in most cases, the beam will return to 
the sensor. If die bean strikes water, it will be reflected off away from the sensor. The only return will be some small 


33-3 







portion that struck something in the air and came back. This results in a random distribution of range values over water, 
leading to interesting fractal effects. Also, when the laser beam either does not return in time or too little returns, the 
pixel at that location receives a value of zero; that location is a "dropout." (Note that both of the above phenomena will 
only occur in the active data.) Preprocessing is necessary. {4) 

Methodology 

Lt. Matthew Whiteley provided two images from the ATLAS data, each of which contained active and passive 
data. The first was an image of an Astros mobile missile launcher (foreign), with few surrounding terrain features, and 
no outstanding natural objects to compare to. The second was an image taken from the local area, containing a building 
adjacent to water, with a road leading up to it and piers jutting out into the water. Trees cover the rest of the image. 

The first image, the missile launcher, contained many dropouts, as described above. There are two problems with 
these. Of primary importance is their effect on the fractal algorithm; the dropouts create false variations in the image, 
damaging to a routine whose purpose is to detect variation. Secondarily, the dropouts make it difficult to display the 
image, as the normalization would destroy all variation in the image. Khoros offers a median filter to remove errors such 
as these, but the filter smooths out variation in the image as well as removing the dropouts, which is unacceptable. Two 
methods offered a solution to die problem. The simplest method was using linear interpolation to fill in the point based 
on the surrounding points. As the objectives of the project included neural networks anyway, a neural network also tried 
to repair the images. Pixels of know values, not dropouts, served as test data. A comparison was made by taking the 
absolute difference between the actual pixel value and the value computed by the two algorithms. The total error of die 
neural network was only half the error from the linear interpolation. When the passive data (the passive data does not 
contain dropouts) supplemented the data used by the neural net, the net error decreased slightly. The finished form of 
the repl a ce m e nt routine replays pixels with values greater or less than three standard deviations from the mean (calcu¬ 
lated from random sampling of one thousand pixels). The values used by the neural network are differences of the pixels 
surrounding the pixel (cardinal directions only) from the active data, and differences with the pixel and the surrounding 
pixels from the passive data. Training the network with differences instead of actual values resulted in a range-indepen¬ 
dent network. 

This works well in an image without a body of water. Because of the random placement of range values, almost 
all water pixels are more than three deviations from the mean, and replacing them often deadens the randomness that 
separates water from other pixels. This is no perfect solution to this, unless the routine can be taught to recognize water 
(perhaps by checking to see how prevalent dropouts are in a region), but, whether using a preprocessed image or not, the 
detection algorithm is robust enough to recognize water as natural No attempt was made to distinguish water from other 
natural terrain. 


33-4 



The previous implementation of the fractal dimension algorithm, by Jason Lindsey, assumed a square unage. The 
modified routine still uses square areas, but checks to ensure that it does not expand beyond the bounds of the image. 

The current pixel is always placed in the upper-left corner of each sub-area, for the sake of simplicity. A slight improve¬ 
ment in accuracy could be obtained by placing the pixel in the center of the sub-areas. In some cases, accuracy may be 
improved with non-square areas centered around the pixel. 

The final neural network, used to distinguish between artificial and natural features, considered three possible 
categories of output. The net assigns each pixel a number from zero to one. quantifying how well a pixel fits into the 
categories of artificial structure, road, or natural. The training data, due to time constraints, came solely from the second 
image, which had a range too large to see any vehicles that may have been present With a greater set of tr aining data, 
vehicles could be included. To classify each pixel, the network used twenty pieces of data, the pixel and surrounding 
pixels from the active and passive images, plus the output from the fractal routine from the active and passive images. 
The tr aining data for the building category came solely from the building; the piers were not included in either the road 
or building data sets. 

All of the algorithms were implemented in C. using routines included in Khoros for reading and writing the image 

files. 

Observations and Conclusions 

The fractal routine alone, on visual examination, demonstrated the ability to set off the edges of objects. The 
fractal dimension is higher at the edges, even though a high fractal dimension is usually only associated with natural 
objects. This is a result of fuzzy edges, which look very fractal Ladar, when bouncing around the edges of an object 
will tend to return a fuzzy result. This should be remedied at closer ranges with higher resolution, but such short ranges 
are useless to a program interested in mounting Ladar devices on aircraft. 


Figure 1- Fractal Imageof Astros 


Figure 2 - Sobel Image of Astros 


* 




V" '~‘"V 

V- 










r-rs 

. >». 


*• 




- 


*■ 

& 






♦ 








Figure 3 - Fractal Image Hurlburt 


33-5 












The fractal image (Figure 1) was compared with a sobel routine (Figure 2). a mare common method of edge 
detection. The results were very similar (again, visual inspection only), but the fractal dimension was much clearer, 
apparently not detecting extremely fine edges. The fractal routine, when ran on both images (results displayed in Figures 
1 and 3). set off the edges of objects in the images successfully. 

The neural network was very successful in distinguishing roads (light grey), buildings (white), and trees (dark 
grey) (Figure 4). Unknowns are in black. The network classified the piers as buildings, which was good. The road 
category consists primarily of artificial planes, but the net may classify certain naturally low and flat areas as roads; 
anything in the building classification is definitely artificial. The only building in the image was clearly marked. 

No objective method tested the accuracy of the detection, but the results of the neural network agree with human 
interpretation of the image. General areas are definitely marked correctly, most individual pixels are marked correctly; 
the ones not marked correctly or marked unknown are not important to visually interpreting the image. 

Other tests concluded that the fractal data was very useful in aiding the network in classification. The tr aining 
process is erratic and undependable when the network's tr aining does not include the fractal data or includes only the 
fractal data. Using both together produced twice the accuracy of the fractal data alone, and more than three rinwa the 
accuracy of die Ladar data without the fractal data. 

The fractal dimension routine alone is a useful tool for segmenting images and providing information about the 
image. Except in the case of edge detection, it requires another algorithm or a human to interpret the output into useful 
form. A neural network can use this data to create a useful image, leading toward greater image unders tanding Work is 
still on-going in testing an :! developing these methods and their application to the ATLAS program. 



33-6 





References 


1. Feder, Jens. Fractals. Plenum Press. New York: 1988. 

2. Lindsey. Jason. Image Analysis: A Fractal Application. Research and Development Laboratories: 1992. 

3. M. Kabrisky and SJC Rogers. An Introduction to Biological and Artificial Neural Networks for Pattern Recognition. 

The International Society for Optical Engineering. Bellingham. Washington: 1991. 

4. Lt. Whiteiey. Technical Meeting. Eglin Air Force Base: July 1993. 


33-7 




EFFECTS OF VARYINS GAUGE LENGTHS ON 
"MINIATURE SPECIMEN" INSTRON AND 
SPLIT-HOPKINSON TENSILE RESULTS 


Barry W. Kress 


Wright Laboratory 
101 W Eglin Blvd. 
Eglin AFB, FI. 32542-6810 


Final Report -for: 

High School Apprenticeship Program 
Wright Laboratory 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 


August 1993 







EFFECTS OF VARYING GAUGE LENGTHS ON "MINIATURE SPECIMEN" 
INSTRON AND SPLIT-HOPKINSON TENSILE RESULTS 

BARRY W KRESS 
HIGH SCHOOL APPRENTICE 
WARHEAD BRANCH 

WRIGHT LABORATORY ARMAMENT DIRECTORATE 

ABSTRACT 

A new miniature tensile specimen is proposed -for use in the 
evaluation o-f explosively formed penetrators (EFP'S) liners. 
This is necessitated due to the desire to evaluate "real life" 
liners that are fully processed and too thin for machining cur¬ 
rent tensile specimens. Tensile testing was performed at both 
low <Instron) and high (Spl it-Hopkinson) strain rates and results 
compared back to the current 0.125" diameter/O.350" gauge length 
sample. The proposed miniature specimen size requirement was 
established as 0.090" diameter because of liner thic ■ ness. 
Various gauge lengths were evaluated from 0.214" to 0.304" in 
0.030" increments. Since the average results varied no more than 
1.1IX for all gauge lengths tested, the 0.244" gauge length was 
chosen as the new length since it preserved the gauge 
length/diameter ratio of the current tensile specimen. From the 
tensile tests performed, the average stress for the 0.125" dia¬ 
meter specimens was 266 KSI and the 0.090" specimens was 269 KSI. 
The difference between the two was 1.12X which is well within 
experimental error. 


34-2 




INTRODUCTION 


The purpose of the these tests is to establish a miniature 
Instron and Split-Hopkinson tensile sample size that is small 
enough to be cut from Explosively Formed F'enetrator (EFP) liners. 
The major problems encountered are the liner's thickness and 
curvature. The standard 0.125" diameter tensile size cannot be 
used because the "sample thread" diameter is too large to be cut 
from the liner. Therefore a smaller tensile specimen was needed 
that could be cut from the liner. After careful deliberation a 
diameter of 0.090" was decided upon. The gauge lengths to be 
tested were 0.214", 0.244" (the gauge length which utilizes the 
established gauge length/diameter ratio), 0.274" and 0.304". 
Once the new miniature sample size is developed, the actual liner 
can be directly evaluated and actual results fed into a computer 
hydrocode model for more accurate modeling and performai pre¬ 
diction. 


34-3 





EQUIPMENT AND EXPERIMENTAL CONDITIONS 


Two types of tensile testing were utilized in the develop¬ 
ment and evaluation of "miniature 1 * tensile specimens. The stan¬ 
dard .125 inch diameter Hopkinson tensile specimen was chosen as 
the control specimen for comparison purposes in the development 
of the new .090 miniature specimen. The slow rate testing was 
performed on a Instron tensile testing system. The Instron 
machine is capable of applying uniformed uniaxial loading on 
metal tensile specimens under various test conditions. The 
particular machine used in the testing relies on a constant rate 
of extension that is ind«?pendent of the applied load. This 
enables the production of consistent results, even when using 
rate-sensitive materials. The machine is programmed to move a 
specified distance in a given amount of time. For our testing of 
the .0125 inch "Hopkinson" and the new .090 inch miniature sam¬ 
ples, the speed was 10' N -1 in/in/sec. The tensile forces are 
created by a high-torque motor that drives two vertical lead 
screws that in turn operate a moving crosshead. A closed-loop 
servodrive system ensures that a constant crosshead speed is 
achieved. The servomechanism controls the crosshead position and 
therefore is able to reduce any error and sustain the inputted 
crosshead speed. The applied force is measured by strain-gauges 
that in the presence of mechanical deformation result in elec¬ 
trical resistance changes. These changes are measured by a load 
cell connected to a bridge circuit. A load cell amplifier sends 
a signal to the circuit that excites the circuit and at the 


34-4 



same time, an applied -force causes an unbalancing effect. 


The 


resulting signal once again returns to the amplifier, is boosted 
and converted into an output signal proportional to the applied 
force. This output is then recorded onto a digital storage 
oscilloscope. It is then transferred via computer controller 
interface to a computer that reduces the data and calculates the 
peak stress, strain rate, engineering stress and true stress. 
The computer then stores the reduced data and is able to plot the 
engineering stress vs. strain, true stress vs. strain, and the 
strain rate vs. strain graphs. The data can then be evaluated 
and conclusions deduced. 

The high rate testing was performed on a tensile Split- 
Hopkinson Bar system. The unit used in our testing is composed 
of a striker bar and an incident bar split into two parts, all 
parts are made of Inconel 718. The two mating ends at the split 
junction have threaded inserts for mounting of small tensile 
specimens as noted in figure 1. The tensile specimen is screwed 
into one end of the incident bar, and then a collar is placed 
over the specimen which is subsequently screwed into the adjoin¬ 
ing end of the bar. It is important to make sure that the bars 
fit tightly against the collar to avoid erroneous data. The 
specimen is pulled into by a tensile pulse created by the reflec¬ 
tion of a compressive pulse that transmits through the bar. This 
pulse is achieved by accelerating the striker bar against the 
incident bar containing the specimen in the split area. This 
impact generates a compressive wave that travels down the bar and 
ideally passes through the collar without prestraining 


34-5 




34-6 


Figure 1 















the specimen. The pulse continues to travel until it reaches the 
•free end of the bar and is then re-flected back as a tensile 
pulse. Upon making contact with the specimen, the tensile pulse 
is partially transmitted and partially re-flected back towards the 
free end of the bar. The collar is not subjected to any tensile 
loads because it is not connected to the bar. Two strain gages, 
one on each bar approximately 3 feet each from the test specimen, 
measure the initial pulse and then the distortion of each future 
pulse in order to determine the peak stress of the specimen. 
This data is then collected and recorded by a digital storage 
oscilloscope. The data is subsequently transferred to a computer 
via computer uplink where it is condensed and the peak stress of 
the specimen is calculated. A plotter is also on-line to plot 
the true stress versus strain curve. 


34-7 





RESULTS AND DISCUSSION 


The tensile Instron was used to evaluate two sets o-f speci¬ 
mens -for the 0.0125'' di ameter/0.350" gauge length. This yielded 
an average o-f 266 KSI for set 1 and 254 KSI for set two. The 
difference being 4.SIX. Four gauge lengths were evaluated for 
the new 0.090" diameter miniature specimens. These gauge lengths 
were 0.214", 0.244", 0.274", and 0.304" and the resulting aver— 
age stresses were 268 KSI, 271 KSI, 268 KSI, and 269 KSI respec¬ 
tively. The difference between the high and low is 1.11%. The 
difference between the 0.12f" diameter high average value of 266 
KSI and the average 0.090" sample value of 269 KSI is 1.12%. 

The tensile Split-Hopkinson Bar was used to evaluate the 
differences between ~.he 0.125" diameter/0.350" gauge length 
samples and the 0.090" diameter miniature specimens. The 0.125" 
diameter samples yielded an average stress of 265 KSI. Testing 
of the 0.090" samples was limited however to the 0.244" gauge 
length and yrelded an "exaggerated" average peak stress of 289 
KSI and an "apparent" actual stress of 255 KSI. The exaggerated 
peak stress was influenced by a misalignment in the bar. 
Although this misalignment was also present for the 0.125" dia¬ 
meter sample, it had little effect because of the relative size 
of the data signal. The raw data signal for the 0.090 diameter 
sample was much smaller than the 0.125" sample. In fact, the 
0.090" peak signal was slightly smaller than the misalignment 
signal and this therefore caused a peak in the stress curve that 
is erroneous (see figures 2 & 3). The difference between the 
0.125" diameter/0.350" gauge length 


34-8 




SPLIT HOPKINSON PRESSURE BAR TEST 

MATERIAL DESCRIPTION:4340 TEST DESCRIPTION:4340 

GAGE LGTHCIN)“.380 DIR(IN)-.125 TEMP(C)- 23 



tE Ul Ul - 
KKh » 
hca 
z in o w 

(T V u If 

a tE in u; 
h y y k 
intu-E 





TRUE STRESS (PSI) VS TRUE STRAIN 



SPLIT HOPKINSON PRESSURE BRR TEST 

MATERIAL DESCRIPTION:4340-1 TEST DESCRIPTION:244A 

GRGE LGTH(IN)-.380 DIRCIN)-.090 TEMP(C)- 23 

STRAIN RATE PER/SEC:597.80 VELOCITY IN/SEC:195.58 

PEAK STRESS (PSI>-287574.96 PERK STRAIN (IN/IN).3197 


i 

t 


i 

i 

i 


i 

t 

i 


oi-n 

i 


s 

in 


u 

z 


tn 

H 

a 

x 

u 

cc 

m 

(E in 
CD 


g 


in 

in 

to 

tn 


s 

5 

O 

ro 

tn id 
tn z 


c Of 

3 Id 
►-> Q 
Z 
CU 3 
W 
«• 

Id - 

H ft 

§5 

I- CE 
CO Id 



TRUE STRESS (PSD VS TRUE STRRIN 




average stress of 265 KSI and the 0.090" diameter/0.244" 
length average “apparent" stress of 255 KSI is 3.77*/.. 


gauge 


34-11 




CONCLUSIONS 


The 0.090“ diameter specimens with gauge lengths ranging 
from .214“ to .304" have very little variation in their stresses. 
In fact the difference between the 0.090" diameter/0.244" gauge 
length specimen and the 0.125“ di ameter/0.350" gauge length 
specimen tested on the Split-Hopkinson Bar was less than the 
difference of the two 0.125" diameter/0.350" gauge length speci¬ 
men sets tested on the Instron machine. Therefore the results 
achieved by testing miniature samples of diameter 0.090" with 
gauge lengths ranging from 0.214" to 0.304" are acceptable and 
within experimental accuracy. The 0.244" gauge length was chosen 
as the gauge length for a 0.090" diameter sample in order to 
adhere to the established gauge length/diameter ratio. 

Bar stock mechanical properties can still be used on 
initial hydrocode development and then the actual mechanical 
properties from manufactured liners can be fed into a finalized 
hydrocode model. This will ensure that final hydrocode models 
are realistic and' result • in the most accurate "warhead perfor— 
mance" evaluation tool possible. 


34-12 



ACKNOWLEDGEMENTS 


First of all, I would like to thank my fellow apprentices 
for all the fun times this summer. Especially Jenny, Christie, 
Christina, Kyle, Jon, Nancy, Mary, and Melissa. 

Secondly, I would like to thank Don Harrison and Glenda Apel 
for not only sponsoring the program, but also for lending help 
whenever it was needed. 

Thirdly, I would also like to thank all the guys at the lab 
that made my summer enjoyable and who shared their advice and 
expertise with me. Thank you: Bob LeBeau, who was always readv 
with a joke, Homer Garner, who was always their to help me get 
things done. The Model Shop, who made my testing possible, Tony 
Weekly, who let me mess up his desk all summer, Thad Wallace, who 
took the time to teach me how to operate all the machines in the 
metallurgy lab, Aaron Brinson, for letting me work in his 
branch, A1 Welle, for allowing me to work in his section, and 
all the other guys in Warheads that treated me as a professional. 

Most of all, I would like to extend my gratitude to my 
mentor. Dr. Morris F. Dilmore. Dr. Dilmore not only taught me 
about metals and warheads, but about life. In the process he also 
replaced a four year old dream of becoming a chemical engineer, 
and in its place left a rtew dream of becoming a materials en¬ 
gineer. For all of this and so much more that cannot be put into 
words I thank him. 


34—13 





REFERENCES 


Metals Handbook . Vol. 8. 

American Society -for 


Ed. John R. Newby. 
Metals: Metals Park, 


Ni nth 
Ohio, 


Edition. 

985. 


34-14 







DEVELOPMENT OF AN ENGINEERING ANALYSIS 
TOOL USING FORTRAN 


Elliot Moore II 
High School Apprentice 
Warheads Branch 


Wright Laboratory Armament Directorate 
WL/MNMW 

Eglin AFB, FL 32542-5434 


Final Report For: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored By: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington D.C. 


August 1993 


35-1 




DEVELOPMENT OF AN ENGINEERING ANALYSIS 
TOOL USING FORTRAN 


Elliot Moore II 
High School Apprentice 
Warheads Branch 

Wright Laboratory Armament Directorate 
ABSTRACT 

For a second summer, I worked as an apprentice to Mr. Michael E. Nixon, 
in the computational mechanics section of the Warheads Branch. My project 
involved the development of a FORTRAN program to be used in testing strain 
values from Taylor Impact Specimens. The main purpose of the program was to 
find the location of elastic/plastic interface, as a function of time. The 
location is based on given strain criteria. The elastic/plastic interface 
data are points written to a separate file for use with the Microsoft EXCEL 
spreadsheet. The data was then plotted and used for analysis by the engineer. 


35-2 





DEVELOPMENT OF AN ENGINEERING ANALYSIS 
TOOL USING FORTRAN 

Elliot Moore II 


INTRODUCTION 

The principle behind this entire paper centers around the Taylor Impact 
Test. The Taylor Impact Test was designed to determine the behavior of 
material deformation under varying strain rates. To a certain point, every 
object is elastic. Elasticity refers to a materials ability to return to its 
natural state, after a certain level of deformation, like a rubber band. A 
rubber band can be stretched, or deformed, and then snap back into its 
original position because it is highly elastic. However, there is a point 
where the strain exceeds an object's elasticity, and it can no longer return 
to its original position. The deformation is no longer elastic, but instead 
becomes plastic. For instance, if a rubber band is stretched too much, or too 
far, the rubber band breaks. Determining the behavior of objects at their 
elastic/plastic interfaces is an area of interest for Warhead Branch 
activities. The test observes the deformation of a cylinder as it is impacted 
against an anvil of hard material. 


OK* 


rHonoi.ooY 


Before any program can be effective, it must have data to use. The data 
is obtained from the EPIC hydrocode. Hydrocodes are lengthy and complex 
computer programs that are used to simulate warhead formations, penetrations, 
and target responses. The EPIC hydrocode is widely used by the Department of 
Defense and is sponsored by Wright Laboratory. Fittingly, EPIC stands for 
Elastic Plastic Impact Computations. Figure 1 shows an example of a partially 
deformed cylinder. The line across the cylinder represents the points where 


35-3 




the elastic/plastic interface occurs. The enlarged block indicates how EPIC 
simulates the material in a Taylor Impact Specimen. The nodes represent mass 
and the elements represent volume of the material in the specimen. The 
positions of the nodes versus time the cylinder impacts, are simulated by EPIC 
and this data is written into a file for post processors. The data is 
eventually written to two -iles, one containing time vs. x-position data and 
the other containing time vs. z-position data. X-position represents the 
position of the nodes radially and z-position represents the position of the 
nodes axially. These files are used by the program to determine 
elastic/plastic interface information. Figure 2 is an example of a file 
containing time vs. x-position. 

The data from EPIC determine the position of the nodes at various times. 
My program determines the time and position of elastic/plastic interface from 
this data. The point of elastic/plastic interface is found by determining 
when the exterior node has met a given strain criteria. Data i av when a 
given node reaches a critical strain as determined by scrit- R-K^/RO. RO 
represents the original position of the node radially (along the x axis) and R 
represents the current position of the node. When the node meets this 
criteria, it has reached the elastic/plastic interface. A flow chart 
containing the overall logic of the program is shown in Figure 3, and the 
final source code is contained in Appendix A. My program, called plastic.f, 
is the result of trial and error. About five or six different derivatives for 
the program were written, each one a little better than the last, until the 
final version was tested and completed. 

The first section of the program is responsible for the variable 
declaration. This simple means that the ype of variables to be used in 

•4 


35 - 





the program are defined as integers, real numbers, characters, or an array. 
This lets the program understand how to read or write the variable values. 

The first section also opens the files that will be used for input and output. 
The statement "open(unit=3,file=‘plasticx.in*,status=‘old*)* defines the file 
named 'plasticx.in* as the unit number 3. This is important for the read and 
write statements in FORTRAN which reads and writes to files by unit number. 
Each file is designated by a unit number. The •status=‘old' * phrase, at the 
end of the first two open statements, signifies that the files 'plasticx.in*, 
which contains data on x-position, and 'plasticz.in', containing data on z- 
position, already exist and will be used as input files. The same basic logic 
holds true for the next OPEN statements concerning the files 'plasticz.dat' 
and 'plastic.out *. The "status='new'■ phrase, at the end of the last two open 
statements, signifies that the files do not exist yet and will be the 
destination of written output. The final two statements in the section 
initialize a counter for counting the number of nodes and data points later in 
the program. 

The second sectior "he program is the internal documentation of the 
program logic. It defin o the user of the program the people responsible 
for its design and the purpose of the program overall. Section 3 of the 
program is also for user reference. It defines what each variable in the 
program represents. The next section is used by the program to find out how 
many strain criteria the engineer wants to test, and what the values of the 
strain criteria will be. When run, the question "HOW MANY STRAIN VALUES?" 
will appear on the screen. After a number is input, the computer will ask the 
engineer to "INPUT STRAIN CRITERIA VALUE" for as many times as the engineer 
requested. These strain criteria's will be stored in the 'scrit(k)' array. 


35-5 






Section 5 sets the program in motion. First, the program finds the number of 
data points per node, represented by the variable *count2*. Then it finds the 
number of nodes, represented by the variable 'countl'. 

After finding the number of nodes and the number of data points per 
node, the command 'rewind 3' tells the program to go back to the beginning of 
unit file 3 (named 'plasticx.in* by the OPEN statement). The program then 
begins to read the data values again, this time placing the data into their 
appropriate array names, such as time values being written to the 'time(j)' 
array. Data is also placed in the 1 xdata (i, j) * array and ‘ zdata (i, j) ' array 
containing data on x and z-position respectively. The next section of the 
program is very important because it determines the strain for each data point 
of the nodes. In the last section of the program, the strain values are 
tested against the strain criteria values input by the user at the start of 
the program, to determine the data points at which the elastic/plastic 
interface first occurs. These data points, one for each node, are written to 
a written to unit file 10 (named 'plasticz.dat• by the OPEN statement). Once 
the points of elastic/plastic interface have been determined, they are 
transferred into a worksheet program for data organization. We used Microsoft 
EXCEL to chart the data from the program. These charts are used for the 
engineer's analysis. An example is shown in Figure 4. 

RESULTS AND CONCLUSIONS 

The program has been used successfully be Warheads branch engineers. It 
has calculated the elastic/plastic interfaces for seven different strain 
criteria at once. I was personally responsible for several modifications to 
the program to make it more efficient, and in the process, I learned a lot of 
FORTRAN. This was my first real exposure to the FORTRAN language. With my 


35-6 





mentor's help, I was able to learn FORTRAN fairly quickly, and know enough to 
assist him in designing this program analysis tool. I also learned debugging 
skills after each program modification. There were several problems with 
logic and syntax, but the problems were solved and I gained valuable knowledge 
that I will take with me into my college years and the rest of my life. 
ACKNOWLEDGMENTS 

Once again, I have enjoyed a totally unbelievable summer here at Wright 
Laboratory under Mr. Michael E. Nixon. I thank GOD for allowing me the 
abilities to be in such a wonderful program. My mentor's efforts are greatly 
appreciated. His patience and trust in my ability has helped me to learn on 
my own, make my own mistakes, and learn how to correct them. More than once I 
would come to him with a problem and he would trust me enough to simply say, 
■Well, fix it!" He is truly the standard by which all mentors should be 
measured! Also very important to the success 1 enjoyed cure Ms. Pam Cortner, 
Mr. Randy Anderson, and Mr. Bizhan Aref who are contractors with Sverdrup, 
assigned to the computational mechanics section. Dr. Bill Cook and Dr. 

Harbans Sidhu were excellent sources of any technical information when needed. 
And last of all, but most definitely not the least, is my "partner in crime", 
Ms. Jennifer R. Bautista, another second year apprentice at the lab. This was 
our second year working together, and once again, I enjoyed her company and 
unique insight. The entire office was very cordial and understanding. I only 
regret that this is my last year to be an apprentice. If I was given the 
opportunity to do it all again, I would not hesitate. 


35-7 






Figure 1 


NODE 



ELEMENT 


ELASne/PLASne INTERFACE 


^ R^Ro =CRmcAL STRAIN 

S\o 


35-8 





Figure 2 


"XYDATA," 

NODEOOOl 

226E-06 

-9.25E-06 

2.90E-06 

7.05E-04 

4.19E-06 

1.41E-03 

5.97E-06 

2.10E-03 

8.16E-06 

2.77E-03 

1.07E-05 

3.42E-03 

133E-05 

4.08E-03 

135E-05 

4.74E-03 

1.75E-05 

5.41E-03 

1.93E-05 

6.09E-03 

2.10E-05 

6.77E-03 

2J28E-05 

7.45E-03 

146E-05 

8.12E-03 

2.67E-05 

8.79E-03 

"XYDATA." 

NODE0002 

2.87E-05 

9.46E-03 

3.07E-05 

1.01E-02 

327E-05 

1.08E-02 

3.49E-05 

1.15E-02 

3.73E-05 

131E-02 

3.98E-05 

1.28E-02 

4.24E-05 

135E-02 

430E-05 

1.41E-02 

4.77E-05 

1.48E-02 

5.02E-05 

135E-02 

530E-05 

1.62E-02 

5.60E-05 

1.68E-02 

5.93E-05 

1.75E-02 

637E-05 

1.82E-02 

6.65E-05 

1.89E-02 

7.07E-05 

1.96E-02 

7.86E-05 

2.03E-02 

"XYDATA." 

NODE0003 

2.87E-05 

9.46E-03 

3.07E-05 

1.01E-02 

327E-05 

1.08E-02 

3.49E-05 

1.15E-02 

3.73E-05 

1.21E-02 

3.98E-05 

1.28E-02 

424E-05 

135E-02 

430E-05 

1.41E-02 

4.77E-05 

1.48E-02 

5.02E-05 

135E-02 

530E-05 

1.62E-02 

5.60E-05 

1.68E-02 

5.93E-05 

1.75E-02 

637E-05 

1.82E-02 

6.65E-05 

1.89E-02 

7.07E-05 

1.96E-02 

7.86E-05 

2.03E-02 


35-9 





Figure 3 



Put Data into 
Arrays 


Input 

Strain 

Values 









(HtoOLUSOd-Z 


Figure 4 


PLASTIC WAVE FRONT VS. TIME (ROD IMPACT VELOCITY - 200m/sec) 



35-11 








35-12 





Appendix A 


PROGRAM PLASTIC 
c 

character* 3 choice 
character* 10 dummy 1 
c 

integer count 1. count2 
c 

real dummy2 

real time(5000),xdata( 100,5000) ,zdata( 100,5000),strain( 100.5000) 
real scrit( 100) 
c 

open(unit=3Xile='plasucx.m',status='old) 
open(unit=4,file='plasticz.in'.status= l old') 
open(unit= 10,file='plasticz.dat',status='new') 
open(unit=95,file= , plasuc.out'^tatus='new') 
c 

count 1=1 
count2=0 
c 

c Designed by ELLIOT MOORE n and JENNIFER R. BAUTISTA under the 
c mentorship of MICHAEL E NIXON. Summer, 1993. High School 
c Apprenticeship Program (HSAP). 

c 

c This program reads POST2 ASCAQ files containing x-position vs time 
c (unit 3) and z-position vs time (unit-4) for nodes along the edge 
c of a cyclinder impacting an anvil. The input files (plasticx.in and 

c plasticz.in) are assumed to contain time vs x-position and time vs 
c z-position separated by headers labeled: 

c 

c XYDATA, NODExxxxx (or ELEMxxxxx) X-POSITION (or Z-POSITION) 
c 

c where xxxxx is the node or element number associated with the data 
c that follows, 

c 

c The program finds the time when the node position matches the 
c criteria (r-rO)/iO = a desired constant and writes the time vs 
c the location of the plastic interface in the form of the current 
c z-position and original z-position to unit 10, named plasticz.daL 
c It allows the number of strain criteria desired to be input from the 
c keyboard as well as whether or not unit-11 is created. Unit-11 

c contains node position and time vs strain, z-position, and 

c x-position. 

c 

c A summary of data available is output to the file plastic.out 

c 


35-13 






Appendix A (com.) 


cccc< 

c 

c 

XCCCCCCCCCCCCCCCCG 

VARIABLES: 


c 

c 

count1 

• number of nodes or elements 

c 

counl2 

- number of data points per node or element 

c 

num 

- number of strain values to be tested 

c 

scrit(k) 

- array of strain criteria values 

c 

choice 

- decision to output lull data information or not 

c 

dummy 1 

- a dummy variable used for counting 

c 

dummy2 

- a dummy variable used to determine z-data 

c 

time(i) 

- array of time data 

c 

xdata(ij) 

- array of x-position data 

c 

zdatafij) 

- array of z-position data 

c 

r 

strain(ij) 

- array of strain values 

ccccccccccccccccccccccaxcccccccccccxxccccmxcaxxxcccccccccccccccccccccccc 


OPTION OF HOW MANY STRAIN VALUES TO CALCULATE 

wrile(6,*)HOW MANY STRAIN VALUES7 
read(5,*)num 


c OPTION OF WHAT STRAIN VALUES TO USE 

c 

dok=ljium 

write(6,*) , INPUT STRAIN CRITERIA VALUE 
read(5,*)scrit(k) 
enddo 


c OPTION TO OUTPUT FORT. 11 IF THERE IS ONLY 1 STRAIN VALUE 
c 

if(num.gLl) goto 6 

5 write(6,ll) 
read(5,12)choice 

tffchoice.eq.'Y’.or.choice.eq.'y') goto 6 
iffchoice.eq.'N’.or.choice.eq.'n') goto 6 
write(6,13) 
goto 5 
c 

c FIND count2 FROM plasticxin 
c 

6 read(3,10)dummyl 

7 count2=count2+1 
read(3,10)dummyl 
if(dummyl.eq.’XYDATA') then 
count2=count2-l 

goto 8 
else 
goto 7 
endif 


35-14 



Appendix A (com.) 


FIND count 1 FROM piasticx.m 

count l=count 1+1 

do i=l,count2+l 

read(3,10.end=9)dummyl 

enddo 

goto 8 

PUT DATA IN ARRAYS time(j), xdata(i j), AND zdata(i j) 


write(95,14)count 1 ,count2 
rewind 3 
do i=l,countl 
read(4,10)diiinmyl 
read(3.10)duinmyl 
do j=l,couni2 

read(3,*)timeO)^data(io) 

read(4,*)dummy2,zdata(ij) 

enddo 

enddo 

ARRAYS ARE SET 

RO IS SET TO THE ORIGINAL X-POSITION. ALL NODES OR ELEMENTS ARE 
ASSUMED TO START AT THE SAME X-POSITION. 

STRAIN FOR EACH NODE OR ELEMENT IS CALCULATED FOR EACH TIME 

rO=xdata(l,l) 
do i=l,count 1 
do j=l,count2 

strain(i j)=((*data(i j) - rO)/rO) 
enddo 
enddo 

UNIT 10 IS CREATED CONTAINING THE TIME AND Z-POSITIONS WHERE EACH 
NODE OR ELEMENT GOI-S PLASTIC, AS WELL AS, THE ORIGINAL Z-POSITION. 

do k=ljium 

write(10*)'SCRIT = ',scrit(k) 
write(10,15) 
iflag=0 
do i=l,countl 
do j=l,count2-l 

if(strain(ij).ge^crit(k).and.iflag.eq.O) then 
i£lag=l 

write(10,*)tune(j),zdata(ij)^data(i,l) 

else 

continue 

endif 

enddo 

iflag=0 

enddo 

enddo 





Appendix A (corn.) 


c 

c CHECKS TO SEE IF UNIT 11 SHOULD BE CREATED 
c 

if(choice.eq.'y'.CM , .chok:e.eq.'y’) goto 900 
goto 999 
c 

c CREATES UNIT 11, CONTAINING TIME VS. STRAIN. Z-POSITION. AND 

c X-POSITION. 

c 

900 do i=l .count 1 

do j=2.counl2 

write( 1 l.*)i.ume(j).strain(i j)^data(i j)jtdata(i j) 
enddo 
enddo 
c 

c FORMAT STATEMENTS 

c 

10 fomuu(a6) 

11 format(TX) YOU WISH TO OUTPUT STRAIN DATA FOR ALL DATA POINTS? 
1 '<Y or N>') 

12 format(al) 

13 formatflNVALID ENTRY. RE-ENTER CHOICE.') 

14 foimatf’DATA FOR THE X AND Z POSITIONS CONTAIN’/ 

113,' NODES OR ELEMENTS WITHY 

215,' PER NODE OR ELEMENT.') 

15 fo^nat(5x,TIME(SEQ',6x,'Z-CURRENT,6x,’Z-ORIGINAL , ) 
c 

999 stop 

end 





35-17 





Appendix B 


AutoCAD 

I began my summer working with the AutoCAD program. AutoCAD is a 
general purpose Computer Aided Design program for preparing 2-D drawings and 
3-D models. We installed AutoCAD on a Bernoulli Disk, which acts like a 
portable hard drive. I spent about a week on the AutoCAD tutorial learning 
how to use some of the functions. To help hone my skills, my mentor, Mr. 
Nixon, had me draw a 3-D model of an Explosively Formed Penetrator, or EFP, 
shown in Figure 1-B. It took about a day because I wasn't yet proficient with 
the commands. I took it upon myself to create other tasks in AutoCAD 
modeling. Over the course of one week, I used some of my spare time to create 
my own version of the U.S.S. Enterprise, shown in Figure 2-B. 

AutoCAD has the ability to save its images to an IGES-(Initial Graphics 
Exchange Specification} file. IGES files contain the geometry of the AutoCAD 
drawings and models. It is possible to take these images, and using a special 
device, input them into PATRAN for simulation. PATRAN is a post processor 
program used partially for graphic representation of EPIC computations. 
Unfortunately, the device is not yet available locally. 


35-18 




























EFFECTS OF THE ATMOSPHERE ON LASER RADAR 


Alexander H. Penn 
Assistant Laser Radar Technician 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratories 
WL/MNGS 
Eglin AF Base 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington. DC. 


August 1993 


36-1 



EFFECTS OF THE ATMOSPHERE ON LASER RADAR 


Alexander H. Penn 
Summer Apprentice 

Abstract 

A system for measuring the effects of the atmosphere on LADAR was set up and data collection 
was begun. It consisted of three parts: a LADAR to take images of a fixed stationary object, a weather 
station to measure existing weather conditions, and a Helium Neon laser to measure the attenuation of the 
beam due to atmosphere. The initial medium for the LADAR has been Nd: Yag, which lases at 1.06 
microns, but it will be replaced by a Ti-Sapphire laser of adjustable wavelength. Data collection to date has 
consisted of fair weather, rain, fog, and nighttime data. It will later cover other atmospheric conditions 
including snow, sleet, and high aerosol content (smoke and dust). The data when fully collected will be 
used to develop filter algorithms and to evaluate the most useful wavelengths under differing conditions. 


36-2 




EFFECTS OF THE ATMOSPHERE ON LASER RADAR 


Alexander H. Penn 

Introduction 

In recent years, LAser Detection and Ranging (or LADAR ) has become of increasing interest to 
the military and to the scientific community. Whether it will ever completely replace the microwave radar 
is difficult to say, but LADAR has already proved that it has many advantages over the traditional radar. 

The much smaller wavelength (over a thousand times) allows for a much higher accuracy and precision 
resolution. The highly collimated nature of laser beams also gives the LADAR a much smaller angular 
beam width. This characteristic provides the LADAR with that most important ability, imaging. Small 
angular beam width also allows aim point assessment, precise target tracking, and autonomous operation. 
The precision of the LADAR system’s range and velocity measurements also give it direct target size 
determination, segmentation of stationary and moving targets, and improved clutter rejection and target 
recognition. The downside of LADAR systems is that they are subject to varying weather and atmospheric 
conditions. This means that they are usually restricted to shorter ranges in the lower atmosphere in other 
than totally clear weather. 

Because of the limitations on tactical LADAR use, it was decided that the exact effect of the 
atmosphere on a LADAR image needed to be studied. It has become necessary to find a way of actually 
measuring the effects so that a way of counteracting them may be developed. The immediate task was to set 
up and then coordinate whatever systems were necessary to characterize atmospheric effects on LADAR. 
Data was then to be collected in such conditions as fog, heavy rainfall, high temperatures (with heat 
shimmers), and whatever else that might come along and prove to be interesting. These images would later 
be compared with that of "normal" conditions. 

Apparatus 

The first system set up was the LADAR itself. For the initial tests the laser medium being used 
was Nd: YAG (output at 1.06 microns in the near infra-red). This is the current standard medium for laser 
radar because of the atmospheric "window" at that wavelength. Except for the case of aerosols in the air, 
there will be little attenuation of the beam due to atmosphere and little or no scattering. Subsequent tests 
will be conducted using a Ti-Sapphire medium of adjustable wavelength so as to find the best possible 
wavelength for each weather condition. The system scans by means of two revolving mirrors. The 
receptor, located immediately beside the laser, receives the light reflected off the target and calculates the 
range based on the time of flight between the impulse and its reception. The raw data already shows up as 
an image often recognizable to the human eye. But for the computer to recognize it as a target, the image 
must first be run through filters to get rid of background "noise", the objects segmented out of the 


36-3 





terrain, and each object evaluated against a template to see if it matches a target's description. The data 
collected here will be run through a special algorithm to "score' how easily the computer is able to 
recognize the "target" (a large truck approximately 85 meters away), from the images taken under differing 
conditions. 

The second system used was the one actually able to measure those conditions. A Texas Weather 
Instruments weather station was employed to measure such important factors as temperature, humidity, and 
rate of rainfall at any given time. The information was recorded into computer via a communications port 
and translated into usable form by KEAterm (a simple communications program). The data is then 
imported into the Excel Spreadsheet and parsed. At this point the data may be used to match up an image to 
its weather conditions or graphed to study weather trends. Here is an example of humidity graphed against 
time: 



The last system used was a small Helium Neon laser (.6328 micron wavelength) in tandem with a 
voltage meter. The purpose of this system is to measure the transmissiveness of the atmosphere via 
attenuation of the beam. Attenuation is measured using Beer's Law : 


which states that attenuation is based on starting intensity, final intensity, and the distance involved. Using 
a power meter to measure the intensity and a retro-reflector to return the beam, it is possible to get a good 
idea of the amount of attenuation due to atmosphere that the LADAR beam undergoes in the same distance. 
This is because the atmosphere has much the same attenuating effect on light at .6328 microns as it does on 
light at 1.06 microns. Only in the cases when there are large amounts of aerosols present will there be a 
discrepancy. 

Problems Encountered 

The sheer number of problems to be faced during this project has been both 


36-4 





phenomenal and daunting. Delays sprang up one after the other. To begin with, no work could even begin 
until the weather station arrived. When it finally arrived, as it was being set up, it was discovered that one 
of the sensors was broken. It had to be sent in to be fixed, another delay. The first data collections were not 
until July 15, leaving less than 3 weeks to work on the rest of the project. Therefore, data collection began 
immediately, though the power meter for the HeNe laser had not arrived yet. This turned out to be a very 
wise choice because to date it still has not come in. The rest of the HeNe system is set up, but the power 
meter is a crucial piece without which the rest is useless. The weather system is also not running perfectly, 
because around ten and eleven ‘o clock the sun sends waves of heat off the concrete to the outdoor sensor 
located under the shade of some eaves. So during this portion of the day the sensor reports erroneously 
high temperatures and. because it is based on temperature, incorrect humidity. It was decided the problem 
would best be solved by putting the sensor in a sort of birdhouse-like shelter which would be placed away 
from the building. The shelter has not arrived yet but this does not prevent data collection during other 
parts of the day or during storms. 

As if this weren't enough, the LADAR itself is having problems. To begin with the laser is not a 
very good one. We have been getting a lot of background "noise". This simply means getting false returns. 
Normally a little noise is to be expected, but we've been getting a lot more than a little. After reviewing all 
the data collection to date a few observations and a tentative hypothesis to explain at least some of the noise 
were reached. First, during the day it seems as if the image gets worse and worse the longer the LADAR is 
left on. This is exactly opposite from the way it should be. Normally about 15 minutes should be allowed 
for the LADAR's diode pump to warm up, and at the beginning of the summer this was basically true. Now 
however, the best images are at around ten minutes and things go downhill from there. Secondly, data taken 
early in the morning, say before and around dawn, seems unaffected oy this rule. It does like it is supposed 
to do, giving the best images at around fifteen minutes and generally staying good after that. The worst 
images in the morning at 100% humidity are better than the best images in tne afternoon at 50% humidity. 
Lastly, during the afternoon imaging, the noise becomes more and more concentrated at just one range, a 
spot between one and two meters from the LADAR. From these three observations, some conclusions were 
drawn and a guess hazarded as to the cause. First, for the worst of the noise it is safe to assume that the 
cause of the noise is not directly related to outside conditions because all the noise is occurring at one 
particular distance, which is still in the shelter of the building! This means that the problem is inherent to 
the system itself. Second, the problem is not likely in the power source because the exact same thing 
happened when the LADAR was hooked up to the generator (used during storms so as to prevent loss of 
power to the LADAR). Third, though by no means is the data dismissed, the fact that better images are 
gotten in higher humidities (early morning data) goes against common sense and the literary research that 
was done, thus leading to the believe 

that the problem does not concern that variable but the other changing variable in the situation 


36-5 






described, temperature. The hypothesis is that the problem is some how tied up with the temperature of the 
system. This would explain why the images are worse in the afternoon and why the longer the LADAR is 
left on, the worse the images become. 

As serious as that is, it isn t even the only problem with the LADAR. It is also exhibiting periodic 
line shifts in which a whole or part of a line is shifted to the right (which is directly related to the fact that 
the LADAR scans from right to left). This seems to be a purely electric problem and is being worked on by 
a hardwares expert. There is also a small, much less serious problem with the scanning system itself. The 
beginning of the scan (and thus the top of the picture) is not always starting at its defauit position thus 
shifting the entire image either up or down. 

Conclusions 

Despite all these problems which are clouding the images it has become clear that the actual effects 
of the atmosphere at this range are almost nil. It will require a greater distance than 85 meters for there to 
be any measurable effect. Therefore the lab is trying to obtain a tank which will be placed much farther 
down the range in hopes that an increased distance wilt increase results. 

Futur e W Qrk 

The data, once it is all collected will be used for a variety of purposes. First, as it is the experiment 
being conducted here, the data will be used in the weather vs. wavelength study. Secondly, it will be used 
for the possible generation of special filters to remove the effects of any given weather condition from a 
LADAR image. Thirdly, it will be used to test the effectiveness of existing algorithms for target detection 
and identification. Finally it will be used to compare against synthetic LADAR data to check for accuracy. 
Though all of this is in the future, hopefully by next year there will be much more to show. 

Rcferenc.es 

Jelalian, Albert V.. Laser Radar Systems . Artech House; Boston; 1992 


36-6 




AIR POLLUTION DETECTION: 
A DUAL USE FOR LADAR 


Kyle D. Perry 
High School Apprentice 
Crestview High School 


Crestview High School 
1304 Ferdon Blvd. 
Crestview, FL 32536 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratory 


37-1 





Air Pollutant Detection: A Dual Use for LADAR 

Kyle D. Perry 
High School Apprentice 
Crestview High School 

Abstract 

Research was done to find laser line absorbance data on various airborne 
pollutants in order to determine if it is possible to detect these airborne pollutants in 
smokestack plumes using infrared spectrometry in the 0.79 to 2.0 micron region. A 
computer program containing this data was eventually found and various parameters in the 
program were manipulated in order to fit the conditions that LADAR would be used to 
detect polluting emissions contained in the smokestack plume. It was found that a few 
molecules contained significant absorbance peaks in the 0.79 to 2.0 micron range, namely 
HCL, OH, HF, and CH 4 . 


37-2 




Air Pollution Detection: A Dual Use for LADAR 
Kyle D. Perry 

Introduction 

With the encouragement of finding peace time uses for weapons technology, the Air 
Force and Wright Laboratories has a growing need to find a use for Laser Imaging Systems 
not only as a tool in combat but as a tool to directly benefit the public. One way to 
accomplish this is to utilize the lasers normally used for imaging enemy tanks and weapons to 
also identify airborne pollutants emitted from factory plumes. Using these laser systems, the 
government would be able to fly over or around these smoke stack plumes and identify the 
molecular composition of the emissions just by activating a laser and assessing the 
atmospheric absorption. The absorption values of the plume emissions could then be taken 
at certain frequencies and compared to the known absorption values of airborne molecules at 
the same frequencies and thus determine if the plume contains pollutants. There are some 
obstacles, however, before this ideal scenario can be enacted. The objectives given deal with 
some of these obstacles. 

One of the major problems in developing a second application is the wavelength of the 
current lasers being used. The lasers currently used for missile guidance are 
Neodymium: YAG, Helium- Neon, and semiconductor diode which operate in the 0.79-2.0 
micron range. Most of the research in the past was with the use of LADAR to identify 
airborne pollutants was done with C0 2 lasers which operate in the 9-11 micron range. 
Another objective given was to find data containing peak absorption rates that coincided 
with the frequencies in the range of the lasers that the Air Force uses. From this data a 
spreadsheet was to be composed presenting the information easily and concisely so that the 
data collected on these pollutant compounds may be compared with the results from laser 
spectrometry tests conducted on smokestack plume emissions. Hopefully, the progress made 
in this stage of the project may serve as a foundation for actual experimentation of an 
atmospheric pollutant detection system in later stages. 


37-3 




Background 

The earlier research of this project was done to find any information on design and 
charts on previous pollutant identification systems. The problem with this research was that 
most information found on the application of LADAR for pollutant detection was found with 
a C0 2 laser which operates in the 9-11 micron range. The purpose of this project was to find 
the absorption peak values for various airborne pollutants in the 0.79 to 2.0 range. 

Another problem with conducting this research was the need to manipulate the 
absorption data found due to the conditions plume would be analyzed. In this application of 
LADAR, the laser will be activated in a plane or helicopter approximately 500 meters 
from the smokestack plume with the plume width approximately 20 meters. These conditions 
yield 3 parts of absorption for the laser pa:h. The first is the absorption the laser beam will 
encounter while traveling approximately 500 meters to the plume. The second is the 
absorption the laser beam will encounter while traveling ten meters through and ten meters 
back through the plume. The third is the absorption encountered while the laser beam is 
traveling 500 meters back to the detector that is adjacent to the source. All these parts must 
be taken into account in two separate calculations. The first is the absorption of a laser beam 
at a certain frequency with a path length of a 1000 meters through normal atmospheric 
molecular composition. The second is the absorption of the laser beam through 20 meters of 
the compound with a concentration of. 1%. Both of these problems were easily solved 
through the use of a spectrometry database containing absorption information in the 0.79 to 
2.0 micron range. This program also had the ability to modify data to suit the given test 
conditions. 

A chart was composed containing the different types of units used to describe the 
wavelength/frequency of an electromagnetic wave. The greatest use of this chart was for the 
conversion between wavelength (which is used by laser catalogs) and wave numbei (which is 
used by spectrometrists). The conversion formula used (with y as the wavelength): wave 
number* My*\0 A . 


37-4 





Pr oblem 

The main objective of this project was to locate laser coincident absorption bands of 
pollutant compounds in the 0.79 to 2.0 micron range for identification in smokestack plumes 
by means of remote sensing. Identification of the pollutant would be done by comparing the 
intensity of a laser aimed through a smokestack plume and comparing it to the intensity when 
aimed through clean air. The presence of a pollutant would then be determined by the 
increase in absorption. 

Methodology 

Once the HitranPC database was received, a overall sweep of the 0.79-2.0 micron 
absorption spectrum was done using the TRANS part of the HitranPC program to find any 
pollutant compounds that could be detected in that wavelength range (graph 1). The 
following compounds had absorption peaks in the 0.79-2.0 micron range :OH, N 2 0, HBR, 
HCL, CH+, CO, HI, HF. After these candidates for detection were found the laser path 
length in the Hitran PC program was reduced from 1000m to 20m to take into account only 
the stack plume. The concentration of the pollutant to be tested in the plume was increased 
from its normal atmospheric proportion to . 1% and a graph was made (graph 2). Molecule 
absorption peaks that still contained significant values above non-pollutant molecules for 
means of identification were narrowed down into separate files in order to be transferred to 
the spectra program. These files account for the amount of absorption in the 20 meter plume 
but duplicate graphs containing the same range and normal molecular proportions had to be 
made to take into account the 1000 meters that the laser beam travels through normal air to 
get to and back from the plume (graph 3). 

These pairs of files were added to form a composite absorption line to give an 
approximate value of the absorption that the laser beam would encounter from a plane 
through the plume containing pollutant emissions (graph 4). A percentage was taken of the 
absorption of the pollutant peak to the composite absorption value at the same frequency. If 
the pollutant absorption accounts for 10% of the composite absorption then it was 


37-5 






'determined that the LADAR system could detect the pollutant (chart 1). Laser wavelengths 
of Diode, Neodymium. YAG, and Helium-Neon lasers were then found to see if any coincided 
with the absorption peaks of detectable pollutants (chart 2). The peaks of detection were 
then divided into two categories. The first category was a list of compound absorption peaks 
that were detectable by LADAR with its present wavelength capabilities. The second 
category consisted of the set of compound absorption wavelengths that did not coincide with 
the current LADAR wavelength capabilities but were still detectable in the 0.79 to 2.0 micron 
range. 

Results 

Graphs 5,6,7 and 8 show the absorption values for pollutant compounds that LADAR is 
capable of detecting with its present wavelength capabilities. The absorption peak data is 
displayed by a set of two graphs. The first graph displays the absorption of the pollutant and 
the absorption of the atmosphere in separate lines. The second graph shows the composite 
graph from the two sets of data. Both graph lines are composed on wavelength versus 
absorption. The wavelength the peak occurs is shown on the x axis in microns and the 
absorption value of the peak is shown on the y axis as a percentage. Graphs 5-8 show the 
absorption lines for the following pollutants, respectively; HF (1.270-1.286 microns), OH 
(1.450-1.453 microns), OH (1.479-1.483 microns), HF (1.33-1.30 microns). 

Another set of graphs was composed to show absorption lines that contained significant 
absorption peaks in the 0.79-2 micron range but were not detectable by LADAR. Graphs 9- 
11 shows the absorption lines for the following pollutants, respectively: HF (1.2594-1.3 
microns), CH 4 (1.641-1.706 microns), HCL (1.1.863-1.714 microns). 

From the absorption data, a chart was composed listing the exact wavelength an 
absorption peak occurred, its absorption value, the component absorption value at the same 
frequency as the pollutant absorption peak, and a percentage value of the pollutant 
absorption to component absorption. 

If the pollutant absorption accounted for more than ten percent of the composite 


37-6 






absorption then the compound containing the absorption peak was considered detectable by 
LADAR (chart 1). From the research and calculation performed, the following pollutant 
compounds were found detectable by LADAR with its present wavelength capabilities. OH, 
HF. CH 4 and HCL are also detectable in the 0.79 to 2.0 micron range, but LADAR presently 
lacks the wavelength capabilities to detect these compounds. 


37-7 




I would like to thank the following for their help in making this paper happen. First I'd 
like to thank my family for their love and support. I'd also like to thank my fellow 
apprentices Barry, Chris, Christie, Jenny, Mary, Nancy, Melissa, Jon, Laura, Elliot, Alex, 
Mark, Darcie, and Deanna. I'd like to thank some of my non-apprentice friends. . . 

Lila (Beth) Harbour, Harbour Family, Robert Johnson, Aaron Green, Mr. Shraeder, 
Mike Reavey, Charlene Dubois, Lynda and Andy Hart, Laura Northington, Meade Family, 
Jenny Jones, and Bryun Souther. 

I'd also like to thank Don Harrison for all the work and time that he has put into the 
HSAP Program. Most of all I would like to thank my mentor, Lynn Deibler, for his guidance, 
and friendship to make this paper possible. 


37-8 








Bibliography 

Air Pollution. Part A: analysis, Ledbetter, Joe 0.,Marcel Debberdoe, NY 1972 

Fundamentals of air pollution, Williamson, Samuel J., Addison Wesley, MA, 1973 

IR; Theory and practice of infrared spectroscopy, Alpert, Nelson L,Plenum Press, NY, 
1970 

Methods of Air Sampling and Analysis by Inter society Committee, American Public Health 
Association, Washington D.C., 1972 

Pollutant Detection by Absorption using MIE scattering and Topographic Targets as 
Retroflectors, Byer, Robert L., Garbuny, Max, Applied Optics, Vol 12,7,73 

Infrared Remote Sensing and Determination of pollutants in G plumes, Prengle, William H., 
Morgan, Charles A., Huang, Cheng- Shen, Camani, Paola, Wu, William W., Vol 7,5,73 

Mobile Remote Sensing System for Atmospheric Monitoring, Edner, Hans, Fredrikkson, 
Kent, Sunesson, Anders, Svanberg, Sune, Applied Optics, 26, 19, 87 

Mobile Atmospheric Profiling System, White, Kenneth O., Guttman, William N., Dowling, 
James A., Applied Optics, 24,21,85 

HitranPc Database(2.0) , University of South Florida, 91 


37-9 






uofss;Mfu«ax 






















































POLLUTANT ABSORPTION 

OF Ladar 


O ^rt-aOCONtOflOOiCOCNiOCOr-S^ 

Wr w • t « • 

SOOaM(MaO) 0 ) 0 )(OCM(OOIONO) 

Ujf^a)NNS0 O)O)O)O)O)(J)©O)O) 

a 


oc oo ^ 

«-OOT--''^tT-OCD^fin--CMIO 

a ■ • 'Ni W " •■•••• w 1^ • 

OOO«NrOOOrtC0(DrrCMO 


j ^^^^ooooooo 0 ? 0 ? 0 ? 

J 0)0 0 ) 0 )«^ 00000000 ) 0 ) 0 ) 
00)0)0>aWWr-ri-rrr-r-0)0)0) 

Q. 


i®waaoacMNt-i-^cMswww 

mWaoiroasaaaa^oi-cM^ 

raNNaaNaasNaaacMr-o 

^CMCMCM^f^^OICMCMCMCMCMCMCOCOCO 


^ILU.U.IIIlLlLU.U.lLU.U.lLU.LL 

qZZIOOOZZIZZIIZZI 


37-12 


Chart 1 






Useful Lasers 


frequency 

laser 

1.15 

he-ne 

1.152 

he-ne 

1.153 

he-ne 

1.523 

he-ne 

1.532 

he-ne 

79-.81 

diode 

1.06 

diode 

1.2-1.3 

diode 

1.27-1.33 

diode 

1.3 

diode 

1.45 

diode 

1.48 

diode 

1.5 

diode 

1.51 

diode 

1.52-1.58 

diode 

1.55 

diode 

1.06 

neodynium 

1.061 

neodynium 

1.064 

neodynium 

1.064-1.319 

neodynium 

1.3 

neodynium 

1.32 

neodynium 


Chart 2 


useful for detection? 
no 
no 
no 
no 
no 
no 
no 
yes 
yes 
no 
no 
yes 
no 
no 
no 
no 
no 
no 
no 
yes 
no 
no 


37-13 







<18^ 2 y.> _(Input Spectra) 












<Input Spectra) 



























































































































































































Environmental Movement of Heavy Metals in Soils and Vegetation 


Mary F. Pletcher 
High School Apprentice 
Environics Branch 


Wright Laboratory Armament Directorate 
WL/MNOE 

Eglin AFB, FL 32542-5434 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored by : 

Air Force Office of Scientific Research 
Boiling Air Force Base, Washington DC. 


August 1993 


38-1 




En : ronmental Movement of Heavy Metals in Vegetation and Soils 


Mary F. Ptetcher 
High School Apprentice 
Environics Branch 
WL/MNOE 

Abstract 

Currentiy tantalum (Ta) and tungsten (W) are used in the construction of penetrators for warheads. 
While the two elements have not been proven to be toxic to humans and animals, the ability of the 
Hcmcnts to enter the food chain is unknown. Thus, this study tested tantalum, tungsten, and a tungsten 
alloy's abilities to enter the food chain. The project consisted primarily of a plant uptake study, however, a 
leaching study was also started. 

The study consisted of planting two species of plants; Zea mays 'Hunters Choice’ (corn) and Phaseolus 
vulgaris ‘Blue Lake' (bean), into soils containing different exposures of tantalum, tungsten and the alloy. 
Hants were grown in a growth chamber insuring uniform conditions. Results from this pro .nary study 
suggest that there will be a minimal plant uptake. 


38-2 




Contents 


Introduction.4 

Background..4 

Procedure and Results.6 

Miscellaneous.13 

Acknowledgements..13 

References.14 


38-3 










Environmental Movement of Heavy Metals in Vegetation and Soils 


Mary F. Pletcher 


Introduction 

The uptake of heavy metals used in the construction of various military weapons into the food chain 
has been the subject of previous studies. However, two heavy metals, tantalum (Ta) and tungsten (W), 
have not been studied due to the only recent increase of the metals use for military weapons. Currently, 
tantalum and tungsten are used in the construction of penetrators of warheads as an alternative to depleted 
uranium, which has some undesirable effects associated with its use. Laboratory studies have shown that 
the elements are not toxic to humans, however, no studies have been conducted to determine the elements' 
ability to enter the food chain. A secondary study, leaching ability, was also started though inadequate 
time did not allow for completion of the study. 

Background 

Tantalum Tantalum is one of the most inert of all metals, not reacting with chemicals below 150 
degrees Centigrade. Tantalum’s inertness is caused by a dielectric oxide film. Medically, tantalum gauze 
is used for surgical repairs such as closing skill defects and tantalum powder is used for the visualization 
of lung morphology and angiography. The current OSHA (Occupational Safety and Health 
Administration) permissible exposure limit, also known as PEL, is 5 milligrams per cubic meter (source 
1 ). 

Tungsten Tungsten is a gray hard metal with many industrial uses due to its high melting point and 
strength. For a long time, tungsten was used primarily used as wire for incandescent lamp filaments and 
for electric vacuum devices (source 2). Early toxicity studies of tungsten and its compounds show a 
difference between the metal's soluble and insoluble compounds. Soluble compounds have a PEL of 1 mg 
per cubic meter versus that of the insolubles which have PEL'S of 5 milligrams per cubic meter (source 1). 


38-4 






Literature Review Technical reports about the plant uptake of heavy metals excluding tantalum and 
tungsten have been published. In Trace and Toxic Metal Uptake bv Marsh Plants as Affected bv Eh. pH 
and Salinity , a study was conducted to determine whether pH, redox potential, and salinity affected trace 
and toxic metal availability to marsh plants. The metals studied were mercury, lead, cadmium, copper, 
iron and manganese The results of this study showed that the uptake of mercury is enhanced by decreases 
in salinity and an increase in pH. Lead uptake was increased in more acidic soil. Cadmium uptake was 
also increased in high oxidation conditions and acidic soils. Zinc uptake was increased under higher 
oxidation conditions. Copper uptake was unaffected. The study concluded that pH, redox potential, and 
salinity did affect uptake (source 3). 

In A Hydroponic Study of Heavy Metal Uptake bv Selected Marsh Plants, eight marsh plants were 
grown in chemically concentrated hydroponic solutions containing three concentrations of heavy metals 
(0.0.0.5, and 1 parts per million) to evaluate the ability of each plant specie to uptake and accumulate 
heavy metals. The heavy metals used in the study were zinc, cadmium, nickel, lead and chromium. The 
study concluded that specific plant species had higher uptake of heavy metals. However, lead and 
chromium accumulated in the roots of all the species with little translocation into plant tops. The results 
also indicated that phosphorous and iron content in the roots appeared to be a major factor in determining 
the ability of the marsh plants to translocate heavy metals from the roots into other plant parts (source 4). 

In Influence of Disposal Environment on Availability and Plant Uptake of Heavy Metals on Pledged 
Material, the heavy metal uptake by Cyperus esculentus from fifteen highly contaminated freshwater 
water sediments under reduced (flooded) and oxidized (upland) disposal conditions was studied. Heavy 
metal uptake by Spartina altemiflora and Distichlis spicata from fourteen highly contaminated saltwater 
sediments under flooded disposal conditions was also investigated. The plants were analyzed for the heavy 
metals zinc, cadmium, copper, iron, manganese, arsenic, mercury, nickel, chromium, ami lead. Plant 
uptake of heavy metals, especially zinc and cadmium, was shown to be greater in plants grown in upland 
sediments compared to with flooded sediments. The study thus concluded that the plant uptake of heavy 
metals was site specific (source 5). 


38-5 





In Ability of Sait Marshes to Remove Nutrients and Heavy Metals From Dredged Material Disposal 
Area Effluents, experimental raceways were constructed in a salt marsh adjacent to a dredged material 
confinement area. The .search progran was designed to determine the ability of the salt marsh systems 
to remove nitrogen, phosphorous, iron, manganese, cadmium, copper, nickel and zinc from the effluent. 
During the study all contaminants were found to decrease in the effluent as it passed through the 
experimental raceways. Mean metal removal efficiencies ranged between fifteen and thirty-two 
percent.Inorganic chemical and physical processes probably account for much of the removal (source 6). 

Apparatus 

Growth Chamber All plants for the study were grown in a growth chamber measuring 3'x2'x2 , 9,’' in 
order to maintain uniform growing conditions. Lighting, temperature, and humidity could be controlled. 
Lighting, incandescent and full florescent, was on a twelve hour cycle. The temperature was set on 84.5 
degrees Fahrenheit/ 29.5 degrees Centigrade. The relative humidity within the chamber was 60%. 

ICP In order to detect trace amounts of elements in the ash; the ICP/6500 Inductively Coupled 
Plasma System for Atomic Spectroscopy by Perkin-Elmer was used. The system consists of a radio- 
frequency (RF) power supply and plasma torch housing, a Model 5000 Spectrophotometer and a Perkin- 
Elmer 7000 Professional Computer. The system can identify elements in a sample due to the wavelengths 
generated (characteristic property). However, standard solutions containing elements of interest are 
needed. Using the standard, the user calibrates the ICP to read a peak of the standard's wavelengths. The 
ICP then uses the calibrated peak to determine the quantity of the element of interest in a sample. Results 
are reported in parts per million (ppm) (source 7). 

Procedure and Results 

Plant Uptake Study The plant uptake study consisted of planting two species of plants; Zea mays 
‘Hunters Choice’ (corn) and Phaseolous vulgaris 'Blue Lake' (bean), into soils containing different 
exposures of tantalum, tungsten and a tungsten alloy. The alloy was 4.04% iron, 4.36% nickel, 2.56% 
copper, and 89.04% tungsten. The exposure ranged from 25 to 800 parts per million. 

The soil used in the study came from Range 52-North located approximately 17 miles northeast of 
Eglin Main, because this area is known to have had no heavy metal contamination from past test and 
evaluation programs. The soil was placed into plastic bags and brought back to the chemistry lab where it 
was then seived. 350 grams of soil was then measured as was the appropriate amounts of tantalum, 
tungsten or the alloy. 


38-6 











|PPM In Terms of Grams | 



ppm 

grams per 350 grams of soil 

25 

0.00875g 

50 

0.0175g 

100 

0.03 5g 

200 

0.070g 

400 

0.140g 

800 

0.280g 


Figure 1- Conversion of ppm to grams 


The plastic containers containing the soil and metal were then spun on a Ball Jar Roller Mill for ten 
minutes to insure that the metal and soil was homogenized. 

After the soil and metal was mixed, the mixture was placed in a plastic pot. Then two seeds, corn or 
bean, were planted approximately one inch deep in the pot. There were four pots of each tantalum and 
tungsten exposure levels of 100, 200, 400, and 800 parts per million. There were two pots of each 
tungsten exposure level of twenty-five and fifty parts per million and of each exposure level of the 
tungsten alloy. Initial planting in the tantalum and tungsten soils was on June 10,1993. Planting in the 
alloy soil was done a week later on June 17, 1993 due to the delayed arrival of the alloyed materia). 

The plants were grown for a little over a month in the growth chamber. Watering was done 
approximately every other day with a nutrient solution which was 3% Nitrate Nitrogen, 5% Ammoniacal 
Nitrogen, 30% Phosphoric Acid, and 20% Soluble Potash. There were a few problems in the study; by 
June 22, 1993, eleven of the initial forty four plants had not germinated, thus replanting was necessary. 
Fortunately, the majority of these did germinate. 


38-7 




















Figure 2- Germination dates of plants 


38-8 

















































































































































































bean 

tungsten 

800 ppm 

1 

I5-Jun 

bean 

tungsten 

800 ppm 

2 

15-Jun 

com 

alloy 

100 ppm 

1 

20-Jun 

bean 

alloy 

100 ppm 

1 

23-Jun 

com 

alloy 

200 ppm 

1 

20-Jun 

bean 

alloy 

200 ppm 

1 

22-Jun 

com 

alloy 

400 ppm 

1 

20-Jun 

bean 

alloy 

400 ppm 

1 

22-Jun 

com 

alloy 

800 ppm 

1 

20-Jun 

bean 

alloy 

800 ppm 

1 

no germination 


Figure 2- Germination dates of plants (continued) 


Additionally, when I returned from a week long trip, I discovered that over half of my plants had died 
(Figure 3). The majority of these were com plants, including the controls. Apparently the com plants were 
not able to grow the root structures they needed to survive. My mentor and I decided to proceed, analyzing 
the remaining plants. 


38-9 
















































Figure 3- Dead plants on July 12,1993 





































































































On July 21, 1993, the plants were removed from the soil so that 1 could begin processing for 1CP 
analysis. First I rinsed the roots of the plants in distilled water to wash off the remaining soil. I then dried 
the plants in the growth chamber. Second. I ashed the samples at 1200 degrees Fahrenheit in a Muffle 
Furnace to eliminate the organics. After ashing the plants, I placed the remains in a plastic specimen cup. 
I then poured 23 milliliters of Nitric Acid and 23 ml of distilled water into the cups so that the inorganics 
could digest for the ICP analysis. The acid, water and ash solution was then filtered. ICP analysis was 
then performed. 


38-11 





Figure 4- ICP analysis results 

The results of the test were very good. Based upon this study there was no plant uptake of tantalum or 
tungsten. In terms of the alloy, only one plant exposed to the alloy , the bean at 100 parts per million, had 
any uptake and this uptake was only four parts per million. These results suggest that there will be a 
minimal of uptake of these metals. However, this is only a preliminary study dealing with only two kinds 
of plants; monochodoleans (Zea mays 'Hunters Choice’) and dichodoleans (Phaseoulous vulgaris 'Blue 
Lake'). Other plants could uptake the metals in higher concentrations. 

Leaching Study 

I was only able to begin the leaching study due to inadequate time however, my mentor will continue 
it The leaching study consists of 6 leaching columns filled with the top four layers of soil from random 
sites of range 52-North. Each layer is six inches deep. The columns have 12.56 grams of various metals 
accross the top. Two columns contain tungsten, two tantalum, one copper, and one nickel. The columns 


38-12 
















































































will be exposed to a simulated yearly rainfall to see how much metal will leach. The runoff will be 
collected via tubes connected to the columns. The runoff will then be analyzed using the ICP. 


Miscellaneous 

Besides completing my plant uptake study I also accomplished many other things. I was responsible for 
the chemical inventory of the MicroAnalyis Laboratory. I participated in the cleanup and organization of 
the Chemistry Laboratory. I also learned many different computer operations including Microsoft Word, 
Excel, Charisma and the VAX and other instruments within the chemistry laboratory. And perhaps most 
importantly I was able to gain valuable experience to help me to determine future plans such as my college 
major. 


Acknowledgments 

Without the support of many individuals this summer would not have been as rewarding as it has been. 
I would first like to acknowledge my mentor, Lt. Brad Noland. I really could not have asked for a better 
mentor. Secondly I would like to thank the Eglin HSAP coordinators, Mr. Don Harrison who was also my 
Branch Chief, Mr. Mike Deiler, and Mrs. Glenda Apel. These individuals truly make the program 
successful, without their time and energy there would not be a HSAP. I would also like to acknowledge 
Mr. Ric Crews and Mr. Luis Santana and the rest of the Eimronics Branch. Everyone was always willing 
to answer my questions, take me to the photolab and range and make sure I felt at home. Finally I would 
like to thank Miss Nancy Deibler. Miss Melissa Griffiths and all of my fellow HSAPs. They all made a 
rewarding educational program into a lot of fun. 


38-13 


References 

1. Harvey, Gregory J: Certified Industrial Hygienist Biochemical Toxicology Branch. "Memo to MNOE 
on Tantalum/Tungsten Toxicity". 12 May 1992 

2. Drobvsheva. Ye. K.. Pavlov. I.M., Ushakov, Ye. V. Cold Brittleness and Structure of Tungsten . U.S. 
Army Foreign Science and Technology Center; Charlottesville. Virginia: 19 February 1987. 4. 

3. Center for Wetland Resources. Trace and Toxic Metal Uptake By Marsh Plants as Affected By Eh. pH. 
and Salinity . Waterways Experiment Station-Corps of Engineers; Vicksburg, Mississippi: December 1977. 

4. Landin, Maty C., Lee, Charles H., Sturgis, Thomas C.. A Hydroponic Study of Heavy Metal Uptake bv 
Selected Marsh Plant Species . Waterways Experiment Station-Corps of Engineers; Vicksburg, 

Mississippi: September 1975. 

5. Folsom. Bobby L. Jr.. Lee, Charles R., Bates, Derrick J. Influence of Disposal Environment on 
Availahilitv and Plant Uptake of Heavy Metals in Dredged Material . Waterways Experiment Station; 
Vicksburg, Mississippi: December 1981. 

6. Windom, Herbert L. Ability of Salt Marshes to Remove Nutrients and Heavy Metals From Dredged 
Material Disposal Area Effluents . Waterways Experiment Station-Corps of Engineers: December 1977. 

7. "Principles of Operation”. Perkin-Elmer ICP/6500 Inductively Coupled Plasma System for Atomic 
Spectroscopy Handbook. 


38-14 





SOLUBILITY AND RECRYSTALLIZATION OF 
l ,3,3-TRINITRO AZETIDINE (TNAZ) 


DAVID A. ROSENBAUM 

FINAL REPORT 

HIGH SCHOOL APPRENTICESHIP PROGRAM 

WRIGHT LABORATORY 

HIGH EXPLOSIVES RESEARCH AND DEVELOPMENT(HERD) FACILITY 

WL/MNME 

SPONSORED BY: 

RESEARCH AND DEVELOPMENT LABORATORIES 

CULVER CITY, CA 

AUGUST 18, 1993 


39-1 





TABLE OF CONTENTS 


I. Abstract 

II. Background 

in. Procedures and Results 

IV. Conclusion 

V. Miscellaneous 

VI. References 


39-2 





SOLUBILITY AND RECRYSTALLIZATION OF 
1,3,3-TRINITROAZETIDINE (TNAZ) 

DAVID A. ROSENBAUM 
HIGH SCHOOL APPRENTICESHIP PROGRAM 
ENERGETIC MATERIALS BRANCH 
WRIGHT LAB ARMAMENT DIRECTORATE 
SECTION 1 

1,3,3-trinitroazetidine (TNAZ) is a heterocyclic compound (energetic material) under 
investigation as a melt cast base for composite high explosive applications. TNAZ's high 
density, thermal stability, and enhanced performance over other military formulations 
make it suitable for high energy applications. This ability to melt cast and provide higher 
energy output combined with less sensitivity, makes TNAZ attractive as a military 
explosive. Thirty four pounds (15.5 kilograms) of TNAZ was procured by the High 
Explosives Research and Development (HERD) Facility in a joint action with the Army 
Research, Development, and Engineering Center (ARDEC) through a commercial best 
effort contract with Aerojet Ordinance. The TNAZ was contaminated with nitric acid. 
Explosive charges fabricated under hydraulic pressing at 30,000 psi, exuded nitric acid 
contaminating the die, ram, and press. This necessitated the removal of the nitric acid. 
Solvent recrystallization was chosen as the best method of purification. Solubility tests 
were done to determine the most efficient solvent. The solvents consisted of alcohols, 
water, and acetone. The TNAZ was recrystallized from ethanol by crash precipitation in 
distilled ice water. Drophammer was conducted to determine the impact sensitivity. 
Thermal analysis was completed to determine the melt point and decomposition 
exotherm. The structure of TNAZ was verified by NMR and FTIR, its morphology 
studied by Scanning Electron Microscopy, and its particle size determined. The 15.5 
kilograms of TNAZ was successfully recrystallized in two batches removing all 
detectable traces of nitric acid contamination. 


39-3 




BACKGROUND 


SECTION II 

The military is faced with many problems when developing explosives. They must 
take into account the explosives performance, insensitivity, cost and many other factors. 
The HERD and the military are addressing the problem of developing a melt cast base 
using heterocyclic materials for High Energy Composite Explosive Applications. The 
present melt cast binder commonly used with Cyclic Nitramines is TNT. TNT when 
combined with RDX, PETN, and HMX produce the composites Comp B, Pentolite, and 
Octol. Although TNT has a stable low temperature melt phase and good thermal stability 
it has a low density and low energy wnic h dilutes the Heterocyclic Nitramine. TNAZ not 
only has a stable low melt temperature kquid phase but also has high density and high 
energy. TNAZ when combined with PETN, RDX, and HMX produces a composite 
explosive which greatly exceeds Pentol te. Comp B, and Octol in energy output. 

Secondly when TNAZ is combined with insensitive particulate explosives such as NTO, 
NQ, CL-14, and ADNBF produces a composite explosive equivalent in energy to Octol, 
yet much less sensitive to initiation. Increased energy content of warheads in air-to-air 
and surface-to-aif missiles can greatly improve the lethality and effectiveness of those 
warheads. Heterocyclic materials possess high density, high energy, and thus high 
performance. One goal of the program addressed is to use 1,3,3-trinitroazetidine 
(TNAZ) as this high energy melt cast base. 

Heterocyclic materials are organic ring compounds containing atoms other than 
carbon in the skeletal ring. These materials are ideal for high energy insensitive 
applications. TNAZ is a heterocyclic compound consisting of a four-me r 'bered ring 
(Figure 1) containing one nitrogen in the ring. Because of it's 101 degree . Celsius melt 


39-4 




point. TNAZ is melt castable using existing steam jacketed melt kettles which usually 
operate at 80-110 degrees Celsius, depending upon the steam pressure. Two alkyl 
(geminal) nitro functional groups are attached to the number three carbon and one 
nitramine functional group is located in the number one position. It has the unique 
structure of containing both aliphatic nitro and nitramine functional groups (Ref. 1). 
TNAZ is synthesized by the condensation of t-butyl and epichlorohydrin (1) to form the 
azetidol (2). The azetidol is then reacted with methanesulfonyl chloride to form the 
mesylate (3). Displacement of the mesylate with sodium nitrate gave t-butyl-3- 
nitroazetidine (4) which is converted to t-butyl-3,3-dinitroazetidine (5) by oxidative 
nitrolysis. Nitrolysis of 5 with acetyl nitrate gave TNAZ (Ref. 2). 

THE PRESENT SYNTHESIS O' 7 ^NAZ 


o 

ZVa 

+ 

NH, 


H OH 


Sl«» I 


-<*> 


H OMs 


Stty 1 


* 


N-HC1 EtjN / McSO>a N 

t-Bn t-Bo 


Sl«f 3 I NO* 


03N.NO} 

S!«, 5 

OjN NOj 

X - 

Sitr 4 

\ 

H NO, 

-A' 

'•j HNOj / Ac ,0 

\ / 

N’ Ha 

HNOj 

\ / 

N 

| 

NOj 

t-Bu 


(Bu 


Figure 1 

w 

The high levels of interest in utilizing TNAZ as a military high explosive has 
stimulated extensive research to improve its synthesis (Figure 2) and preparation by 
alternative, less costly, and higher yielding methods (Ref. 2). 


39-5 







ANOTHER APPROACH 


Simultaneous Introduction of Geminal Dinitro Groups 
and Nitrolysis: 


\ NHjOH \ 


.OH 


/ 


c-o- 


/ 


C-N 


UNO« 


Cli,CI, 


\ ,* NO > 

/^NO, 



N N N 

I I I 

R R NO; 


Figure 2 

The density of TNAZ is 1.84 g/cm 3 , greater than all commonly used military high 
explosives except HMX. TNAZ has a high performance level with a detonation velocity 
of 9 km/sec and a detonation pressure of 360 kbar which compares favorably to HMX 
with a detonation velocity of 9.11 km/sec and a detonation pressure of 390 kbar (Ref. 3). 

The scientific approach used was divided into four stages. The first phase involved 
solubility tests to determine the best solvent to use in the recrystallization. The second 
phase included a small scale (100 g) recrystallization to verify our process procedure and 
technique. On successful completion of phase two, phase three was initiated. This phase 
involved recrystallizaiion of 33.94 pounds in two separate batches. Phase four consisted 
of analyzing the recrystallized TNAZ. This included determination of structure, 
morphology, thermal properties, and sensitivity. 


39-6 



! 


PROCEDURES AND RESULTS 
SECTION HI 

Solubility of TNAZ in 20 ml of acetone, ethanol, methanol, 2-propanol, and 
distilled water(200 ml at ambient) was determined at ambient and elevated temperature. 
Solubility tests were conducted to determine the most efficient solvent to use to 
recrystallize TNAZ. Solubility is defined to be the maximum amount of solute that will 
dissolve in a fixed amount of solvent at a defined temperature. When a two component 
system contains the maximum quantity of solute in a solvent, it is saturated (Ref. 4). 
The results of the tests are in the chart below. 


SOLVENT (TEMP C) 

QUANTITY OF TNAZ 

SOLVENT (TEMP C) 

QUANTITY OF TNAZ 


DISSOLVED(g) IN 20 ml 


DISSOLVED(g) IN 20 ml 

ACETONE (34) 

11.08 

ACETONE (50) 

15.17 

ETHANOL (29) 

1.46 

ETHANOL (70) 

5.40 

METHANOL (25.2) 

1.78 

METHANOL (60) 

3.75 

2-PROPANOL (23.5) 

0.38 

2-PROPANOL (70) 

2.0 

WATER (23.3) 

0.04 in 200 ml 

WATER (70) 

0.2 


Figure 3 



The determining factor in choosing a solvent was yield. To maximize the yield ethanol 
was used because it had a strong temperature versus solubility gradient and could 
dissolve a large percentage of TNAZ. The TNAZ was crashed out in distilled water 
because it is minimally soluble in cold water and the ethanol is soluble. Precipitation in 
water made the TNAZ easily separable from the ethanol and traces of the ethanol easily 
removed from the TNAZ. A one liter scale recrystailization of 100 g of TNAZ was 
conducted to test the procedure and techniques. 278 ml ot ethanol was added to the one 
liter glass jacketed reactor. The system was heated to 70 degrees Celsius, under 


39-7 





solution was stirred until all of the TNAZ was dissolved. The hot solution was 
discharged into a crash tank with 80 ml of distilled ice water. The TNAZ precipitated 
and while still cool the precipitate was suction filtered. The TNAZ air dried overnight 
and was then dried in the oven under Atmospheric Pressure at 50 degrees Celsius 
overnight. 95.30 g was recovered, corresponding to a 95.3% yield. This test validated 
the process procedures and the techniques. Subsequently preparations for a 50 Liter 
scale recrystallization were undertaken. 

The third phase consisted of the 50 Liter recrystallization. The first step entailed all 
of the planning. Thirty gallons of distilled ice was made in preparation for the final crash 
of the hot solution. The 50 L glass reactor was assembled. The cooling and heating unit 
was connected to the exchange coils iocated inside the reactor. The suction filtration 
equipment consisting of Buchner funnels, four liter side flasks. #5 filter paper, and the 
vacuum connections was assembled. The full set up is shown in the schematic drawing 



Figure 4 
39-8 






Figure 5 

After all the preparations were made, the recrystallization was ready to begin. To avoid 
exceeding the optimum operating capacity of the reactor the recrystallization was 
conducted in two batch operations. Batch one consisted of 16.675 pounds of coarse grain 
and pellet TNAZ. The pellets were crushed to increase the dissolving rate. 5.6 gallons 
of ethanol was added to the reactor through the inlet port by the liquid addition system. 
TNAZ was added slowly with agitation and the system was heating to 70 degrees 
Celsius. Within one and a half hours the TNAZ had dissolved and was ready to 
precipitate out. Six gallons of distilled ice and two gallons of distilled water was added 
to each of two crash tanks. The hot solution was discharged out the bottom outlet port 
into the ice water and the TNAZ precipitated rapidly. The quick precipitation and sub¬ 
zero temperatures experienced in the crash tanks was due to the water/alcohol azeotrope. 
The water/alcohol azeotrope, the point at which the composition of the vapor and liquid 
compositions are equal, explains the below zero temperatures and rapid precipitation in 
the crash tanks. The quantity of ice used was excessive to that required hence it was 
reduced by half in the second batch. The precipitated TNAZ was suction filtered using 

39-9 









the already prepared suction filtration assemblies. The TNAZ was first airdried and then 
oven dried for two days at 50 degrees Celsius. The remaining equipment used is shown 
in the pictures below. 



Figure 6 

For Batch one, 16.094 pounds of TNAZ was recovered, corresponding to a 96.51% yield. 
Batch two was conducted using the same procedure with the exception of a reduction in 
half the amount of ice/water slurry in each tank. 16.976 pounds was recovered in Batch 
two, corresponding to a 99.04% yield. The next step as to clean the reactor and all the 
equipment In the cleaning process 15.66 g of TNAZ was recovered from the reactor. A 
small scale recrystallization was conducted to purify it using the established procedure. 
The TNAZ was added to 43.6 ml of ethanol at 70 degrees Celsius with agitation. The 
solution was crashed into a 50/50 mixture (65 ml each) of distilled ice and water. 14.4 g 
of TNAZ was recovered, corresponding to a 92% yield. 

The recrystallized TNAZ was then analyzed. The first step was a verification of 
structure. Carbon 13 and proton NMR spectra were obtained by using a Bruker AC-300, 
300 MHz Fourier Transform Super conducting NMR Spectrometer. One 20 mg sample 

39-10 











of TNAZ was dissolved in 4 mi of deutero-acetone. Nuclear magnetic resonance 
spectrometry measures the applied field strength plotted against the absorption signal. In 
NMR a super conducting magnet produces a homogenous magnetic field of 
approximately 7.1 Tesla between poles. The sample is spun about its mainfield (Z) axis 
by a stream of air to average out any existing homogeneities in the xy plane. The 
number of signals in the spectrum indicates the number of like atoms there are in the 
molecule. The position of the signal indicates the electronic environment of each atom. 
The splitting of a signal into several peaks indicates the environment of carbon with 
respect to other, nearby carbons. In the proton NMR, the hydrogen nuclei absorb the 
magnetic radiation and create peaks. The peak shifts (Figure 7 and 8) are observed and 
represent the different functional groups of the molecule (Ref. 5). 

The FTIR was also used to confirm the structure of TNAZ. As the molecule absorbs 
the infrared light its molecular bonds bend, stretch, and create absorption peaks. These 
peaks are observed and represent the different functional groups of a molecule (Figure 
9). A particular group of atoms gives rise to characteristic absorption bands, that is, a 
particular group absorbs light of frequencies that are much the same from compound to 
compound. For example, C-N Amine group absorbs strongly at 1180-1360 reciprocal 
centimeters (Ref. 5). The three charts below illustrate the plots of the verification 
process. 


39-11 





IN I 


S4lt U*'1| 


3 *58 


Nt/Oi IN. M* 
NV« I.IM 
M llt.il 


Figure 7 


i.S.9-nM»ITNUCTI«l« imai/l 


i m 


CMLS.MI 

OitliH-N 

V IN.ISI 
ST SIS.3 
o» iiii.m 

« IWM 

avn .. 


m 9/it 

ca nu.ur 
or iil ft 

a .i” 

ca «.;• 

CT tl.«l 

n .j;- 
Nl/3» HI.*:* 
PM/CM . til 

si *4i«. «a 








INFRARED SPECTRUM (FTIR) 

BOND INDICATED PEAK LOCATION (WAVE NUMBERS) 

C-H 2914.62, 1427 

C-C 867.50,842.46 

NO 2 1544.59, 1366.85 

C-N 1328.26,1279.12, 1218.3 

Figure 9 

The Carbon-13 NMR Spectrum of TNAZ in deutero-acetone revealed peak shifts of 
84.889 ppm and 105.012 ppm. The peak at 84.889 ppm represents the carbon atoms in 
the number two and four ring positions and the peak at 105.012 ppm represents the 

geminal carbon group in the number three position. Both peaks representing 
TNAZ's different functional groups were found. The proton NMR spectrum indicated a 
peak at 5.482 ppm corresponding to the Alkyl protons in the number two and four 
position. The FTIR was also successful in recording TNAZ's functional groups. All 
three techniques yielded spectra consistent with that expected from TNAZ and confirmed 
its structure. 

The particle size of TNAZ was determined using a Brinkman Particle Size Analyzer 
2010. The average panicle size was 11.72 microns. Scanning Electron Micro graphs 
revealed that TNAZ’s particle shape was somewhat rectangular (Figure 10). 


39-13 






Figure 10 

The thermal properties of TNAZ were then determined. Half a milligram samples of 
TNAZ were thermally analyzed by Differential Scanning Calorimetry. Heat was applied 
at a rate of 10.00 deg C/min. The scan produces a trace of endothermic and exothermic 
events which take place as the temperature of the sample rises in an inert Nitrogen 
environment (Ref. 6). The following plot (Figure 11) is an example of an endothermic 
and exothermic run on TNAZ. 


39-14 








la 


aaa 


-IS. 31. 99 


u.a !3L0 ia.9 :«.« aa.a ivlib suo ma 


Figure 11 

The DSC results indicate that TNAZ's melt onset ranges from 98.05 to 98.17 degrees 
Celsius and the decomposition onset ranges from 243 to 243.4 degrees Celsius. 

Impact sensitivity was determined using the Bureau of Mines Type 12B Drophammer. 
Twenty five samples weighing 35 milligrams each were placed on pieces of abrasive 
sandpaper. A 2.5 kg weight was dropped at varying heights registering go's and no go's. 
The H 50% of TNAZ was 20.1 cm. The chart below (Figure 12) compares TNAZ’s 
impact sensitivity to other sources of TNAZ. 


MATERIAL H50%(cm) 2.5 kg wt. 

TNAZCBATCH ONE) 20.1 

TNAZCARMY POWDER) 17.1 

TNAZ(AEROJET POWDER) 23.8 

TNAZ(ARMY POWDER) 25.3 

Figure 12 

Charge fabrications were initiated. 1.5 g of TNAZ was pressed at 70 degrees Celsius 
at 30,000 psi in three intensifications. The mean density achieved on the pellets was 
1.812 g/cm 3 corresponding to a 98.5% Theoretical Maximum Density. The sixteen 


39-15 






pellets were detonated to measure detonation velocity. 10 piezoelectric pins were placed 
10 mm apart to record the shock wave arrival time using a Textronix 710A waveform 
digitizer. Velocity was obtained by differentiation of distance with respect to time 

v=dxdt 

where v is the velocity, x is the distance (10 mm), and t is the time. The average 
detonation velocity achieved was 8.71 mm/usec. 


39-16 







CONCLUSION 
SECTION IV 


The solubility and recrystallization of TNAZ was successfully completed. Ethanol 
was found to be an efficient solvent from which to crash precipitate TNAZ in distilled ice 
water. The yield average was 98 % in the batch processes. The purity of the recovered 
material was in excess of 99%. The melt onset of TNAZ was found to be 98 degrees 
Celsius. The NMR and FT®, verified the structure of TNAZ. The particle size was 
found to be 11.72 microns and rectangular in shape. Pressing of the TNAZ to fabricate 
charges to determine its explosive properties has resumed. 


39-17 




MISCELLANEOUS 


SECTION V 

This summer through HSAPI learned the importance of patience and safety in 
conducting an explosive scientific operation. I have a more complete understanding of 
organic chemistry and explosive technology. I gained experience with actual chemical 
engineering which will be useful when I begin to study chemical engineering. My 
summer project would not have been such a success without the guidance and support I 
received from the personnel at the HERD facility. First of all I would like to thank Mr. 
Stephen Aubert for creating a step-by-step project that involved hands on operation of 
different types of lab equipment. I would also like to thank everyone at the HERD for 
helping me to use the equipment in the labs. I appreciate everyone's cooperation and 
eagerness to assist me at all times. It was most definitely a pleasure to be a part of this 
program. 


39-18 






REFERENCES 
SECTION VI 


1. Iyor, Dr. Sury, et. al. ScaJed-up Preparation of 1,3,3-trinitroazetidine. Army 

Armament Research, Development, and Engineering Center. Picatinny Arsenal, 
NJ. 

2. Archibald, T.G., Richard Gilardi, K. Baum, and Clifford George. Synthesis and X- 

Ray Crystal Structure of 1,3,3-trinitroazetidine. American Chemical Society, 
1990. 

3. Dobratz, B.M. LLNL Explosives Handbook: Properties of Chemical Explosives and 

Explosive Simulants. University of California, 16 March 1981. 

4. Shugar/Shugar and Bauman/Bauman. Chemical Technician's Ready Reference 

Handbook, Second Edition. New York: McGraw Hill Book Co., 1981. 

5. Morrison, Robert T. and Robert T. Boyd. Organic Chemistry, Third Edition. 

Boston: Allyn and Bacon, Inc., 1973. 

6. Dobratz, B.M. LLNL Explosives Handbook. Lawrence Livermore National 

Laboratories. March 16, 1981. 


39-19 





I 

r 

i 


Hypervelocity Impact Studies Utilizing Semi-Empirical Codes 

Randy Thomson 
High School Apprentice 


WL/MNSA 

Eglin Air Force Base, Florida 

Final Report for: 

High School Apprenticeship Program 
Wright Laboratory/Armament Directorate 


Sponsored by: Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 
August 1993 


40-1 




Hypervelocity Impact Studies Utilizing Semi-Empirical Codes 

Randy Thomson 
High School Apprentice 
Abstract 

One o! the tasks of the Technology Assessment Branch (MNSA) is to assess the lethality of 
hypervelocity kinetic energy weapons against foreign aerospace threats. MNSA uses several analytical 
tools to fulfill this task. Chief among these tools are Eulerian codes with fixed grids, excellent for 
modeling the massive distortions that occur in the initial hypervelocity impact; Lagrangian codes with 
flexible grids that accurately model late time structural response; and semi-empirical codes, which rely 
on engineering models based on actual theories that are empirically fit to experimental data using 
regression techniques. The Eulerian and Lagrangian codes provide accurate results, but require 
enormous amounts of computer time, even on modem supercomputers. A need was seen for a fast¬ 
running code that could provide accurate data on desired lethality criteria. These codes have existed for 
some time, but due to their nature, require extensive amounts of ongoing analysis to accurately portray 
hypervelocity impacts as more experimental data becomes available. During the summer of 1993, the 
accuracy of semi-empirical tools for modeling hypervelocity impact was studied, and the information 
drawn from this research is the basis for this report. 


40-2 








Hypervelocity Impact Studies Utilizing Semi-Empirical Codes 

Randy Thomson 

BACKGROUN D 

One of the main tasks of the Technology Assessment Branch (MNSA) is to assess the lethality of 
hypeivelocity kinetic energy weapons against foreign aerospace targets. To fulfill this task, MNSA 
utilizes several techniques, including full and sub-scale tests as well as analytical modeling. The Branch 
uses three main analytical techniques. Lagrangian codes utilize a flexible grid that allows the cells to be 
appropriately distorted as stress is applied, but permits neither mixed material cells nor loss of material 
within any one cell. This makes Lagrangian codes excellent for late time analysis of hypervelocity 
impact. Eulerian codes utilize a fixed grid that allows material to flow through the grid cells, but does not 
allow deformation of the grid cells themselves. This makes these codes good for measuring early time 
effects, when massive fluid deformation occurs. Semi-empirical codes do not divide the projectile into 
many small sections or use cells to measure an integrated response. Rather, semi-empirical models 
treat the projectile holistically, and directly calculate lethality parameters from explicit equations. The 
equations start by making an assumption based on theory that a lethality quantity is proportional to an 
expression in several variables, implying that the quantity is equal to the expression multiplied by some 
constant of proportionality. If the equation is a differential equation, as many of these equations tend to 
be, the equation is integrated. Regression analysis is used to fit the resulting curve to an experimental 
database. The resulting equation, then depends both on physics-based material parameters and 
interrelationships of those parameters, as well as empirical constants resulting from the curve fitting. 
The resulting equation is simple to evaluate by a computer code, and lends itself to modular architecture, 
evaluating each group of parameters in a separate module and linking and outputting the results through 
the main program shell. If accurate assumptions are made in the development of the model, the semi- 
empirical algorithm will have the powerful advantages of high accuracy and fast running time. However, 
even with accurate assumptions, the inherently semi-empirical nature of the algorithms can make it 
difficult, unwise, or even impossible to extrapolate beyond the extreme values of the calibration 


40-3 






database. Thus, continual calibration with experimental data is necessary. A particular semi-empirical 
code developed by MNSA is KAPPII (Kinetic Energy Analytic Erojectile Effects Erogram H). The main 
focus of my work was to test the effectiveness of the current version of KAPPII for accuracy versus 
theoretical predictions as well as experimental data. If the accuracy was lacking, input parameters would 
be manipulated to better fit the experimental data and empirical constants would be varied if necessary. 
Other duties directly and indirectly related to this goal will also be discussed. 

MEEll 

KAPPII is a set of fast running modular algorithms in FORTRAN 77 used to calculate damage to 
complex three-dimensional targets impacted by multiple hypervelocity projectiles including long and 
short rods, long and short hollow rods, and spheres. It has been calibrated against an extensive 
experimental database covering a wide range of impact conditions. Targets include Theater Ballistic 
Missiles (TBMs), Reentry Vehicles (RVs), Post Boost Vehicles (PBVs), and a variety of both simple and 
complex flat plate targets. Targets can be modeled either with a simple series of flat plates for 
calibration and querying purposes, or with a computer modeling program such as FASTGEN, GIFT, or 
BRL-CAD. An extensive and easily customizable materials database contains all material parameters 
necessary for the code. Due to its modular architecture, various algorithms can be either combined 
singly or in groups called emulation modes to better model the particular scenario at hand and output the 
appropriate damage values. The code is also portable and can be implemented on anything from Intel 
x86-based or Macintosh PCs, to Silicon Graphics or Sun workstations, up to a supercomputer such as 
the Cray Y-MP. To keep all of this flexibility in check, a configuration management system with a 
configuration control board oversees all changes, evaluates all errors discovered, and rates various 
corrections and enhancements for priority of implementation into the next code release. MNSA is 
working toward incorporating additional models into the code that are based more upon physics than 
empirical curve fits to experimental data. Even the most purely scientifically based model is no good if it 
does not reflect reality, however, and the complex nature of these algorithms requires large amounts of 


40-4 





data to gain confidence in their output. MNSA implements this code by entering necessary parameters 
for a hypervelocity impact scenario into a series of ASCII files containing projectile and target data, as 
well as control keywords and, optionally, non-standard values for empirical constants. The code is run 
on a Silicon Graphics workstation and manipulated through one of several X terminals throughout the 
Branch. 

TRAINING 

First, a study was conducted to increase proficiency in UNIX in order to simplify access to the 
code and its associated files. Also during this initial training period, hypervelocity impact theories were 
studied through a short course in penetration mechanics to allow the evaluation of the code and 
algorithms from a more theoretical point of view. This along with a study of the KAPPII user's manual, 
algorithms, code structure, and goals helped avoid the “black box” syndrome when manipulating the 
code. Now with a firmer understanding of the theories and tools that would be involved, the project 
proper could begin. 

METHODOLOGY 

Then it was time to conduct hands-on testing and manipulation of the code. First, the sample 
input files were executed and the outputs studied. A series of variations on the original flat plate models 
in the example was tested with reasonable results. A flat plate approximation of an RV-type target was 
developed, attempting to keep the materials as accurate as possible. The output damage values were 
inaccurate, however. Damage readouts in the lower semi-infinite layer of the target, made of uranium 
and intended to model the fissionable section of a nuclear warhead, were grossly exaggerated, resulting 
in crater volumes far greater than they should have been. It took a thorough search of the code and the 
input files to finally discover that there was a faulty parameter in the material database resulting in 
extremely low target strength. Many of the errors in the output could be traced to the materials database. 
Due to its inherent customizability and the rapid addition of material parameters needed for more 
accurate models rarely accessed by anyone outside of MNSA and the developers of the algorithms. 


40-5 








several typographical errors had been made in the database. To fix this problem, as well as the problem 
of searching a pure ASCII file for the desired material, the database was exported to a PC and 
manipulated using Microsoft Excel. This made the database easier to read, analyze and use. Utilizing 
simple search and organization tools, the database could be searched for materials meeting any desired 
criteria and errors were easily weeded out. Without large numbers of varied trial runs and the 
subsequent update of the database, many less severe errors could have gone unnoticed, leading to 
inaccurate data. 

PROCEDURES AND RESULTS 

With a basic confidence in the output from KAPPII after gaining familiarity in possible problems and 
successfully surmounting them, it was time to test KAPPII against actual experimental data. Obviously, 
data to which the code had not already been calibrated was necessary. For these purposes, the 
NASA/Mars ^oll Space Right Center (MSFC) hypervelocity impact database was selected. This 
particular database consists of a large series of damage values for hyperveloc ipacts on targets at 
up to 8 km/s. Target configurations included single and multiple bumper specimen - constructed from a 
wide range of engineering materials, including aluminum, Kevlar, graphite epoxy, cadmium, and 
alumina, of various thicknesses with variable spacing. Tests were performed with and without multi-layer 
insulation between the bumper plates and pressure wall plates. This resulted in an excellent database 
for comparison, but not without some drawbacks. NASA developed the database with an eye to 
assessing vulnerability of space station materials to orbital debris impact. This reflected itself in the 
database. Large numbers of composite materials were used in the test database. KAPPII, or even a 
hydrocode, for that matter, does not adequately treat composite materials because it assumes materials 
to have equal tensile, compressive, and shear strength in all directions; in other words, it assumes the 
material to be isotropic, meaning that the material is symmetric in its response to stress. This is a 
grossly inadequate assumption for composite materials, whose strength factors could vary by orders of 
magnitude depending on alignment of the force with respect to the fibers within the material (foe to their 

40-6 




anistropic nature. Second. KAPPII can not adequately handle multi-layer insulation in its present form. 
Third, the vast majority of projectiles were spherical, to approximate space debris. This is perfectly fine, 
but MNSA is more concerned with long rod penetrators than spheres. The projectile materials, Lexan, 
various aluminums, and steel, were designed to model the various densities of micrometeoroids. Lexan 
was designed to simulate ice, aluminum was designed to model rocky meteoroids, and steel was 
designed to simulate iron meteoroids. These were perfectly modelabie projectiles, but there was a 
lacking in the higher strength and density regimes, such as data for tungsten and depleted uranium 
projectiles. In general, if the proper test series were extracted, the data would be useful, but care was 
needed in extracting the data. First, the data was converted from the native Lotus 3-D format to an 
Excel Workbook. The file was then searched for a moderate size test series with enough constant 
parameters for KAPPII to yield consistent data. After much searching, an adequate test series was 
extracted. The parameters were converted to input files for use by KAPPII. After setting up parameters 
to gain the highest amount of accuracy available from KAPPII as it stood, the test series was evaluated 
by KAPPII. Also, to make it easier to compare KAPPII results to experimental data, a huge input file was 
set up to give a wide range of projectile velocities, masses, materials, and target configurations. This 
database could be used by future researchers to quickly compare KAPPil's predictions for an impact 
regime of interest and evaluate KAPPil’s performance when a more extensive database becomes 
available in the future. Once the damage readouts had been obtained from KAPPII, the predicted 
damage numbers could be compared to the experimental values in the NASA database. After unit 
conversion to ensure compatible units, the results were compared. In Figure 40-1, the good correlation 
between KAPPII and the database can be seen. KAPPII underpredicts the damage values by 
approximately 9 percent. This is good correlation for an extraction from a database this size, allowing for 
experimental error. It is good that KAPPII underpredicts rather than overpredicts the damage, for this 
means that if KAPPII predicts a kill, there is an extremely high probability that a kill will take place due to 
the conservative damage readouts. 


40-7 







Figure 40-1 
First Bumper Damage 



VELOCITY (KM/S) 

However, due to improper characterization of the debris cloud, damage readouts for the second layer of 
the target are 30-40% lower than expected. One of MNSA’s major research thrusts is to better 
characterize the debris cloud. By this time next year, algorithms should be in place to correct this 
deficiency by modeling impulse loading of targets by debris clouds. In the meantime, simple 
manipulations of the debris cloud empirical constants result in accuracies approximately on the order of 
that obtained for the first layer. Otherwise, damage predictions for the target are excellent. Similar 
results were obtained from extractions of other sections of the database. Figure 40-2 displays the 


KAPPII input files for th ' particular experiment. 


KAPP11 El 




teJnauLBia 


Figure 40-2 


.363319818 ’AL1100* 4.410.0.0 7 
/ 

363319818 ’AL 1100’ 4.49 10.0.0 7 
/ 

.363319818 *A11100’ 4.52 10.0.07 
/ 

363319818 ’AL 1100’ 4.76 10.0.07 
/ 

363319818 ’AL 1100’ 5.55 10.0.07 


40-8 









KAPPH Target Input File 

0. 10. 90. VOID*/ 

1. 0.08128 90. ‘AL6061-T67 

0 10.16 90. ‘VOID’/ 

2. 0.08128 90. •AL6061-T67 

0 2.54 90. 'VOID7 

3. 0.3175 90. ’AL2219-T877 

KAPPli Input File 

title SB2 NASA Calibration Run 

projectile platt51 

model platt56.pmd 

kapdir /disk6/thomson/kappii/sys/ 

material kappall.dat 

Plot 

write title 
one ovel o 
onepdenO 
one odiam 0 
max hatot 1 
max hatot 2 
max hatot 3 
control 
emulate sb2 
prtpst true 
trace hits 
klohat 1.2.3,0/ 
constants 
pencrt-0.75 
qkgsiz -.25 


KAPPH Pto t Output Eilfi 


ovel oOO 

pden oOO 

odiam oOO 

hatot xOI 

hatot x02 

4.40 

2.71 

0.635 

0.615 

2.31 

4.49 

2.71 

0.635 

0.620 

2.81 

4.52 

2.71 

0.635 

0.622 

3.13 

4.76 

2.71 

0.635 

0.636 

3.00 

5.55 

2.71 

0.635 

0.685 

2.63 


A SB2 NASA Calibration Run cpu- 88.2 elapse- 94.0 


hatot _x03 

0.000E+00 

0.000E+00 

0.000E+00 

0.0O0E+00 

0.000E+00 


40-9 






CONCLUSIONS 

On the basis of the results of this summer's research, it would seem that if used properly. KAPPII 
in its present incarnation does fulfill many of its design requirements, it is very fast running compared to 
the hydrocodes, and with appropriate parameters, gives accurate results. It directly outputs the 
requested damage readings without searching through a long output file and setting up complicated 
equations to solve for the desired values. It is an excellent tool for obtaining results for hypervelocity 
impact scenarios if one stays within its limitations. The biggest problem is the fact that for many 
combinations of algorithms, those limitations are not well defined. Hopefully, the test series set up 
during this research period will make it easier to see where KAPPII's difficulties lie as more experimental 
data becomes available. The verification and validation process of the code continues, and its accuracy 
grows by leaps and bounds with every revision. To sum up, KAPPII gives relatively reliable results for 
predicting hypervelocity impact damage in its present configuration and should only improve in the year 
to come. 

ACKNOWLEDGMENTS 

The author would like to thank Research & Development Laboratories (RDL) for sponsoring this program 
and the Wright Laboratory/Armament Directorate for serving as a host site for the 1993 HSAP. Thanks 
are also in order for Mr. Don Harrison, Mr. Mke Deiler, and Ms. Glenda Apel for running the local 
aspects of the program smoothly. This project could not have been accomplished without Mr. Bruce 
Patterson, who served as a mentor for the author and provided guidance. These people also gave 
assistance from a supervisory perspective: Major Paul Coutee; MNSA Branch Chief, Mr. Ron Hunt and 
Mr. Dave Jerome; MNSA Section Chiefs, and Mr. Walt Maine, MNS Division Chief. Other people who 
gave the author assistance included Dr. William Schonberg, an associate professor at the University of 
Aiabama-Huntsville, who attempted to provide technical explanations for the more bizarre results the 
code provided. Mr. Larry Cohen from Science Applications International Corporation (SAIC) provided 


40-10 






technical support for problems with the code itself. Mr. Brian Peterson, a co-op student from North 
Carolina State University, helped with too many little things to possibly mention. Mr. Dan Biubaker 
(MNSA) helped with preparing outbriefings and the final report. Everyone else in MNSA also deserves 
thanks for assistance rendered and for suffering through the summer with this apprentice. Thank you all 
for your support, for this project could not have been done alone. 


3 


40-11 






Area Centroid Analysis 


Christina M. Trossbach 
High School Apprentice 
Advanced Processing Systems Branch 


Wright Laboratory Armament Directorate 
WL/MNGA 

Eglin Air Force Base, FI 32542-6810 


Final Report fon 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 


August 1993 


41-1 






Area Centroid Analysis 

Christina M. Trossbach 
High School Apprentice 
Advanced Processing Systems Branch 
WL/MNGA 

Abstract 

The project studied was to determine the effects of the Gain & Offset Compensation on the Area 
Centroid. Gain & Offset and Area Centroid are two of seventeen signal processing algorithms that were 
used in the testing and evaluation. The Area Centroid is the location of the center of the area of a target 
A program in Pascal was created to calculate the output, and the Area Centroid was found in coordinate 
form. 


41-2 






Area Centroid Analysis 

Christina M. Trossbach 

Introduction 

Reagan's Administration implemented a new mission in the Air Force in 1983. It was called the 
Strategic Defense Initiative (SDI), but was changed to the Ballistic Missile Defense Organization 
(BMDO) in 1993. Its goal is to provide new technology for our country's defense. One of Wright 
Laboratory's tasks for BMDO is to develop components for interceptor missiles. The Advanced 
Processing Systems Branch (WL/MNGA) works with the development, analysis, and evaluation of signal 
processing hardware and algorithms. Through the work of different branches at Wright Laboratory and 
contractors, a component program called the Signal Processor Package Design, or SPPD, was created. 

Although I now work for MNGA, the project I completed this summer can be tied into my 
project done last year, in that it involves incorporating additional calculations with some of the Signal 
Processing Algorithms of SPPD. Each of the seventeen algorithms completes a specific task, but for the 
purpose of my studies. I was concerned with only two algorithms-Gain & Offset and Area Centroid. The 
final objective was to find the location of the Area Centroid in coordinate form. 

Background 

SPPD is one example of a signal processor that the engineers in MNGA have developed. A 
signal processor, when put with a sensor make up a seeker, which is located at the front of an interceptor 
missile. The sensor is made up of a matrix of detectors, and is called an FPA, or focal plane array. The 
sensor absorbs radiation from the target and creates an image, which then is transferred to an A/D (analog 
to digital) converter. The converter then sends the image to the signal processor which processes the 
image to find the target SPPD has a set of algorithms which perform these tasks in order to find the 
target. 

Theoretically, the image that the sensor creates can be either an ideal or a non-ideal image. An 
ideal image which is given the same intensity on every pixel in turn gives every pixel the same output 


41-3 




value. This case does not happen in reality, so the pixels may have different values, such as the example 
below. 


Ideal 


1 

i 

i 

i 

1 

i 

t 

i 

1 

i 

i 

i 

7" 

i 

T 

7“ 


Non-Ideal 


T 

2 

7" 

2 

7" 

T" 

T 

T 

0 

i 

2 

7" 

7" 

7" 

i 

t 


When the sensor produces a non-ideal image, die Gain & Offset algorithm ^ needed to compensate for 
the different values of intensity. Below is an example of the graph of a non-ideal response, or a non¬ 
uniformity. 


Pixel (M.N) 



Actual Response 
Desired Response 


Input 


The gain is the slope of the actual response, and the offset is the difference between the desired response 
and the actual response. 

Methodology 

My first task was to learn the Pascal programming language, because it was going to be the main 
tool used for calculating the Area Centroid. I started out with simple programs, and I gradually built up 
to more complicated programs involving arrays, like what I would be using to find the Gain & Offset. 

My second task was to acquire more knowledge of the Gain & Offset and Area Centroid functions. It 
was necessary for me to completely understand what the arrays of Gain & Offset stood for before I could 
implement them in Pascal. 


41-4 







I started out with the Gain & Offset algorithm, which is the first of the functions that the signal 
processor Performs. Below is the equation used in the program. 

PC[M,N] = G[M,N] * PV[M,N] + 0[M,N] 

The array GfM.N] is the gain adjustment, PV[M,N] is the raw pixel value, 0[M,N] is the offset 
adjustment, and PC[M,N] is the corrected pixel value for the array. The value of PC[M,N] was then used 
in solving the next function, the Area Centroid. 

The Area Centroid algorithm solves for the location of the center of area of the target. The 
output is given in (X,Y) form. The equations below solve for the X coordinate and the Y coordinate of 
the Area Centroid, respectively, 

XCA = Sum(N * PC[M,N] / Sum(PC[M,N]) 

YCA = Sum(M * PC[M,N] / Sum(PC[M,N]) 

The final output of the program is the location of the Area Centroid, which is the location of the center of 
area of the target, in coordinate form. 

Conclusions 

The program I wrote implementing the SPPD Algorithms Gain & Offset and Area Centroid was 
successful. However, there were delays in the coding because I encountered some memory problems. At 
the beginning of my program, you have to set the X and Y values to the maximum size array you are 
using. I used a frame size of I28x 128 because that is what SPPD uses in the actual design. After many, 
many days of unsuccessful rewrites, my mentor suggested decreasing the array size. I tried many 
combinations, and the largest the PC could accommodate was an array size of 64x64, which is one-fourth 
the size of the actual SPPD design. As a result, it was impossible to evaluate and analyze any real data, 
so we tested some simple values in which we were able to calculate the results in another fashion, so we 
could compare them with the results of the program. Consequently, the program cannot be used in its 
present form, because of the varying frame size, and there was not any image data in Pascal form to test. 


41-5 







I would like to thank all the people that made my apprenticeship this summer an enjoyable and 
worthwhile experience. Paul McCarley-my mentor and friend. Emily Martinez-my other mentor. 
Thanks for all the help with Pascal and thanks for adopting me as your second apprentice. I would also 
like to thank the other apprentices in the HS AP Program for making this an awesome summer, especially 
Christie Gooden, who I shared knowledge and personal experiences with-Christie, Emily, and I went 
through a lot together. Jennifer Bautista-thanks for being a special friend, you silly goose! Barry Kress- 
tlte Stallion of the Earth. Jon Ward-the Domino Guru. Kyle Perry-the GQ God himself. Elliot Moore- 
Mr. Apprentice. And all the others in the program. I'll miss you all. 1 especially want to thank Mr. 
Harrison, Mr. Deiler, and Mrs. Apel for their support of this wonderful program. I've learned so much in 
these past two summers, and the experience I've gained is priceless. Thanks again. 



Area Centroid Analysis 


Christina M. Tross bach 

References 

Christina Trossbach, 'Sensor Computation Analysis." 1992 HS AP Final Report 
Paul McCariey, General knowledge on SPPD Algorithms 
Emily Martinez, General knowledge on Pascal Programming 


41-7 





Area Centroid Analysis 

Program Listings 

This program solves for the Gain & Offset correction to find the compensated output pixel value. 

program Gain & Offset; 
const 
X = 64; 

Y = 64; 
type 

MyArrayType = ARRAY[1..X,1..Y] of integer, 
var 

M: Integer, 

N: Integer, 

Ans: Integer, 

PV: MyAnayType; 

G: MyAnayType; 

O: MyAnayType; 

PC: MyArrayType; 

File_Name: String[30]; 

New_File: file of integer, 
begin 

WriteCEnter the filename to receive the output data: ’); 

Readln(File_Name); 

Assign(New_File, File_Name); 

Rewrite(New_File); 

for M:=l to X do' 
for N:=l to Y do 
begin 

PV[M,N] := 1; 

G[M,N] ;= 1; 

0[M,N] := 0; 
end; 

for M:= 1 to X do 
for N:= 1 to Y do 
Write(New_FiIe, PVfMJf]); 
for M:= 1 to X do 
for N:= 1 to Y do 
Write(New_File, G[M,N]); 
for M:= 1 to X do 
for N:= 1 to Y do 
Write(New_File, 0[M,N1); 

Close(New_Fik); 

end. 


41-8 





This program reads the data from the first program to solve for the location of the Area Centroid. 


program Area Centroid; 
const 
X = 64; 

Y = 64; 
type 

MyArrayType * ARRAYfl. JC.1..Y] of integer, 
var 

M: Integer; 

N: Integer; 

Ans: Integer; 

PV: MyArrayType; 

G: MyArrayType; 

O: MyArrayType; 

PC: MyArrayType; 

File_Name: String[30]; 

01d_FiIe: file of integer; 

New_File: text; 

Sural: Longinq 
Sum2: Longinq 
Sum3: Longinq 
XCA: Real; 

YCA: Real; 

begin 

Write(*Enter the filename containing the input data:'); 
Readln(File_Name); 

Assign(01d_File, File_Name); 

Reset(OldJFile); 

Write('Enter the filename to receive the output data: *); 
Readln(File_Name); 

Assign(New_FiIe, File_Name); 

Rewrite(New_File); 

for M := 1 to X do 
for N := 1 to Y do 
read(01d_FIle, PV[M,N]); 
for M := 1 to X do 
for N := 1 to Y do 
read(01d_Rle,G[M,N]); 
for M := 1 to X do 
for N := 1 to Y do 
read(01d_File, 0[M,N]); 
for M := 1 to X do 
for N := 1 to Y do 

PC[M,N] := G[M,N] * PV[M,N] + 0[MJi); 

Suml := 0; 

Sum2 := 0; 


41-9 




Sam3 := 0; 
for M := 1 to X do 
far N := 1 to Y do 
begin 

Suml := Suml + PCfM.N'l; 

Sum2 := Sum2 + (N * PC(M,N]); 
write(NEW_FILE4um2:6 > n:5,pc[m,n]:4); 
if (N mod 5 = 0) then 
writeln(NEW_FILE); 

Sum3 := Sum3 + (M * PC[M.N]); 
end; 

XCA := Sum2/Sumi; 

YCA := Sum3/Suml; 

WritelnC(XCA,YC A)=7C,XCA,7,Y CA,’)’); 
Writeln(NewJFile, ’C-XCA.V.YCA,')'); 
Writein('(Sum 1 )=','(',Siun 1 .O'); 
WritelnfXSui^ysVO.Suntf,’)’); 

WritelnCfSumS^VC.Sun^,’)'); 

Qose(New_Ftle); 

end. 


41-10 




THE DEVELOPMENT AND ANALYSIS OF A SPARXGAP FIRESET 


Darcie Tutin 
High School Apprentice 
Fuzes Branch 


Wright Laboratory Armament Directorate 
WL/MNMF 

Eglin AFB, FL 32542-5434 


' 


Final Report for: 

High School Apprenticeship Program 
Wright Armament Laboratory Directorate 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 


August 1993 





THE DEVELOPMENT AND 'ANALYSIS OF A SPARKGAP FIRESET 


Darcie Tutin 
High School Apprentice 
Fuzes Branch 

Wright Armament Laboratory Directorate 
Abstract 

The purpose of this project was to make a fireset which 
would be an accurate way to send high currents through test 
items. The development of a sparkgap fireset was ideal because 
of its low inductance and resistance. It was possible to analyze 
waveforms received on an oscilloscope from the current viewing 
resistor to find total circuit resistance and inductance. These 
values were then compared to P-spice simulations. 


42-2 





THE DEVELOPMENT AND ANALYSIS OF A SPARKGAP FIRESET 


Darcie Tutin 


INTRODUCTION 

The purpose of this project was to construct a sparkgap fireset 
which would be an effective way to send high currents through test items. 
An ideal fireset should have both a low inductance and a low resistance. 
Total circuit inductance and resistance determine the peak current output 
as well as waveform frequency and risetime. 

DISCUSSION OF THE PROBLEM 

Previously, problems with switching high currents using an extended 
range gap switch have been that the triggering grid places a severe drain 
on the triggering source. The voltage necessary to trigger the circuit is 
quite high so repetitiveness and accuracy are very difficult to achieve. 
This type of switch requires a very fast triggering pulse which is often 
impossible to apply without increasing overall circuit inductance and 
resistance. 

METHODOLOGY 

In order to achieve low inductance a planar stripline circuit was 
constructed. This stripline minimizes enclosed loop area, maximizes peak 
pulse current, and significantly reduces pulse rise time. 

The setup (See Figure 1), begins with an application of between 2000 
and 3500 volts to the fireset from the high voltage source. A resistor of 


42-3 






5 megohms serves as a current limiter. A .01 microfarad capacitor is 
trickle-charged from the high voltage source to serve as a storage capaci¬ 
tor. The high current then travels through a 300 megohm resistor. This 
resistor significantly reduces the voltage. Next in line on the low 
voltage side of the circuit is the zener diode. The diode sets a maximum 
voltage reference of 280 volts. At this point the second capacitor of .02 
microfarad is waiting to be discharged. In order to discharge this capaci¬ 
tor a silicon controlled rectifier (SCR) must be switched. The SCR, recom¬ 
mended for its capabilities [1], acts as an intermediate low current switch 
and having a normally off state. In order to switch the SCR on, so that 
tne capacitor can be discharged, a pulse generator is used. This pulse 
generator is set in single shot mode to produce a square wave pulse of 5 
volts. Following a voltage divider consisting of two Ik ohm resistors, 
in series, the pulse opens the SCR switch. As a result of sending the 
SCR into a state of conduction, the discharge from the second capacitor 
advances to the high voltage trigger transformer. The transformer is a 
TR130 and has a 40:1 turns ratio. The high voltage pulse travels 
through a 4.7k ohm resistor and triggers the CP Clare sparkgap switch. 

The sparkgap switch (See Figure 2) is made up of three main compo¬ 
nents. One side of the switch is the anode electrode with a very high 
potential. On the other side there is an identically sized electrode. 
This electrode is known as the cathode or adjacent electrode because di¬ 
rectly in its center is the trigger probe. The sparkgap is an excellent 


42-4 







switch because of its ability to change from an insulator to a conductor 
by straightforward application of the proper voltage to each electrode. 

This is controlled by the fireset circuit. A high voltage pulse creates 
the signal the sparkgap needs to arc across the gap between the electrodes. 
Total gap breakdown and conduction of current successfully completes 
the circuit. The switch has the ability to conduct effectively in this 
circuit between the applied voltages of 2000 and 3500 volts. This normal 
operating range takes into account the resistance of the switch between 
the cutoff region and the region of self breakdown (See Figure 3). 

The relatively small values of resistance and inductance are very 
difficult to measure. Contributions to the total circuit resistance and 
inductance are made by each circuit component since the capacitor, 
switch, current viewing resistor, and stripline are non-ideal devices. 
Theoretical predictions of circuit inductance were made using derived 
equations. It is possible to find the total inductance within a circuit 
by examining the first dampened cycle of the waveform obtained from 
circuit discharge. Knowing that: 


-Rt /2L 

V *V e sin(wt ) 

10 1 
-Rt /2L 

V =V e sin(wt ) 

2 0 2 

when t =Td/4 and t =5Td/4 
1 2 

-RTd/2L -RTd/8L 

setting V /V =e sin(wt )/e sin(wt ) 

12 1 2 

it is possible to see that: 

2 2 2 
L(inductance)=Td /C[ln (V /V )+ 4(3.1415) ] 

1 2 


42-5 






Total parasitic resistance , primarily due to the switch, is determined 
by subtracting lenown values of resistance (such as the current viewing 
resistor plus the known resistance of the test item) from the total 
resistance in the fireset circuit. 

Waveforms obtained from the oscilloscope via the current viewing 
resistor can be simulated using P-spice computer modeling. Iterative 
substitution of hypothetical values of resistance and inductance was 
performed in order to best ntetch the data with the theoretical predictions 
of total circuit resistance and inductance. With this program it is 
possible to model the total resistance and inductance within the high 
current portion of the sparkgap fireset. 

The sparkgap switch has a high current path controlled by a low 
current path. The P-spice model developed was primarily concerned with 
the high current path; this greatly simplified rcuit analysis 
(See Figure 4). The low current path consists ot ny triggering 
circuit components which are well isolated from the high current 
portion of the switch. A simple high voltage switch can be used 
in computer simulation of the high current path, yet the P-spice 
simulation requires the user to specify a resistance within the 
switch [3], Since it is not possible with ordinary test equipment to 
simply measure the internal resistance of the switch, the on-state 
switch reistance must be found via simulation through trial and error. 
RESULTS 

P-spice modeling validated the measurements from the oscilloscope as 


42-6 







well as serving as an accurate simulation of the circuit as a whole. The 
simulations do, as will any simulation, exhibit small discrepancies (See 
Figures 5,6&7). These differences can be tolerated because experimental 
variations occur due to existing signal-to-noise limitations. Measuring 
the voltage at node 4 (See Figure 4) is the simulation of the point at 
which the current viewing resistor takes measurements from the actual 
stripline. Comparing the voltage at this point leaves a difference of 
between eight and ten volts. Since typical measurements were scaled to 
50 volts, less than 15 percent error existed between the measured and 
calculated values. 

The sparkgap fireset proved to be an effective way to send high 
currents (up to lOkA) through test items. The inductance of the circuit 
was very low and remained below 25 nanohenry during all experimentation. 

The resistance of the entire circuit was also very low. By comparing 
resistances at different applied high voltage source voltages it is 
possible to examine variations in switch resistance as a function of 
applied voltage (See Figure 8). This is in agreement with Figure 3. 
Internal resistance losses of the switch are expected to decrease as 
the applied high voltage increases. 

CONCLUSION 

Sparkgap switches can be used to construct low inductance 
firesets, offering many kiloaraperes of current and capable of high 
voltage operation. Switches are available with maximum ratings well 
beyond the 4kV limit used for this experimentation. P-spice modeling 


42-7 






validated measured values of current and voltage determined by a 
digital storage oscilloscope. In addition, this modeling revealed 
the values of total circuit inductance and resistance which were virtually 
impossible to measure by alternative methods. 


9 




REFERENCES 


1. Boberg, R., "Trigger Circuits" General Purpose 1993 

pg.02 

2. "Ceramic Metal Triggered Spark Gaps" Electric Components 

Data Sheet G600E-3 Salem, Mass, pg.1-7 

3. Tuinenga, Paul W., SPICE £ Guide to Circuit 
Simulation and Analysis Using P-Spice Prentice Hall, 

N.J. 1988 pg. 183-184 


42-9 






GROUN1 


FIGURE 1 


42-10 







fn0>Hf“0< 33 m o O 3J H 



FIGURE 2 



CHARACTERISTICS OF A 
SPARKGAP SWITCH 


FIGURE 3 


42-11 












•Spark Gap Trigger 
•Oarcle 

Cl 1 0 O.IU IC-3.5K 
LI 2 3 12.153N 
R1 3 4 60.208M 
R2 4 0 4.82M 
SI 1 2 6 0 SMODEL 

•SWITCH CONTROL CRT 

VSN 6 0 PULSE(0 1 l.SU ION 0.5U l.SU 10U) 
RSW 601 

.OP 

.MODEL SMODEL VSWITCH<RON-O.5M ROPP-IOG) 
.TRAN 2N 5U UIC 
.PLOT TRAN X<R1) 

.PLOT TRAN I(R2) 

.PLOT TRAN V(3) 

.PLOT TRAN V(4) 

.PROBE 

.OPTIONS LIMPTS-1000 

.BED 


FIGURE 4 
42-12 








Grapn ?*S?? 


i . U!J J 


500 :v?. tii 


-0.500 


Cur? 

or 0 

y.. 

3.£3 V 

T 

t 

“.CO .-o 

Vo 

1.3 4 V 

! 0 

370.0 os 


A-/ 

i-T 033-.3 


OSCILLOSCOPE READING AT 2.5 KV 


Oata/TIm rm: 07/30/93 12:10:24 


•Spark Gap Trtqptr 


Taaparatura: 27 0 


►- 

0» 

P VH» 



2.0m 3 Out 

Tim 


4 0m 3 Ova 


P-SPICE SIMULATION AT 2.5 KV 

42-13 





















. W«-'—M J ■ U I 


.. -*-• U- 


Cur«or 0 


; 4 _ ^ 

t. 90..If! 


I « I KJ. cj Ts 


U l ww « iw • • 5 


OSCILLOSCOPE READING AT 3 KV 


•Sew* 6w trigger 

DtO/Tlaa w 07 / 30/13 «:!»:«• TwotrtUr*: 27 0 



P-SPICE SIMULATION AT 3 KV 

FIGURE 6 


42-14 

















o mi i*i chj »t. <.n 



OSCILLOSCOPE READING AT 3.5 KV 


Mte/riw ran: 07/2V73 H:16:0* T«w«r»tur»: 27.0 



P-SPICE SIMULATION AT 3.5 KV 

42-15 




















42-16 












BALLISTIC HOLOGRAPHY 


Jon R. Ward 
High School Apprentice 
Instrumentation Technology Branch 
Mentor Mr. David Watts 


Wright Laboratory Armament Directorate 
WL/MNSI 

Eglin AFB, FL 32542-5434 


Final Report For: 

High School Apprenticeship Program 
Wright Laboratory Armament Directorate 


Sponsored By: 

Air Force Office of Scientific Research 
Boiling Air Force Base, Washington D. C. 


August 1993 


43-1 




BALLISTIC HOLOGRAPHY 


Jon R. Ward 
High School Apprentice 
Instrumentation Technology Branch 
Vrii it Laboratory Armament Directorate 


My summer was spent working as an apprentice in the Instrumentation Technology 
Branch (WL/MNSI) on Eglin Air Force 3ase. My mentor was Mr, David Watts and my 
project focused on ballistic holography in the laboratory run by Advanced Ballistic 
Holography program manager, Mr. Joseph Gordon. This technology can be used in place 
of orthoganol flash X-ray to make replicated three-dimensional images of any objects in 
ballistic testing. A computer program may be developed to turn the image into digital 
data and analyze the fragmentation. 11 can be used in three-dimensional modeling, 
behind panel fragmentation analysis, and lethality/survivability analysis. 


43-2 




BALLISTIC HOLOGRAPHY 


INTRODUCTION 

Ballistic holography is a new instrumentation technology that allows a person to view 
the full three-dimensional image of a ballistic event. One holographic picture can show a 
bullet's path, the fragmentation of the object hit, and the spall pattern, shock wave, and 
other events of ballistic importance created by the projectile. There are two main types 
of holograms: reflective and transmission. The reflective holograms are viewable by 
white light, while the transmission holograms are viewable by laser light. The kind used 
in ballistic holography is a cylindrical transmission hologram. 


43-3 





METHODOLOGY 

The methodology includes the setup of optics and the angle at which the laser beam 
strikes the holographic film. Before the setup of optics and beam angles are discussed, it 
is important to understand the materials used in the science of holography. The most 
important material to making holograms is the laser. I worked with two different types 
of lasers this summer, the sixty milliwatt Helium-Neon laser and the three joule Ruby 
laser. The Helium-Neon laser has a firing time of six to eight seconds. The length of 
time that it fires requires a two minute waiting period for the room to settle and all 
objects in the hologram must remain static. The Ruby laser has a firing time of twenty 
nanoseconds. The speed of its firing time allows quickly moving objects to be viewable 
on a hologram by the use of cylindrical transmission holography. The firing burst can be 
split into two shots milliseconds apart, allowing two images of the same ballistic event to 
be exposed in one hologram. This is double-pulse ballistic holography, which creates 
what is called an interferogram. For any form of ballistic holography the Ruby laser 
must be used. 

Another important instrument is the variable beamsplitter, it may be used to split a 
single beam into two beams by refracting part of the beam and allowing part to pass 
through. It is only used in split-beam holography, the variable beamsplitter may be 
manipulated to increase or decrease reference or object beam strength. For transmission 
holography, the object beam should be nearly equal the strength of the reference beam, 
so the beamsplitter would be set at fifty/fifty. At this setting forty-five percent of the 
beam will be refracted, forty-five percent of the beam will pass through, and ten percent 
of the beam will be absorbed or lost in the beamsplitter. For reflective hologram, the 
object beam should be only thirty percent as strong as the retcrence beam, but a fifty/fifty 
setting is still used because only about thirty percent of the original object beam reaches 


43-4 






the film after it reflects off the object, creating the image. 


The spatial filter is used in both single beam and split-beam holography. It, along 
with mirrors and lenses, is the most integral of the optics equipment required. The 
spatial filter collects the beam through a pinhole, which cleans up the beam, then the 
beam passes through a series of precision focusing devices that projects the beam onto 
the film. There are two types of films we have used in the making of the holograms: 

The holographic plate and the holographic film. Both are special holographic films that 
have a small layer of emulsion on one side storing the image in three-dimensions. The 
holographic plate is made of glass with emulsion and isn't practical for ballistic 
holography. The holographic film is flexible and is wrapped around 180 degrees of the 
inside of the cylinrical tube. 

Now that the materials have been clarified, the setup of optics will be discussed for 
each separate type of hologram; first, the transmission hologram. Transmission 
holography is usually a split-beam technique with both the reference and object beams 
striking the emulsion of the film on the same side. The laser beam is directed through a 
beamsplitter, creating the object and reference beams. The object beam continues 
through the beamsplitter and reflects off a turning mirror into the spatial filter. The 
spatial filter focuses the beam onto the object, which then reflects it onto the film. The 
reference beam was refracted by the beamsplitter onto a turning mirror, which then 
reflects the beam through a spatial filter onto the film. Both beams strike the film from 
the same side, creating an interference pattern on the emulsion on the film, thus yielding 
a three-dimensional image viewable by laser. The diagram on page 43-7 is an example 
of the setup for creating a transmission hologram. 

In order to produce a reflective hologram, it may be single beam or split-beam. The 
single beam method is rather simple. The single laser beam serves as both the reference 


3-5 





and object beams. The beam is sent through a spatial filter and projected onto the film. 
The beam which passes through the film and strikes the object is the reference beam. 

The beam that is reflected off the object and onto the film is the object be .. The object 
and reference beams strike the emulsion from opposite sides, creating an interference 
pattern which yields a hologram when viewed in the presence of adequate white light. 
The diagram on page 43-8 shows the setup of a reflective hologram (both single beam 
and split-beam). The split beam holography technique is done by using a beamsplitter to 
divert the object beam around the back of the film where a spatial filter projects it onto 
the film from behind the object. The reference and object beams leave an interference 
pattern in the emulsion creating a three-dimensional image viewable by the use of white 
light. 

The technique used in ballistic holography is cylindrical transmission holography. 

The laser beam is sent through a spatial filter projecting the beam onto the cylinder. This 
illuminates the cylinder, which is wrapped on the inside with 180 degrees of a reflective 
material and 180 degrees of film. The reflective material reflects the beam onto the film 
creating an interference pattern which yields a three-dimensional image of a ballistic 
event with a clear 180 degree point of view of fragmentation and the projectile. The 
image is viewable by use of laser light. The diagram on page 43-9 is a good example of 
this technique. 


43-6 






Split Beam Transmission Holography 


Laser 



H Mirror 


Reference Beam 


Film 


Beamsplitter 


Spatial Filter 



Beam 


Reconstruction 






Single Beam Reflective Holography 
Laser Mirror 


Film 

Objectf u! J 


reference 
Beam _ 



Spatial Riter 


Object Beam 

Split Beam Reflective Holography 


Mirror 


Reconstruction 

/^\ (White Light) 

/7T\\ 


Object Beam 


Beamsplitter 


Object R|m 


Spatial Riter 


Reference 

Beam 


43-8 







Cylindrical Transmission Holography 




Spatial Filter 


Reconstruction 



My work this summer has been in the preparation for an actual ballistic hologram. 
Before it was attempted my summer research was ended, but the goal we had been 
working for was accomplished after acquisition of precise optics equipment. We were 
creating high resolution transmission holograms in one month after starting at the 
holography laboratory in Gun Bay 10. Our most vivid picture was that of a static model 
F-16 with such high resolution that the shadow it created was visible in the background. 
Though we didn't make a dynamic cylindrical hologram over the summer, the 
holography lab in Gun Bay 10 is ahead of schedule and I look forward to next summer 
working in the proposed mobile holography unit. 


43-10 





ACKNOWLEDGEMENTS 


I have enjoyed this summer greatly because of the many people who showered me with 
help and support. First and foremost, I would like to thank God for getting me through 
these seventeen years and giving me a natural ability needed to make it this far (into the 
HSAP). Next, I would like to thank my fellow apprentices; I have made many close 
friendships that could last for years, especially Christie Gooden, Christina Trossbach, 
Jennifer Bautista, Barry Kress, and Kyle Perry. I thank them for either throwing great 
parties or for putting up with my innate sense of charm, cominess, and chauvinism. I 
would also like to thank the HSAP coordinators, Mr. Mike Deiler and Mr. Don Harrison. 
I thank Mrs. Glenda Apel for getting my pay vouchers to RDL and making sure they got 
our pay back to us. I want to thank Mrs.. Kip Cooper for showing me around the MNSI 
branch. Last but not least, I would like to thank my co-mentors, Mr. David Watts and 
Mr. Joseph Gordon for introducing me to the civil sendee as an engineer. I worked on 
projects from data reduction to developing film. I learned that an engineer either knows 
everything or will someday. 


43-11 







THE TRANSPORT 07 A FUEL JET 


IN SUPERSONIC FLOW 


Brian J. Banaszak 
High School Apprentice 
Aero Propulsion and Power Directorate 

Tecumseh High School 
9830 West National Road 
New Carlisle, OH 45344 

Final Report For: 

Summer Research Extension Program 
Wright Laboratories/POPT 

Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 

and 

Wright- Patterson Air Force Base 
August 1993 




44- 1 






THE TRANSPORT OF A FUEL JET 
IN SUPERSONIC FLOW 


Brian J. Banaszak 
High School Apprentice 
Aero Propulsion and Power Directorate 
Tecumseh High School 

Abstract 

The Wright-Patterson laboratory is currently studying the effects 
of fuel injection in supersonic flow. To obtain supersonic air speeds, 
engineers developed a wind tunnel that provides air at specified Mach 
numbers ranging from 1.5 to 3.0, depending on the nozzle that is in 
place. Also, a splitter plate was developed in order to inject helium 
into the supersonic stream. The splitter plate simulates strut 
injection, one possible method of fuel injection in a scramjet. The test 
section of the wind tunnel contains windows on 3 sides and there is a 
window in the diffuser that allows the flow to be sliced in all 3 
directional planes. Having this capability will allow concentration 
measurements of helium, at various locations to be made using acetone 
PLIF (Planar Laser-Induced Fluorescence). A YAG laser in conjunction 
with a Wavelength Extender (WEX) was used to generate a sheet of UV 
(266nm) laser light to pass through the test section causing the acetone 
o fluoresce. A CCD camera was positioned to take pictures of the 
fluoresced acetone, thus capturing the helium flow pattern. An excimer 
laser was used to measure the velocity of the fuel flow through an OH 
flow- tagging method. These laser diagnostics provide a good reference 
for future experiments that will use hydrogen instead of helium. The 
results of the hydrogen combustion will be applied to design a new 
scramjet engine capable of travelling at hypersonic speeds. 


44- 2 





TEE TRANSPORT OF A FUEL JET 


IN SUPERSONIC FLOW 

Brian J. Banaszak 

introduction 

The development of a faster airplane has been a topic of conversation 
since the Wright brothers. In the Experimental Research Branch (WL/POPT), 
Advanced Propulsion Division, Aero Propulsion and Power Directorate, at 
Wright-Patterson Air Force Base, engineers are developing the cornerstones to 
create a hypersonic airplane. Fuel injection in supersonic flow is presently 
being studied. Gaseous helium is being injected and studied as a reference 
to the eventual study of hydrogen flow. When hydrogen is injected, the lab 
will deal with supersonic combustion at different Mach numbers (M 1.5, 2.0, 
3.0) and observe the different shock waves. Soon, this will lead to the 
development of a hydrogen fueled scramjet engine that will be able to power 
an airplane to move at hypersonic speeds. Estimated speeds will be at 21,000 
raph+. The development of this kind of aircraft will put the United States in 
the driver's seat of technology. 

Combustion Facility Peslgn 

The combustion facility (Figure 1) is a wind tunnel made up of the air 
supply system, the inlet section, the settling chamber, the nozzle section, 
the test section, and the diffuser section. The facility was just completed 
a couple of months ago and its design is the only type in the world; it may 
run continuously for long periods of time. 

The air supply system (Figure 2) is the source for the necessary amount 
of flow rates in the facility. The laboratory's air supply system is made up 
of two air lines, hot and cold, in order to have better control of the flow 
rates allowed during testing. The facility allows one to have control on how 
much each air line is open in order to have better control of temperature and 
pressure of the air that enters the test section. One final note on the air 


44- 3 






Figure 1 Sohetuntic ot Supersonic ComUisu.m i 



























Hot Aik I .ini 



Figure 2 Air Supply System Schematic 









supply system is the placement of a rupture disk downstream of the mixing. 
If an over pressure condition were to occur, the disk would burst and prevent 
any damage from occurring downstream in the test section. 

The inlet section (see Figure 1) was developed to transport the air from 
the supply manifold to the settling chamber. This section consists of four 
parts including the upper manifold, the lower manifold, the block valve, and 
the expansion section. The upper manifold penetrates from the ceiling into 
the laboratory. The upper manifold consists of six outlet ports made up of 
flexible stainless steel hose that take the air supply from the upper manifold 
to the lower manifold. The lower manifold is also connected to the six outlet 
ports and it serves the purpose of transporting the air supply from the upper 
manifold to the block valve. The lower manifold looks exactly like the upper 
manifold. The block val'e controls the air supply that goes through the test 
section. It can be either fully opened, partially opened, or fully closed and 
it requires two air lines for operation. The expansion section serves as the 
transition section from a 6 inch line to the 24 inch diameter of the settling 
chamber. This section allows the air to spread out before it enters the 
settling chamber. 

The settling chamber (Figure 3) straightens the air flow before entering 
the test section. The chamber contains 3 screens and a honeycomb section to 
reduce turbulence .'evels and straighten the air flow. This chamber prepares 
the air for its entrance into the nozzle section. 

The nozzle section (Figures 4 and 5) produces the engineer's desired 
Mach number. During my stay, the desired nominal Mach number was 2.0. 
However, by changing nozzle blocks, different Mach numbers can be achieved in 
this facility. This is a vital section in the wind tunnel because it allows 
the engineers to control and reach the velocity required for the air flow. 

The test section (Figures 5 and 6) is where the engineers acquire most 
of their data. This section is sealed by silicone O—rings and graphite 
packing material in order to keep the test section air tight like the rest of 


44- 6 




Figure 3 Settling Chamber Schematic 






Nozzle Throat 



Figure 4 Nozz 







Nozzle Section/Test 













Figure 6 Test Section Cross- Section Schematic 








the facility. Also, the connections to the nozzle and diffuser sections are 
designed so the surfaces stay perfectly flush in order to avoid any kind of 
disturbance in the air flow. The section is made up of four walls. The 
bottom wall provide 5 modular ports so different fuel injection experiments 
can take place in the test section via these ports. The other three sides 
each have their own window that are 17.5 inches long. These windows provide 
a great view of the entire test section. The windows are made up of fused 
silica. Fused silica poses very good light transmission in the UV range, 
which is necessary when performing acetone PLIF. Also, the fused silica works 
well at high and low temperatures because of its extremely low thermal 
expansion rate. 

The last feature of the basic supersonic combustion facility design is 
the diffuser section (see Figure 1). This section is designed to decelerate 
and cool down the air before it goes into the exhausters of the air facility. 
The air flows through a series of pipe lines which help decelerate and cool 
down the air before it enters the exhausters. In addition, a water cooling 
system has been installed in the diffuser and water is sprayed into the 
diffuser section causing additional air to deceleration and cooling down. One 
last feature of the diffuser section is the addition of a window which looks 
directly into the flow of the test section. This window gives the viewer an 
additional optical access to another dimension of the jet flow. 

Methodology 

During my stay in WL/POPT, the transport of a fuel jet in supersonic 
flow was studied. This specific experiment consisted of a splitter plate, 
which contained a sonic helium injector on center line. The experiment also 
involved the use of a YAG laser, an excimer laser, and additional optical 
lenses in order to project an image and measure the velocity of the fuel jet. 

A splitter plate (Figure 7) is installed parallel to the two nozzle 
blocks of the nozzle section and the plate extends 3 inches into the test 
section. In the center of the splitter plate, a 3.5mm d aaeter hole is 


44- -aru 



Supersonic Wind \ ael: 

(H,W,L) - 12.70 x '4 x 91.44 cm 





1.27 cm. height 12.70 cm 


Bluff Body 



Supersonic Nozzle 


Fuel Injection Hole: 0.25 cm. Diameter 



The tip of the bluff body 

Bluff Body could be wavy in order 

to enhance mixing 


Figure 7 -upersonic Parallel Fuei Iniecrion Tunnel 



present in order to provide an injection port for different fuels. In this 
experiment, helium is the fuel injected. The helium is injected at various 
pressures in order to observe the helium at sub-sonic and sonic speeds. It 
is obvious that the helium flow has gone sonic with the presence of the barrel 
shock wave. 

In order to get an image of the fuel flow, we have developed a planar 
laser- induced fluorescence (PLIF) system. In this procedure, we create a 
seeder containing acetone. The seeder includes 2 pressure regulators and a 
heater. The pressure regulators allow the engineers to control and know the 
acetone and helium flow rates. The heater is heated to approximately 250° F 
and the liquid acetone is injected into this super heated gas mixture. This 
vaporizes the acetone so it flows with the helium through the splitter plate 
and out of the injector. A YAG laser is combined with a Wavelength Extender 
(WEX) to produce a laser sheet at 266nm. A cylindrical lens, a thin lens, and 
a prism are used to produce a uniform laser sheet that is approximately 2mm 
by 50mm. The laser is set to have its best definition when it is covering the 
injection hole of the splitter plate. The optics are arranged three different 
ways in order to show three different profiles of the flow jet: a streamwise 
profile (vertical beam plane), a spanwise profile (horizontal beam plane), and 
a transverse profile (vertical beam plane perpendicular to flow velocity) . 
In this experiment, the YAG laser causes the acetone to be excited and a well- 
focused CCD camera is set in accordance with the laser to take a picture of 
the helium flow as the acetone is fluorescing. The image is then displayed 
on a computer screen with an option to save the good jet images. In the 
images, one can observe the barrel shock, other shock waves, and the general 
flow pattern of helium at different flow pressures. This imaging technique 
will come into great use when hydrogen combustion takes place in the test 
section; since helium and hydrogen are closely related, the engineers are 
able to predict ahead of time what the flow pattern of the hydrogen will be 
during combustion based of the helium flow tests. 


44 - 






Supplied with krypton, fluorine, and helium, the engineers developed a 
UV beam to come out of an excimer laser with a wavelength of 248nm. This 
powerful laser is the facility's source to set up our OH-Flow-Tagging 
Velocimetry System (Figure 8), which measures the velocity of the helium jet 
flow. In this procedure, water is the media used to measure the velocity of 
the jet flow. The radiation of the excimer laser beam splits the water into 
hydrogen (H) and hydroxide (OH) molecules. The OH radicals are "tagged" in 
the flow one after another on a known time delay and the displacement can be 
found. This gives one enough information to decipher the velocity of the fuel 
flow. The time interval must be very short because the fuel is flowing -1000 
meters per second. 

Ra aaAfca 

The planar laser—induced fluorescence scheme showed the profiles of the 
helium flow from three different views. The streamwise profile (Figure 9) 
shows that the helium is at sonic speed with a pressure of 40 psi. The barrel 
shock shows that the helium is flowing at a sonic speed and the rest of the 
flow give the onlooker a good idea of the way hydrogen will flow when it flows 
at sonic speed. The spanwise profile (Figure 10) also shows that the helium 
flow is sonic at 40 psi. The barrel shock is evident from the top view as 
well and the viewer can see how hydrogen will flow on the top of the jet when 
it reaches sonic speed. Finally, the transverse profile (Figure 11) shows 
what the fuel flow will look like from a bottom view of the thrust and how the 
flow will spread out. This profile is not at a good angle to find the barrel 
shock to indicate a sonic speed but it does show the pattern the fuel will 
flow downstream. Profiles were down at pressures other than 40 psi, but 40 
psi seemed to be the pressure at which the fuel flow was distinctly flowing 
£• nic. Throughout this experiment, the nozzle flow was at a nominal Mach 
number of 2 and the fuel injector would allow a maximum nominal Mach number 
of 1. 

Conclusion 

The Experimental Research Branch is on the verge of a breakthrough. 
Presently, the laboratory is testing helium to study how its related 
counterpart, hydrogen, will react under certain air pressures. Later, hydrogen 
will be injected in our test section at high speeds in order to perform 


44- 14 







Figure 8 Schematic of OH- Flow- Tagging Veiocimetry System 








•44- lb 


4Ctrf*.<nwi!>£ 

Figure 9 Histe Profile of Helium Flow at 40 psi 
(vertical laser beam plane) 







ip«m*is£ 











supersonic combustion. After years of further research and development, a 
scramjet engine will be built based on the data acquired from our engineers 
to fly an airplane at hypersonic speeds at 21,000mph+. 


44- If 





w«nci»miCBS 


1. Anderson, John D Jr. Introduction To Flight . McGraw- Hill 
Book Company. New York, NY. 1989. 

2. Chen, T H; Goss, L P; Trump, D D; Sarka, B; Nejad, A S. "The 
Effects of Nozzle Geometry Upon Sonic Fuel Injection as 
Studied by OH- Flow- Tagging Velocimetry. " AIAA paper 
91- 0575. Reno, Nevada. January 7- 10, 1991. 

3. Donbar, J M; Glawe, D D; Nejad, A S; Driscoll, James F; Chen, 
Tzong H; Goss, Larry P. "Transport of Fuel Jet in 
Supersonic Flow." AIAA 32nd Aerospace Sciences 
Meeting. Reno, Nevada. January 10- 13, 1994. 

4. Eckbreth, Alan C. "Laser- Induced Fluorescence Spectroscopy 
(LIFS)." Laser Diagnostics for Combustion Temperature 
and Species . Abacus Press. Cambridge, MA. 1988. 

5. Goss, L P; Chen, T H; Trump, D D; Sarka, B; Nejad, A S. "Flow- 
Tagging Velocimetry Using UV- Photodissociation of 
Water Vapor." AIAA paper 91- 0355. Reno, Nevada. 
January 7- 10, 1991. 

6. Gruber, M R; Nejad, A S. "Supersonic Combustion Research 
Laboratory." Experimental Research Branch. Wright- 
Patterson AFB, OH. January 1993. 

7. John, James E A. Gas Dynamics . Allyn and Bacon, Inc. Boston, 
MA. 1984. 

8. "The Pocket Ramjet Reader." Chemical Systems Division. United 
Technologies. Sunnyvale, CA. 


44- 20 




FREQUENCY CONVERTOR REPAIR 


Nick DeBrosse 


Final Report fors 
Summer Research Extension Program 
Wright Laboratory 


Sponsored by: 

POTX 

Wright Patterson Air Force Base 


July 1993 






FREQUENCY CONVERTOR REPAIR 


Nick DeBrosse 

Abstract 

The Compressor Research Facility (CRF), located in Wright 
Patterson Air Force Base, studies and records the behavior of full 
scale, multistage, single pool, axial flow compressors and small 
fans. The 44,00 horsepower, 6,900 volt frequency convertor 
contributes a vital role in CRF's overall operation. If the 
frequency convertor should malfunction, CRF's compressor testing 
would be terminated until the frequency convertor would be once 
again operational. Therefore, proper annual preventive maintenance 
is mandatory. 


45-2 








FREQUENCY CONVERTOR REPAIR 
Nick DeBrosse 


During the annual preventive maintenance, a meggar check (a 
method to check electrical isolation characteristics) was performed 
on the frequency convertor. When tested, the machine read zero 
resistance with respect to the ground. This reading was not within 
previous specifications for this electrical machine. Therefore, 
the technicians with my help went through a process of trying to 
isolate the cause of the electrical anomaly. Bearing pedestals #1, 
#4, #3, #2 [1] were cleaned up in that order. The bearing 
pedestals were raised by lifting the shaft through the bearing 
clearance and then lifting the pedestal whatever distance required 
for shim removal. All shims were in very bad condition. The shims 
were coated with an oily residue. They also appeared to be 
corroded and ill-fitted to the pedestals. The embossed serial 
numbers and rough edges on the shims hampered proper positioning. 
Although the phenolic sheets meggared o.k., they were in very poor 
condition and should have been replaced. With the amount of oil 
and water found in the shims, the phenolic sheets were probably 
saturated in oil as well. 

After all the shims were cleaned and reinstalled, all water 
lines and temperature probes removed, the machine meggared at 
approximately 55 Kohm. With the 500V range on the meggar the value 
in 1968 was 300 Kohm but 3.3 Kohm in 1982. 


45-3 




With all of the water lines hooked up and the temperature 
probes installed the one minute reading was 47 Kohm, but after five 
minutes the reading was 53 Kohm. We then started testing all of 
the motors and pedestals separately. The following results are 
from those tests. 

South motor, east side [1] 

South ring to ground: 1 and 5 minute readings - 46 Kohm 
North ring to ground: 1 minute reading - 46 Kohm 
North ring to shaft: instantaneous reading - 80 Kohm 
South ring to shaft: instantaneous reading - 80 Kohm 

#2 Bearing Pedestal [1] to ground read about 47 Kohm for a one 
minute reading. 

North motor, east side [1] 

North ring to ground: 1 minute reading - 45 Kohm 
South ring to ground: 1 minute reading - 45 Kohm 
South ring to shaft: instantaneous reading - 80 Kohm 
North ring to shaft: instantaneous reading - 80 Kohm 

#3 Bearing Pedestal to ground read 50 Kohm for a 1 minute 
reading. 

#4 Bearing Pedestal to ground read 50 Kohm for a 1 minute 
reading. 

North brush holders between frequency convertor and bearing #4 
to ground read 12 Mohm for all holders. 1000+ Mohm for slip rings. 

Bearing #4 to ground, with water flowing through the pipes, 
read 17 Kohm. 


45-4 




30 second reading on the brush rings between the frequency 
convertor and bearing #4 were 650-800 Mohm. 

Shaft to brush holders read 5.5 Mohm. 

Shaft to slip ring read 1000-2000 Mohm. 

At this point everything looked all right and ready to run f so 
we began the long process of reassembly. A run was made the day 
after everything was assembled with no apparent problems. oil 
tests were taken immediately after the run to see if the oil was a 
part of the problem, besides an insignificant high lead count 
nothing seemed to be wrong. A final meggar reading was taken the 
day after the test and it read about 10 Rohm. 




FREQUeUct - convert^ 
if^Ovo uf 
5\^An.5 KFm 


yc wr°(L. 
55*00 i/p 

_ lyy/^z EPm 

] 3 *j 4 f:yW 


J 7 c h^tok 
5fov UP 
137/2.82. £Pn 
li-t/aijsif 

EAST ENP 
rd^i Biu 


VC CraHePATOR, 

WooW 
3*0 jePM 


vc QeneRATot^ 
HMo KW 
200 RPH 




£§ 

gees 




9 //‘// < 30 


Wt 


l^p BJ Vh 

W55T £Np 


Ac hotc?r 
WP 

3flt> RfX 















LU 

> 

(Z 

Q 

O 

OC 


< M 

o: UJ xa 
CD -J _!_ 

< W =j 
^ Q | , 

yljlt 

i* 8 >§ 

S E 

LU LU 



CADRE 
























David B. Hartsock 
Apprentice 
Operations Group 


Ccrtpressor Research Facility 
1950 Fifth Street 
WPAFB Dayton, OH 45433-7251 


Final Report for: 

FDL's Sumner Apprentice Program 
Aero-Prcpulsicn Laboratory 


JUly 1993 




During ray strainer job at the Compressor Research Facility (CRF) I was 
involved with the Mechanical Drive System. Die CRF is presently upgrading 
their drive system. This allowed me to do an intricate study of the CRF's 
Mechanical Drive System. The majority of my job I was studying up on the 
design requirements so I would be properly prepared when we n eed e d to make the 
new designs for the drive system. 

PURPOSE OF SYSTEM 

The Mechanical Drive System interconnects the existing 30,000 HP drive 
motor and the driven equipment of the CRF to transmit power and provide shaft 
rotation ranges to match those required far test articles. 




The Mechanical Drive System consists of the existing drive motors, speed 
increasing gearboxes, turning gear, support bearings, inter connecting shafts 
and couplings. 

A combination of one low speed gearbox in series with one of three high 
s peed gearboxes containing fixed ratios will be used in order to cover the 
operating range. For speeds below the minimum drive motor speeds listed 
above, the horsepower will decrease no more than proportionally to the cube of 
the speed. 


46-2 



PggqHffireE CRITERIA 


The design of this system assures that the following criteria will be 
satisfied: 

The speed increasing gears will operate over their design speed and power 
range with minimim objectionable noise an vibration. 

The entire test stand including the drive motors, gears bearings, and shafting 
will be free of serious resonant vibration either lateral, torsional or axial, 
throughout the entire operating speed range. This requirement applies to all 
operating test conditions including cycling tests of the compressor for which 
the loads may be varied from full load to 20% load and back to full load at 
mavimim facility acceleration/deceleration rates. The time between successive 
cycles may be varied to avoid potential resonance effects. 

The entire drive system is designed to withstand, without damage, a minimum of 
105% of the maximum operating speed. 

All partitions, cover plates and other parts of the machinery will be free 
from resonant vibration arising from either the drive system or the test 
compressor. 


The drive system gearing is designed to have an expected service life of at 
least 5,000 hours under the most arduous operating conditions. 


46-3 





The drive system will have a minimal backlash to provide far smooth transition 
from drive to brake of the test article. 

Drive system components will be protected from test article induced failures 
by the use of a shear section located in the final shafting which drives the 
compressor. The shear shaft will be supported by bearings located at the 
shear notched section to restrain fractured components. 

The entire drive system will be designed to accommodate a future expansion of 
the drive capability to a dual drive configuration. The second drive will 
power the front or fan spool of dual spool compressors. Space is provided at 
the input side of the high speed gearboxes to accommodate and provide access 
to the additional drive. 

Gearbox sets are designed to provide as much speed overlap at the ratio change 
points as can reasonably be obtained. 


The drive system gearing is designed to operate with both clockwise and 
counter clockwise input rotation. Output rotation will be in the same 
direction as the input rotation in either case. 


Gear assemblies will be given a full factory speed test, and oil flow, gear 
losses and temperature rise of bearings from the test will be recorded. 
Overspeed tests in both directions at 105 percent speed far five minutes will 
also be per f ormed at the factory as a minimum. 


46-4 





A variable speed, electric motor powered turning gear will be provided in the 
drive system far turning the drive train during cool down, inspection, ard 
coupling alignment periods. Rotation in either direction will be possible, 
and the motor will be normally disengaged from the drive when not in use. 

The drive system design includes an electric motor for starting the main 
synchronous drive motors. The starting motor will be coupled through a clutch 
and chain drive, and turn the main motor shaft to 10 rpm in either 
direction. The time to accelerate the system to 10 rpm will not exceed 20 
seconds. 


All mechanical parts will be designed for operation over an ambient 
temperature range of >20 to +110 degree F. The equipment will neet 
specifications with the relative humidity as high as 100% and as high as 95% 
without condensation. 

All bearings used in the drive system are designed to allow there removal and 
replacement with minimum disturbance of the associated shafting. 

Guards will be installed around all rotating equipment to prevent accidental 
contact with moving parts by operating and maintenance personnel. 

Seeds are provided at each shaft location which extends outside of the casing 
to prevent oil leakage throughout the design speed range of the shaft. 


46-5 




The design approaches available far the drive system involved 
considerations of the gearboxes, types of couplings, pe d estal bearing 
locations, and drive shafting. The selection of the gearbox arrangement had 
the greatest overall influence on the configuration of the rest of the drive 
system. The location of the pedestal bearings, and the lengths of the 
shafting in particular were affected by the gearbox a r rangement. The types of 
couplings, bearings, and shafting used are depe n dent on the application; i.e. 
speed, horsepower, and alignment requirements. The design of the shafting and 
couplings which connects the test compressor to the Mechanical drive system 
jackshaft will be designed by the test article vendor. 


Three design approaches were available, based on gearbox considerations. 
One used the reconditioned DIT60 and three additional interchangeable sets of 
increaser gears. Another approach used a replacement far the existing DTT60 
and three additional interchangeable sets of increaser gears. The third 
approach eliminated the use of the DIT60, and replaces it with a set of 
interchangeable speed increaser gearboxes. 

In each case the remainder of the drive system had to meet the same 
performance requirements for shafting, couplings, and bearings although the 
particular arr a ngement and number of individual components varied for each 
configuration. 


COUPLINGS 







The couplings interconnect the drive and driven equipment of the facility 
and allow for misalignment and axial expansion of shafting and housings. Due 
to the high power and speed requirements, only two types of couplings are 
suitable far the application; gear tooth or diaphragm type. The number of 
sets of couplings required will be in accordance with the shafting 
requirements far the particular speed and torque ranges. The performance 
criteria established far the total drive system is applicable to the 
couplings. In addition, the couplings will also meet the following criteria 
regardless of their type: 

Couplings will operate over the CRF power and speed range without 
deleterious vibration and axial oscillations. 

Couplings will be capable of interchange without disturbing the fixed 
elements of the drive. 

All coupling components will be supplied in match marked sets to ensure 
dynamic balance. 

The couplings will be capable of operation at a misalignme nt of .002 
inches per inch of coupling length and a minimum axial displacement of .050 
inches. 

Couplings and shafting will be capable of s u pp ort ing the instrument 
conduits. 


The couplings will be of a c o mm e rci a l ly available type. 

46-7 







The choice of coupling size will be decided on the specific application and 
performance required for the coupling location. Flexible diaphragm coupling 
types will be used due to their more favorable operational characteristics. 
Final dimensions and coupling selection will reflect critical speed, torsional 
and axial vibration considerations. 

3&EEBS 


For the gearbox configuration selected with a low speed section of 
shafting and a high speed section of shafting will be required to enocnpass 
the full operating range of the test facility. Bach shafting assembly used 
will incorporate a shear section rated to fail at 90% of the rated torque of 
the selected high speed gearbox. The shear section will be a diaphragm 
coupling assembly, centrally notched. 

individual sets of shafting between the pedestal housing and the test 
article will be required and will be furnished as part of the test article 
system. 


MAJOR INTERFACE DEFINITION 

The following major system interfaces have been considered in the design of 
this system. 


46-8 




The location of the final gearbox output shaft centerline defines the 
location of the test chamber centerline in both a horizontal and vertical 
direction. The core flow collector will not heat the drive shaft or its 
supporting structure an excessive amount. There is no requirement far the 
drive shafting to be supported by test chamber structural members. 


In the event that an out of tolerance condition of any of the drive 
system components is detected, the Facility Control System will initiate 
proper action commensurate with the degree of severity of the condition 
detected. 


The existing foundation will be used and a new structural base will be 
mounted on it. The existing gearbox and pedestal shaft cover are redundant to 
the new facility. 


The existing lubrication system will furnish 1,000 rpm of filtered oil to 
the entire drive system at a temperature of +100 degree F to +120 F. The 
supply pressure will be increased from 10 psig to 25 psig for all components 
of the drive system. 


46-9 



Oil piping connections will be located to minimize piping runs to the 


existing lube oil system and still permit access to and around the drive 
system components. 




Instrumentation interfaces include vibration pickups, KID'S, and speed 
indication means located on the final output shaft. Provisions are made far 
mounting slip rings to obtain rotating measurements from the test article. 

SERVICE AIR 

Service air connections currently used by the OTT-60 will be updated as 
necessary. 


Hie Mechanical Drive System will interface with the Hydraulic System as 
required far proper operation. 


POOLING WATER SYl 




Interface of the Mechanical Drive System directly with the cooling water 
system will not be required. Pooling of the drive system components will be 
per f ormed by the lubricant. Hie lubricant will then be cooled in a 
water-to-oil heat exchanger in the Drive Lube Oil System. 





Interface of the Mechanical Drive System with the Test Article will be by 
means of test article peculiar shaft and coupling assemblies, one aid of which 
mates with the facility coupling and the other with the Test Article. 
Provisions will be made in the design of the test article system to balance 
the thrust applied to the test article drive shaft in this typical turbine 
engine operation. 

This is what I did and learned over the eight week period that I 
worked at the Ocnpressar Research Facility. Although the process of what the 
CRF is doing will not be done until the future, I feel that I have done 

an anple job of helping the CRF complete their project. 




TRANSVERSE INJECTION STUDIES INTO A MACH 2 FREESTREAM 


Melanie L. Hodges 
Research Apprentice 
Advanced Propulsion Division 


Wright-Patterson Air Force Base 
Area B, Building 18 
WL/POPT 

Wright-Patterson AFB, OH 45433-6563 


Final Report for: 
Summer Research Program 
Wright Laboratory 


Sponsored by: 

Air Force Office of Scientific Research 
Bolling Air Force Base, Washington, D.C. 


August 1993 


47-1 



TRANSVERSE INJECTION STUDIES INTO A MACH 2 FREESTREAM 


Melanie L. Hodges 
Research Apprentice 
Advanced Propulsion Division 
Wright Laboratory 
Wright-Patterson Air Force Base 

Transverse injection studies were performed in a Mach 2 freestream using schlieren photography 
and Mie scattering with carbon dioxide as the injectanL The turbulent structure and penetration 
characteristics of three typical injector geometries were examined. One of these was positioned at a low 
angle to the fre es tream while the others were injected perpendicular to the freestream. Results of the 
schlieren photography revealed typical features of the flow including a bow shock, barrel shock, Mach 
disk, and recirculation zones. The Mie scattering images showed a variety of flow structures akmg the 
boundary between the jet and the freestream. These structures were more evident in the flowfields created 
by the perpendicular injectors than by the angled injector. This preliminary research merits further 
analysis and investigation. 


47-2 




Ack nowfojppffts 


1 would like to thank the following people for their tirng and patience during my aimmw 
research at Wright-Patterson: Cape Lou Carraro, Mark Gruber. Jeffrey Dorter, Capt D ean Fetters, 
Diana Glawe, Charlie Smith, Gary Haines, Tzong Chen, Dave Schommer, Abdi Nejad, Lt. Jeffrey 
Fillmore, Lt James Clegem, and others. 


47-3 




TRANSVERSE INJECTION STUDIES INTO A MACH 2 FREESTREAM 


Melanie L. Hodges 


IrtgifagthHi 

Goals for the National AeroSpace Plane (NASP) include flight velocities in the hypersonic 
regime. In order to achieve such velocities, a fundamental knowledge of the fuel injection, mixing, and 
combustion processes carried out in the combustion chamber is necessary. These processes will take place 
in a supersonic stream and therefore, must occur quickly since the residence time within such combustors 
will be small. Numerous fad injection concepts have been proposed for such combustors, including 
sidewall or transverse injection. This paper focuses on the preliminary investigation comparing the 
turbulent structures and penetration of two typical injector geometries. 

PytarrwH 

Interest in transverse injection p rocesses began in the mid-sixties and has recently resurfaced 
with the increased interest in hypersonic flight and the NASP program. Early studies in this area were 
conducted by Schetz and Billig* and Zukoski and SpaitP. These investigations produced schlieren 
photographs and concentration measurements of the jet flowfiekt Schlieren photographs revealed a bow 
shock wave upstream of the injector exit, rapid turning of the injectant gas, a Mach disk, and recirculating 
flow regions up stre am and downstream of the injector exit These features are represented in Figure 1. 

Recently, Papamoschou, et al.~ used schlieren photography to study the penetration 
characteristics of s u perso nic transverse jets. Their study examined the effects of changes in the lireestream 
Mach number, static pressure ratio, density ratio, and momentum ratio on the extent of the jet’s 
penetration. Their investigation revealed that penetration of the jet is primarily dependent on the jet-to- 
fi ecs trea m mom e nt u m ratio (Jj given by 


47-4 






ip ^ ) fr**strtam (r pM 2 ) 

fr€€Jtraam 

where p is the gas density, u is the gas velocity, y is the gas's specific heat ratio, p is the static pressure, 
and M is the Mach number. Other results of their investigation included observations that the jet Mach 
number and the jet-to-freestream density ratio have no effect on the e xtent of jet penetration. Based on 
this observation, the present investigati o n co n c e ntrates on the study of sonic jets. 

The effectiveness of a transverse jet for combustion purposes depends not only on the jet 
penetration, but also on fuel/air mixing. This issue was addressed by Zukodd and SpawP. who used a 
probe to obtain concentration profiles within the jet flowfield. These studies suggested the presence of two 
vortices, which played an important role in the jet-to-freestream mixing. More recent investigations have 
utilized nonintrusive diagnostic techniques to study the concentrations in various flowfields. Planar laser 
induced fluorescence (PLIF), one of the more commonly used of these terhniqnes was applied by Hollo, et 
al. 4 in their examination of the complex flowfield between two staged transverse injectors. 

In an effort to improve the operating characteristics of scramjet fuel injectors. Mays, et al. 5 
experimented with an injector oriented at a low transverse angle to a Mach 3 frecstream. Their work 
showed that this injector geometry operated in an overpressurized mode results in a lower total freestream 
pressure kiss than the same jet injected perpendicular to the freestream. The same injector operated in a 
pressure matched condition produced slower mixing and redheed penetration compared to the 
overpressurized case. 


The new supersonic combustion facility at Wright Laboratory offered a perfect environment for 


the transverse injection studies reported on in this paper. The combustion tunnel and the fuel injectors 
used fat study, along with the diagnostic techniques employed in these experiments are discussed in the 
following p aragrap hs. For further details concerning the combustion tunnel, see Gruber and Nejat^. 


47-5 




The state of the ait research facility at Wright Laboratory is a result of a collaborative in-house 
design effort The preliminary planning began in 1990 and the facility was completed by the end of 1992. 
It offers a wide range of flow capabilities, including: 

• Variable Mach number capability (1.5 to 3.0) 

• Continuous flow operation 

• Stagnation conditions up to 400 psig at 1660 *R 

• 5-inch by 6-inch test section 

• Peak air flow rate of 34 Ib^sec 

• Optical access to test section from four sides including the end. 

The results of their efforts are shown in a schematic of the facility (Figure 2). Tim test section allows 
optical access on three sides and the end for the use of nonintrusive diagnostic techniques. The windows 
are made of fused silica, which allows except i onal UV laser penetration for diagnostic techniques such as 
FUF of acetone or OH-Flow-Tagging Velocimetry as described by Chen, et al. 7 . The test section also 
c ontain* five access ports on the bottom wall. These ports allow various test articles or injectors to be 
easily installed for experiments (see Figure 3). 

For this investigation, several injector geometries were available, including a circular jet injected 
perpendicular to the flow (a), a circular jet injected at 15* with the flow (b), and an elliptical jet injected 
perpendicular to the fre cst ream . (c) All injectors have the same cross sectional area (perpendicular to the 
jet flow direction) of0.049 square inches. Therefore, the mass flow rate through each injector is the same 
at a given pressure, allowing for a proper comparison of each injector's performance. Consequently, 
injector (a) has an exit diameter of 0.2S inches, and the elliptical exits of injectors (b) and (c), though 
similar in geometry, are different in exit area. Figure 4 offers an illustration of the three injector 
configurations. 

The diagnostic methods used in this study include schlieren photography and Mie scattering. 
Schlieren photography served as the preliminary source for flowfield characteristics This technique relies 
on density d i f fe rence s within the injector flowfield Parallel light was passed through the test section and 
focused on a knife edge. Regions of the flow with different density caused the light to be bent The knife 


47-6 


edge cut off the light that was bent toward it and these regions of the flow ap pe ar ed dark on the film. A 
schematic of the schlkren system used for these experiments is shown in Figure 5. 

Mie scattering was used in addition to schlieren photography to identify positioning of injectam 
in the freestream. Heated carbon dioxide was injected into the test section and allowed to mix with the 
freestream A laser beam was passed through cylindrical and sp heric al lenses to create an ultraviolet laser 
sheet at a wavelength of 266 nm. An illustration of the laser configuration is shown in Figure 6. This 
sheet was then passed through the top window of the test section, parallel to the side windows. Naturally 
occurring water vapor within the test section reflected the laser light and was collected through the side 
window using a CCD camera. These digital images were then stored on a personal computer for analysis 
at a later time. 

Reaafts — DiacnssiQB 

■aaasuum^i i Mm xml 

The diagnostic techniques used in this investigation allowed a variety of flow characteristics to be 
examined Schlieren photography was used to observe the general characteristics of the jet's flow region 
during continuous operation. The photographs taken of each injector fkwfidd revealed steady bow 
shocks ahead of the jet They also illustrated the band shock and Mach disk for each jeHo-fieestream 
momentum ratio. These images were used for insight into the major flowfidd characteristics. 

Still-framed images of the various injector flowfields were obtained using Mie scattering. 
Figures 7-9 illustrate the results of the Mk scattering technique In these images, light regions represent 
the fr ees tream (and in some areas solidified carbon dioxide), while gaseous carbon dioxide is represented 
by dark areas. In each image, the freestream flows from right to left. Each image was collected at a jet- 
to-fieestream momentum ratio of 3. 

Figure 7 depicts the flowfidd created by the circular jet (a) injected perpendicularly to the 
fre es tream . This image is characterized by the presence of large scale turbulent structures created at the 
jet/fteestream boundary. The development of these structures indicates that the carbon dioxide has a 
higher velocity than the fr ee s t ream air at the boundary. The barrel shock and Mach disk of the 
ovetpressurized jet are also evident in the image. The band shock region is seen to be a light region in 


47-7 



this image. It is believed that the light region appears due to frozen carbon dioxide rather than freestream 
air. The gas accelerates as it exits the injector. This acceleration creates a decreasing pressure and 
temperature in the carbon dioxide resulting in a phase change from gas to solid. As the COj passes 
through the Mach disk and slows to subsonic velocity, sublimation occurs and the jet appears dark once 
again. In addition to these features, the bow shock is visible and carbon dioxide is seen to be in the 
recirculation region upstream of the injector. 

Figure 8 illustrates the flowfield of the low angle injector (b). This image also reveals the 
presence of a bow shock, however the barrel shock is absent. The bow shock is situated at a lower angle 
than in the previous case; this indicates a weaker shock and thus, lower total pressure loss of the 
freestream. There is no apparent recirculation of COj upstream of the injector in this case. The turbulent 
structures at the jet/freestream boundary are comparably smaller and less frequent than those found in 
Figure 7. However, the penetration of the jet is consistent with that of the circular injector (a). The bright 
regions in the jet suggest the presence of solid carbon dioxide particles. The low angle of the injector may 
cause the COj to accelerate fester than the other injector gemoetnes. 

The flowfield created by the elliptical injector (c) is shown in Figure 9. The structures found in 
this image are similar to those displayed in Figure 7 in that they are large in scale, and they also seem to 
occur more frequently than the small scale eddies of injector (b). The bow shock shown in the figure is 
steeper than the ones created by both of the previous injectors. As in Figure 8, there is no evidence of 
carbon dioxide in the upstream recirculation region of this injector. In addition to these characteristics, 
the barrel shock for this case shows evidence of the same phase change which seems to occur in Figure 7 
and is created at a steeper angle to the fr ee stream than the other perpendicular injector (8). 

This study r epres e nts a preliminary investigation into the behavior of three transverse injector 
geome tri es supplied with carbon dioxide. Results of the schlienen photography revealed typical features of 
the flow inHiirfing a bow shock, band shock, Mach disk, and recirculation zones. The Mie scattering 
images showed a variety of large and small scale flow structures along the boundary between the jet and 


47-8 



the freestrcam These structures were more evident in the flowfields created fay the perpendicular injectors 
than that of the angled injector. Penetration of the three jets appeared to be similar which may suggest 
that the jet-to-freestream momentum ratio is a useful term for the prediction of the jet's penetration. This 
research merits further analysis and investigation. 




1. Schetz, J.A and F.S. Billig, "Penetration of Gaseous Jets Injected into a Supersonic Flow," AIAA 
Journal. Vol. 3, No. 11, pp. 1658-1665, 1966. 

2. Zukosia, E.E. and F.W. Spaid, "Secondary Injection of Gases into a Supersonic Flow," AIAA 
Journal. Vol. 2, No. 10, pp. 1689-16%, 1964. 

3. Papamoschou, D„ et al., "Observations of Supersonic Transverse Jets,” AIAA Paper 91-1723. 
1991. 

4. Hollo, S.D., et al., "Characterization of Supersonic Mixing in a Nonreacting Mach 2 
Combustor," AIAA Paper 92-0093, 1992. 

5. Mays. R.B., et al., "Low Angle Injection into a Supersonic Flow,” AIAA Paper 89-2461,1989. 

6. Gruber, M R. and AS. Nejad, "Development of a Large-Scale Supersonic Combustion Research 
Facility; Volume 1-Design and Fabrication," Wright Laboratory Report No. WL-TR-93-2032, 
1993. 

7. Chen, T.H., et al., "Multi-Zone Behavior of Transverse Liquid Jet in High-Speed Flow,” AIAA 
Paper 93-0453, 1993. 


47-9 




Bow Shock 



Figure 1 Flow Regions of a Transversely Injected Jet 





















47-12 


















Projection Screen 



47-14 


Focusing Mirror 






Laser Beam 



5 

















Figure 8 Mie Scattering Image of Injector (b) 







*• f M\ ^14 *<i f 

'Vi.*'..;*/, #.y .v-* * 



Figure 9 Mie Scattering Image of Injector (c) 
47-18 








USAF JET FUEL THERMAL STABILITY TESTING 


Daniel L. Prevost 
Student 

Kettering Fairmont High School 
3301 Shroyer Road 
Kettering, OH 45429 

Final Report for: 

AFOSR Summer Research Program 
Wright Laboratory 

Sponsored by: 

Air Force Office of Scientific Research 
Wright Patterson Air Force Base, Dayton, OH 

August 1993 


48-1 





USAF JET FUEL THERMAL STABILITY TESTING 


Daniel L. Prevost 
Student 

Kettering Fairmont High School 

Abstract 

Fuel thermal stability has always been a problem for the designers of aircraft 
engines who must balance component life against high temperatures, high heat fluxes and 
the thermal stability of fuels. Due to newer high performance and fuel efficient engines that 
stress fuel beyond its thermal stability limits, a large amount of maintenance time and 
energy is spent removing and cleaning fouled nozzles, afterburner spraybars, and 
sprayrings. Current investigations show that future engine systems will stress fuels even 
further. This has lead the Air Force to embark on a program to improve the thermal 
stability of its fuel This program would increase the thermal limits of the fuel by 100°F 
through the use of an additive package. However, before it can be put into use any additive 
package must be thoroughly tested in a variety of systems in a variety of fuels in order to 
maximize its performance. 


48-2 



USAF Jet Fuel Thermal Stability Testing 


Introduction 

For the past 40 years the United States Air Force has operated the majority of its 
jet planes on a fuel known as JP-4(Jet Propellant-4) while commercial jets have flown on 
Jet A. During the Southeast Asian Conflict combat experience demonstrated that USAF 
aircraft using the highly volatile JP-4 fuel had higher combat losses than US Navy 
aircraft using low volatility JP-5. Also crash data showed that the probability of a 
postcrash rite was almost 100 percent when using JP-4; much higher than with a 
kerosene-based fuel such as JP-S or commercial Jet A. The increased safety of kerosene 
fuels, as compared to wide distillation range fuels such JP-4, was also evident in the 
number of ground handling accidents. 1 Therefore, JP-8 was developed to give the USAF 
a safer fuel that still performed well. To simplify the production of JP-8, the properties 
selected closely match those of commercial fuels. However, due to the differences in 
engine temperature and flight altitude, commercial jet fuel proved not to be thermally 
stable enough to handle the Air Force's differing fuel system. Fuel is used in integrated 
aircraft thermal management systems to cool aircraft subsystems and the engine 
lubricating oil prior to being consumed. All current U.S. fighter aircraft circulate fuel on 
the airframe to match heat loads with available heat sink. These thermal stresses push 
current fuels to temperatures as high as 163°C at the inlet to the main burner fuel nozzles 
and 205°C inside the fuel nozzle passages.^ At these conditions, engine fuel nozzles, 
afterburner spray assemblies and manifolds are plugging, causing increased maintenance 
and cost In some instances fuel degradation changes the spray pattern in the combwion 
chamber of afterburners leading to damage to engine components. 

A large amount of thermal stability research over the years has focused on fouling 
and coking of engine components. Current investigations show that damage goes well 


48-3 




beyond the simple plugging of spray nozzles, to actual damage to the hot section 
components due to improper spray patterns, and on some engines, rumble, a low 
frequency acoustic condition that can damage engine parts. 2 Advanced fighters that will 
be produced in the late 1990's will require even more cooling resources. Fuels will be 
subjected to higher temperatures, heat fluxes and multiple heating and cooling cycles as 
fuel is used as the primary cooling medium for aircraft engines (Figure 1). It is 
anticipated that these aircraft using current fuels, such as JP-8, will require increased fuel 
system maintenance to replace fouled components. Anticipating these problems the 
USAF initiated a program in 1989 to improve the thermal stability of JP-8. Two 
approaches were originally considered; one would be to develop a new refinery 
specification for a more thermally stable JP-8. This approach was discarded early on 
since a new fuel would be costly to obtain. Instead the approach chosen by the Air Force 
was to develop and additive package that would improve the thermal stability by 100°F 
without costing more than $.001 per gallon, referred to as "JP-8 + 100". Current research 
indicates that the additive package will consist of four main additive types: antioxidants, 
metal deactivators, detergents, and dispersants. 2 The Air Force contacted major additive 
manufacturers and oil companies to supply additives for further study. 

Each additive received from a manufacturer goes through a complex screening 
and testing process (Figure 2).2 Each additive is put through a series of tests in a variety 
of different fuels to determine its anticipated effectiveness. This is how most of my time 
working at Wright Patterson AFB has been spent. I have had the opportunity to become 
familiar with most of the tests run here. 

MCRT 

The Micro Carbon Residue Test (MCRT) determines the amount of carbon 
residue formed after evaporation and pyrolysis of petroleum materials under certain 
conditions and is intended to provide some indication of the relative coke forming 


48-4 



FUEL COOLING 


AIRFRAME 

RECIRCULATION 


- -\ 

RAM AIR HEAT EXCHANGER 
OR 

WING COOUNG 



BOOST 

PUMP 


HYDRAULICS 


GEARBOX 


GENERATORS 


T- 163*C 

INLET TO NOZZLE 





ENGINE 

PUMP 


FUEL/LUBE OIL 
HEAT EXCHANGER 


T« 80-120*0 


AIRFRAME HEAT EXCHANGER 


Figure 1. Fuel System, Current Fighter Aircraft 



Figure 2. Additive Screening Approach 


48-5 












tendency of such materials. The MCRT oven consists of a test chamber surrounded by a 
hollow wall containing tubular heaters. Gas flows through this hollow wall into the 
chamber via 12 small holes at the top of the chamber. A circular sample basket 
containing six vials is placed into the chamber where it is continuously purged with air at 
150 cc/min and heated at a constant rate. The fuel boils, oxidizes, and degrades 
somewhat simulating the conditions found in fuel nozzles and afterburner assemblies at 
cancellation or during leakage across the valves. At the conclusion of the test the vials 
are cooled, weighed and the degraded fuel that has been condensed and trapped is 
filtered, dried and weighed (Figure 3). 2 The carbon residue value of the various 
petroleum materials serves as an approximation of the tendency of the material to form 
carbonaceous type deposits under similar degradation conditions, and can be useful as a 
guide in manufacture of certain stocks. 3 
ICOT 

The Isothermal Corrosion Oxidation Test (ICOT) has been proven to be very 
effective in screening the relative behavior of the thermal stability additives. In this test 
up to ten samples of fuel are heated in a block heater, usually for five hours. Air is 
bubbled through a glass blower tube. The stressed samples are first visually inspected for 
color and visible particulates, then filtered, dried and the deposition measured 
gravimetrically or by carbon bum-off (Figure 4).2 
RULER AND PERFECT 

A device that has been developed to study the effectiveness of antioxidants in 
fuels is the Remaining Useful Life Evaluation Rig (RULER). The RULER technique is a 
cyclic voltametric test that is performed with a commercially available voltammograph 
equipped with a glassy carbon disk working electrode, a platinum wire reference 
electrode, and a platinum wire auxiliary electrode. The oxdatively stressed fuel sample is 
diluted (1:8) with isopropanol containing an electrolyte. The voltage of the auxiliary 


48-6 







DEPOSIT, MICROGAMS CARBON 


JET A JET A ♦ JFA-5 JPTS JET A -f ADDITIVES 

"JP-8+100- 

Figure 3. Micro Carbon Residue Test (MCR^ Results 



JET A JET A + JFA-5 JPTS JET A + ADDITIVES 


•jp- 8+1 oo- 

Figure 4. Isothermal Corrosion Oxidation Test (ICOT) Results 

48-7 















































































electrode is increased for 0.0 to 1.0 Volts at a rate of O.lV/second. The derivative of the 
current produced by the glassy carbon working electrode is plotted versus scan voltage by 
a microcomputer system interfaced to the voh nomograph. The peaks produced by the 
stressed fuel are integrated and normalized to the peaks produced by the fresh fuel to 
determine the concentrations of the additives remaining in the stressed fuels. 

A variation of the RULER is the Peroxide in Fuel Estimation and Concentration 
Test (PERFECT). In the PERFECT technique, the peroxides are reduced by potassium 
iodide in an acidic medium. An equivalent amount of iodide is liberated and quantified 
by voltammetry calibrated with measured amounts of iodine. 2 The magnitude of the 
peroxide number is an indication of the quantity of oxidizing constituents present. 
Deterioration of turbine fuel results in the formation of peroxides and other oxygen¬ 
carrying compounds. The peroxide number measures those compounds that will oxidize 
potassium iodide. The determination of the peroxide number of aviation fuel is 
significant because of the adverse effect of peroxides upon certain elastomers in the fuel 
systems. 4 
BOCLE 

The Ball-on-Cylinder Lubricity Evaluator assesses the wear aspects of the boundary 
lubrication properties of aviation turbine fuels on rubbing steel surfaces. The fuel under 
test is placed in a test reservoir in which atmospheric air is maintained at 10% relative 
humidity. A non-rotating steel ball is held in a vertically mounted chuck and forced 
against an axially mounted steel ring with an applied load. The test cylinder is rotated at 
a fixed speed while being partially immersed in the fluid reservoir. This maintains the 
cylinder in a wet condition and continually transports the test fluid to the ball/cylinder 
interface. The wear sea* generated on the test ball is a measure of the fluid lubricating 
properties. Wear due to excessive friction resulting in shortened life of engine 


48-8 






components such as fuel pumps and fuel controls has sometimes been attributed to lack 
of lubricity in aviation fuels. 5 

Total Add Number 

Some acids may be present in aviation turbine fuels due to either the acid 
treatment during the refining process or to naturally occurring organic acids. Significant 
acid contamination is not likely to be present because of the many check tests made 
during the various stages of refining. However trace amounts of acid may be present and 
are undesirable because of the consequent tendencies of the fuel to corrode metals that it 
may contact or to impair the water separation characteristics of the aviation turbine fuel. 
The sample is dissolved in a mixture of toluene and isopropyl alcohol containing a small 
amount of water. The resulting single phase solution is blanketed by a stream of nitrogen 
bubbling through it and is titrated with standard alcoholic potassium hydroxide to the end 
point indicated by the color change of the added p-naphtholbenzein solution. Acid 
number is the quantity of base, expressed in milligrams of potassium hydroxide per gram 
of sample that is required to titrate a sample in die solvent to an endpoint using p- 
naphtholbenzein.6 Figure 5 shows the TAN results from a fuel with different 
combinations of additives after having been stressed in the ICOT. 7 The total acid 
number (TAN) is calculated by the below calculations: 

TAN=[(A-B)N x 56.1J/W 

where: 

A = milliliters of KOH solution required for titration of the sample 
B = milliliters of KOH solution required for titration of the blank 
N = normality of the KOH solution 
W = grams of sample used 


48-9 




O CM 


2922 stressed In ICOT (or 5hrs @ 180C 



■ BLANK 3-25-93 
Q 2922 NEAT 

♦ 2922 fuel with additives 2791 
and 2727 

° 2922 fuel with additives 
2791.2727. and Metal 
Deactivator 


0 10 20 30 40 50 60 


ml of .0083N KOH 


Figure 5. Total Acid Number (TAN) Results 










JFTOT 

Static tests do not simulate the actual environments found in aircraft subsystems 
and engines. To accurately simulate these environments, flow tests must be used. Tests 
have been conducted using the Jet Fuel Thermal Oxidative Tester (JFTOT). This test 
method subjects the test fuel to conditions that can be related to those occurring in gas 
turbine engine systems. The fuel is pumped at a fixed volumetric flow rate through a 
heater after which it enters a precision stainless steel filter where fuel degradation 
products may become trapped. The test results are indicative of fuel performance during 
gas turbine operation and can be used to assess the level of deposits that form when liquid 
fuel contacts a heated surface that is at a specified temperature. 8 
Flash Point Test 

Flash point, the lowest temperature at which application of a test flame causes the 
vapor of a specimen to ignite under specified conditions of test, can indicate the possible 
presence of highly volatile and flammable materials in a relatively nonvolatile or 
nonflammable material. For example, an abnormally low flash point on a sample of 
kerosene can indicate gasoline contamination. The flash point is also used in shipping 
and safety regulations to define flammable and combustible materials. In this test a 50 ml 
sample of fuel is placed in to a closed cup and heated at a slow, constant rate (about 
1.0°C/minute). A small flame is directed into the cup every 1.0°C until the flash point is 
reached. The flash point is the lowest temperature at which application of die test flame 
causes the vapor above the sample to ignite.^ 

Existent Gums 

It has been proven that high gum content in a fuel can cause induction-system 
deposits and sticking of intake valves, and in most instances it can be assumed that low 
gum will ensure absence of induction-system difficulties. This test determines the 
amount of existent gum, the evaporation residue of aircraft fuel, in aviation fuel. A 


48-11 




measured quantity of fuel is evaporated under controlled conditions of temperature and 
flow of air or steam. The resulting residue is weighed and reported as milligrams per 100 
ml. Large quantities of gum are indicative of contamination of fuel by higher boiling oils 
or particulate matter and generally reflect poor handling practices in distribution 
downstream of the refinery. 10 
Kinematic Viscosity 

Kinematic viscosity is the measure of the resistive flow of a fluid under gravity. 

In this test the time is measured for a fixed volume of liquid to flow under gravity 
through the capillary of a calibrated viscometer under a reproducible driving head and at 
a closely controlled temperature. The kinematic viscosity is the product of the measured 
flow time and the calibration constant of the viscometer. Many petroleum products, as 
well as non-petroleum materials, are used as lubricants for bearings, gears, compressor 
cylinders, hydraulic equipment, etc. The proper operation of the equipment depends 
upon the proper kinematic viscosity of the liquid. Therefore, the accurate measurement 
of kinematic viscosity is essential to many product specifications. 11 
Conclusion 

Due to the number of fuels, additives, and tests that must be run it is impossible at 
this point in time to make any conclusions about which additives and which fuels are 
good or bad. The JP-8 + 100 plan does call for engine testing to begin in 1995 and in 
service use to begin in 1997. 


48-12 





References 


1. AFWAL-TR-87-2062, "Military Jet Fuels, 1944-1987, "November 1987, Charles R. 

Martel 

2. Harrison, W. E. HI, T. Edwards, S. D. Anderson, "U.S. Air Force Improved JP-8 

Development Program-An Overview," Wright-Patterson AFB, OH. 

3. ASTM D 4530, "Standard Test Method for Micro Carbon Residue of Petroleum 

Products," 1992 Annual Book of ASTM Standards. Volume 5.03. pp 441-445. 

4. ASTM D 3703, "Standard Test Method for Peroxide Number of Aviation Turbine 

Fuels," 1992 Annual Book of ASTM Standards. Volume 5.03. pp 52-54. 

5. ASTM D 5001, "Standard Test Method for Measurement of Aviation Turbine Fuels by 

the Ball-on-Cylinder Lubricity Evaluator (BOCLE)," 1992 Annual Book of ASTM 
Standards. Volume 5.03. pp 757-762. 

6. ASTM D 3242, "Standard Test Method for Acidity in Aviation Turbine Fuel," 1992 

Animal Book of ASTM Standards. Volume 5.02. pp 664-667. 

7. Grinstead, Becky, UDRI, "Total Acid Number Results." March 25,1993. 

8. ASTM D 3241, "Standard Test Method for Thermal Oxidation Stability of Aviation 

Turbine Fuels (JFTOT Procedure)," 1992 Annual Book of ASTM Standards. 
Volume 5.02, pp 644-663. 

9. ASTM D 93, "Standard Test Methods for Flash Point by Pensky-Martens Closed 

Tester," 1992 Annual Book of ASTM Standards. Volume 5.01. pp 28-40. 

10. ASTM D 381, "Standard Test Methods for Existent Gum in Fuels by Jet 

Evaporation,” 1992 Annual Book of ASTM Standards. Volume 5.01. pp 142-147. 

11. ASTM D 445, "Standard Test Methods for Kinematic Viscosity of Transparent and 

Opaque liquids (and the Calculation of Dynamic Viscosity," 1992 Annual Book of 
ASTM Standards. Volume 5.01. pp 154-159. 


48-13 




MY AVIATION FUEL RENDEZVOUS 


Jonathan D. Servaites, high school apprentice under 
2d Lt John M. Garver 


Centerville High School 
500 East Franklin St. 
Centerville, OH 45459 


Final Report for: 

High School Apprenticeship Program 
Wright Laboratory/Air Propulsion Directorate/Fuels Branch 


Sponsored by: 

Research development Laboratories 
ver City, CA 


August 1993 


49-1 





MY AVIATION FUEL RENDEZVOUS 

Jonathan D. Servaites 
High School Apprentice 
Centerville High School 

Abstract 

During my summer tour I participated in research with both sections of the Fuels 
Branch of the Air Propulsion and Power Directorate at Wright Laboratory (WL/POSF), 

Fuel Development and Fuel Combustion. A primary concern of Fuel Development is based 
around the rapidly growing heat loads generated by the engine and aircraft subsystems. 

Fuel Development works with this problem by analyzing aviation fuel’s role as a cooling 
medium for aircraft subsystems. All U.S. aircraft match heat load with available heat sink 
of the fuel. However, with increasing temperatures, greater thermal stability and heat sink 
capability are needed to help curb fuel fouling in current systems and to provide further 
security in the future. Therefore, Fuel Development has established the "JP-8+100" 
program that offers a JP-8 fuel with 100°F (56°C) improvement in thermal stability. This 
task is to be accomplished by the means of an additive package. The additive package 
should cost less than $0,001 per gallon and would significantly reduce the amount of engine 
and subsystem maintenance that is currently needed. 1 During the latter part of my summer 
tour, I also spent time working in Fuel Combustion. The primary objective of this program 
is to provide the fundamental understanding of gas and liquid fueled combusting flows 
needed to develop gas turbine combustor design models that result in errors of less than 5% 
in predicted flow field parameters such as temperature, velocities, and species 
concentrations. In the program’s hope of bettering methods for designing gas turbine 
combustors, a "vaporline" visualization technique was developed where water droplets, 
vapor from the droplets, and their interaction with the carrier gas are observed 
simultaneously. 2 


49-2 







MY AVIATION FUEL RENDEZVOUS 
Jonathan D. Servaites 


Introduction 

As in most cases, compromise is necessary in the analysis of fuel thermal stability. 
Engine designers have been forced to balance the significance of component life with high 
temperatures, high heat fluxes, and the thermal stability of a fuel (all three of which affect 
the aircraft’s performance). As fuel is circulated throughout the aircraft to cool airframe 
and engine components, it undergoes dramatic thermal stress which can affect aircraft 
performance. Figure 1.' Thermal stability research programs over recent years have 
continued to focus on the importance of limiting fuel fouling and coking of engine 
components. Meanwhile, a number of test devices have been developed to analyze the 
effect of JP-8+100 additive packages. Figure 2. 1 Many of these tests simulate the 
environment of the actual aircraft in an effort to determine an additive’s ability to reduce 
the amount of residue left behind after fuel circulation. Test devices employed by Fuel 
Development include the following: the Micro Carbon Residue Test (MCRT), the Peroxide 
in Fuel Estimation and Concentration Test (PERFECT), the Isothermal Corrosion Oxidation 
Test (ICOT), the Total Acid Number Test (TAN), the Leco RC-412 Surface Carbon 
Analyzer, and the Ball-on-Cylinder Lubricity Evaluator (BOCLE). 

In the past, combustor development has been a gradual, evolutionary process with each 
generation being only slightly different from the preceding one. However, in meeting 
future Air Force engine performance requirements, the process has become revolutionary 
rather than evolutionary. Computational Fluid Dynamic and Chemistry (CFDC) models 
used by the engine companies in developing combustors for Air Force weapon systems 
offer the potential of exploring revolutionary combustor concepts in an economical and time 
efficient way. 2 Fuel Combustion has set out to better understand and to improve CFDC 
design model capabilities. Several tests have been established to simulate the combustor 
environment with the intention of fulfilling this goal. This understanding of revolutionary 
combustor concepts will be needed by future Air Force programs in the 21st century. 

49-3 




AIRFRAME 

RECIRCULATION 


FUEL COOUNG 


RAM AIR HEAT EXCHANGER 
OR 

WING COOUNG 


T» 163*0 

INLET TO NOZZLE 





►- •. UITEV—l• *- 

it— 



MAM 

FUEL 

TANKS 

T« 66*0 
MAX 


BOOST 

PUMP 


HYDRAULICS 

GEARBOX 

GENERATORS 


ENGINE FUEL/LUBE OIL 
PUMP HEAT EXCHANGER 


Ta 80 • 120*C 


AIRFRAME HEAT EXCHANGER 


Figure 1: Fuel System, Current Fighter Aircraft 


- _ SUPPLY 

ADDITIVE MANUFACTURERS P“|J INIT.AL A^mVEJCR|ENINQ 


MODIFICATION 

SwNEEDEDT^X 


SHOW 

PROMISE? 


ADDITIVE SCREENING 
IN SEVERAL FUELS 


DETERMINE 'UNIVERSAL!TV 
OF ADDITIVES 
HLPS/MCRT/ICOT/QCM 


FORMULATE PACKAGES/ 
OPTIMIZE CONCENTRATIONS 


SELECT MOST PROMISING 
ADDITIVES FOR IMMEDIATE 
EVALUATION 


REJECT 


LARGER SCALE FLOWING TESTS nltmay»3 


COMPATIBILITY STUDIES 
WITH CURRENT AND 
FUTURE ADDITIVES 


COMPATIBILITY STUDIES 
WITH CURRENT AND 
FUTURE MATERIALS 


REJECT 


PROBLEMS? 


NO COMPONENT TESTS 
’-* IN MODIFIED RIGS 


FY93W4 


*RELIMINARi 

SPEC 


RSFSS 


Figure 2: Additive Screening Approach 









The MCRT, a static test that is being used to screen additives in the JP-8+100 
program, helps determine the amount of carbon residue after the fuel has been thermally 
stressed. In this procedure six small vials are filled with approximately SO milliliters of 
fuel each, placed in a furnace, and heated to 250°C at a constant rate of 8.3°C per minute. 
Air is purged through the chamber at a rate of ISO milliters per minute. During the heating 
process the sample undergoes extreme coking reactions. 3 The environment roughly 
simulates the conditions found in fuel nozzles and afterburner assemblies at cancellation or 
during leakage across the valves. 1 At the conclusion of the test, the vials are cooled, 
weighed, and the amount of residue determined by difference. The degraded fuel which 
has been condensed and separated is filtered, dried, and weighed allows the user to 
determine the amount of gum left behind. Figure 3. 4 The MCRT illustrates the tendency 
of the sample to form carbonaceous type deposits under similar conditions. 


1 3 a £•>;< i »j 4 iciau a wasmctlffilKlJi HHKIiSSI oBE I m piaiaaaiail 


The PERFECT covers the determination of the peroxide content of aviation turbine 
fuels. In the PERFECT technique, the peroxides (especially hydroperoxides) are reduced 
by potassium iodide in an acidic medium (acetic and hydrochloric acid). An equivalent 
amount of iodide is released and measured by voltammetry calibrated with known amounts 
of iodine. The results are reported as milliequivalents of peroxide per liter of fuel 1 . 
Although the PERFECT is not a perfectly consistent and flawless test, it does provide 
helpful information concerning the magnitude of the peroxide number of JP-8+100 test 
samples which indicates the quantity of oxidizing constituents present. Deterioration of 
turbine fuel results in the formation of peroxides and other oxygen-carrying compounds. 
These compounds tend to have adverse effects upon certain elastomers in the fuel system. 5 


mon 11 a * 3 f.y a m i i m m m *1 mi mi n 


The ICOT measures the oxidation or thermal stability of liquids by subjecting them to 
temperatures in the range from 50°C to 375°C in the presence of air, oxygen, nitrogen, or 
other gases (for Fuel Development’s purposes, air is used) at rates of 1.5 to 13 liters per 







49-6 


MG/Gof fuel 

O — W Co & Oi 0“ 


2963 NEAT 


JFA-5 


2791 


2791&2727 


Metal 

deactivate* (10 
MG/L) 


Metal deativator 
.2791.2727 


Metal 

deactivator (20 
MG/L) 


2799 



VI 

H 


Figure 3: Evaluation of the effects that various additive packages have 
had in the MCRT ( 2 ) 250°C for 3 hours with 9 L/hour air flow. 








hour. The sample is subjected to thermal or oxidative degradation or both. The gas may 
also be bubbled through the liquid to provide agitation or to promote oxidation. The ICOT 
provides helpful versatility which is required to conduct oxidation or thermal stability tests 
on liquids using a wide variety of test conditions. The test is also utilized for the purpose 
of stressing fuels that will be further tested for acidity (see Total Acid Number Test 
below). 6 

TO T AL. ACI D N UM BER TEST (T AN ) 

After heating, the acidity of fuels increases, thereby producing the threat of corrosion to 
metals that the fuel comes in contact with. The purpose of the TAN test is consequently to 
measure the acidity in aviation turbine fuel after it has been thermally stressed in the ICOT. 
In this test a mixture of toluene and isopropyl alcohol containing a small amount of water 
(1000 mL, 990 mL, and 10 mL respectively) is blanketed by a stream of nitrogen gas in 
order to displace surrounding carbon dioxide. 7 This solution is called "the blank." Using 
an automatic titrater, the blank is titrated with 0.083 N potassium hydroxide (KOH). Next, 
the same process is repeated, but this time a fuel sample is added. The purpose of adding 
the toluene/isopropyl alcohol/water solution is to dissolve the fuel sample (its contribution 
to the fuel solution acidity is later subtracted). In calculating the acid number one uses the 
following equation: 

TAN**(mL of sample-mL of blank) x 56.1 (atomic mass of KOH) x 0.083 (normality of KOH) / mL afKOH. 

LECO RC-412 SURFACE CARBON ANALYZER (LECO) 

The LECO is another test utilized to determine the source of several types of carbon 
content. By analyzing the sample in an oxidizing atmosphere, all forms of carbon (except 
for some carbides such as SiC) are converted to carbon dioxide. Organic forms produce 
water and CO 2 . Consequently, organic compounds can be found by determining coincident 
peaks in H 2 0 and C0 2 . This device employs a furnace control system which allows the 
temperature of the furnace to be stepped or ramped. From there various sources of carbon 
can be differentiated by the temperature at which they volatize or oxidize. The LECO’s 


49-7 





mg solid/L fuel 


2963 NEAT 


JFA-5 


2791 


2727 


2791 8(2127 


Metal deactivator 
(10MG/L) 


Metal deativator, 
2791,2727 


Metal deacth/ator 
(20MG/L) 


Metal deactivator 
(20). 2791 


Metal deativator 
(20.2791,2727 


2799 


8 



Figure 4: Evaluation of additive packages in the ICOT at 180°C for 5 hours with 1.3 L/hour 






Figure 5: Evaluation of the acidity of various additive packages 












capabilities are very helpful in determining a fuel sample’s tendency to leave behind carbon 
residues. After fuel is circulated through simulated aircraft tubing, the LECO analyzes the 
fuel residue and provides the necessary data in determining the sample’s thermal stability 
qualifications. 9 

BALL-ON-CYLINDER LUBRICITY EVALUATOR (BOCLE) 

The previous tests discussed have dealt with a fuel’s effect on the engine and aircraft 
subsystems due "remains" left behind; however, fuel residue is not the only threat to the 
aircraft’s well-being. With each additive package, a sample possesses a certain level of 
lubricity. In the case of this test lubricity is defined "in terms of a wear scar, in 
millimeters, produced on a stationary ball from contact with the fluid wetted rotating 
cylinder operating under closely defined and controlled conditions.'" 0 The BOCLE simply 
assesses the wear aspects of aviation turbine fuels on steel surfaces. The test method calls 
for the fuel under test to be placed in a test reservoir in which atmospheric air is 
maintained at 10% relative humidity. . non-rotating ball is held in a vertically mounted 
steel ring with an applied load. The cylinder is rotated at a certain velocity while being 
partially immersed in the fuel. Once a section of the cylinder is brought out of the 
reservoir, it is brought up to the ball/cylinder contact-point. After one thirty minutes of 
testing, the size of the scar on the ball is measured and is indicative of the fuel lubricity 
properties. The BOCLE has aided Fuel Development in identifying an JP-8+100 additive 
package that reduces wear due to excessive friction in turn lengthening the life-span for 
engine parts such as fuel pumps and fuel controls. 10 

FUEL COMBUSTION 

After half of my eight week period had been completed, I began working with the other 
section of the Fuels Branch, Fuel Combustion. Initially, this work involved entering 
hazardous material (HAZ-MAT) information into a database, but I soon obtained the chance 
to become involved in several of the tests that were being conducted. One task that I 
became involved with, "Effects of a Driven Cross Flow O^er a Constant Heat Flux Wall," 
enabled me to engage in hands-on research with engineers working on the project. Before I 

49-10 







arrived, initial tests had been done where piezoelectric actuators (also known as "flappers") 
were used to manipulate or drive the jet flow. These prelimary tests were performed 
primarily to determine if the jet flow could be controlled, thereby preparing the engineers 
for the test that I participated in where the heat transfer to the jet air from a heated wall 
would be measured. 1 ' Titanium tetrachloride (TiClJ is used in the test so that once the wet 
air from the jet comes in contact with TiCl 4 in the chimney, titanium dioxide (Ti0 2 ) forms. 
Figure 6. 13 The Ti0 2 in turn scatters the light from the laser sheet enabling the jet flow to 
be seen. Photographs are then taken of the transparent chimney, thereby allowing the 
engineers to obtain a visual illustration of effects the cross flow has on the jet air. Note 
that the actuators used in these tests are not used in an actual combustor, however they do 
provide very useful information as to how the jet air should be controlled in order to most 
effectively cool the combustor walls. Particulary with recent combustor designs, 
temperatures have continued to escalate, thermally stressing the combustor walls. By taking 
thermocouple readings and using heat sensitive paper, the project’s goal is to set the jet and 
cross flow so that the coolest temperatures could develop. ("Cool” is to be taken in relative 
terms; for this test, a cool temperature is somewhere below the melting point of the 
combustor walls.) 

Another task that I have had the chance to participate in revolved around advanced dual¬ 
dome combustor designs from General Electric and Pratt & Whitney (two competing 
aircraft engine companies). A primary concern of these combustors dealt with reducing the 
amount of ozone-depleting nitrogen oxygen (NOX) emissions. The environmental 
advantage of these designs is that the dual-dome allows for a large range of functions 
depending upon what stage of flight the aircraft is in (ie. idle, take-off, cruise, landing). In 
contrast to the inflexible combustor design, the dual-dome allows the Air Force to ready the 
combustor for a certain stage of flight, thereby preventing any unnecessary functions from 
being performed (a diagram would be included, however for propriety reasons it cannot be 
included.) This technology’s value is very significant when one considers the danger high- 
altitude aircraft pose to the ozone layer. For instance, the envisioned high-speed civil 
transport would fly at an altitude of 120,000 feet. At this level in the atmosphere, nitrogen 
oxide emissions effect ozone depletion at a rate thirty times greater than those emissions 


49-11 



released from ground level. 11 


CO N CLUSIO N 

This summer’s tour provided a variety of opportunities to become familiarized with 
numerable aspects of fuels research (more specificly, with fuel heat stability and fuel 
combustion). At the conclusion of my eight week stay, the major tests 1 had participated in 
had yet to be completed; therefore, final results are not available. Nevertheless, this 
experience has been extremely productive in the light that my understanding of the my 
potential career field has grown and that my future decisions and expectations can now be 
more knowledgable. 


49-12 


















REFERENCES 

1. Harrison, W.E. Ill, T. Edwards, S.D. Anderson, "U.S. Air Force Improved JP-8 

Development Program: An Overview," Wright-Patterson AFB, OH. 

2. Roquemore, Dr. W.M., "Annual Report for Task 2308BW," Wright-Patterson AFB, 

OH. 

3. ASTM D 4530, "Standard Test Method for Micro Carbon Residue Petroleum 

Products," 1992 Annual Book of ASTM Standards. Volume 5.03 . pp. 441-445. 

4. Grinstead, Becky, UDRI, "MCRT Results," 14 March 1993. 

5. ASTM D 3703, "Standard Test Method for Peroxide Number of Aviation Turbine 

Fuels," 1992 Annual Book of ASTM Standards. Volume 5.03 . pp. 52-54. 

6. ASTM D 3241, "Standard Test Method for Thermal Oxidation Stability of Aviation 

Turbine Fuels (JFTOT Procedure)," 1992 Annual Book of ASTM Standards. Volume 
5.02 . pp. 644-663. 

7. ASTM D 342, "Standard Test Method for Acidity in Aviation Turbine Fuel," 1992 

Annual Book of ASTM Standards. Volume 5.02 . pp. 664-667. 

8. Grinstead, Becky, UDRI, "Total Acid Number Results," 11 June 1993. 

9. Leco Corporaton. Instrumentation for: Metals. Energy, Agriculture. Geology. Mining . 

St. Joseph MI. 

10. ASTM D 5001, "Standard Test Method for Measurement of Aviation Turbine Fuels by 

the Ball-on-Cylinder Lubricity Evaluator (BOCLE)," 1992 Annual Book of ASTM 
Standards. Volume 5.03 , pp. 757-762. 

11. Sutkus, Don, "Conversation on 6 August 1993," Wright-Patterson AFB, OH. 

12. Hancock, Robert D., Glezer, Ari, and Rivir, Richard B., "Manipulation of a Jet in a 

Cross Flow Using Piezoelectric Actuators," Wright-Patterson AFB, OH. 


49-14 




DIAMOND GROWTH BY LOW-PRESSURE 
CHEMICAL VAPOR DEPOSITION 
WITH BIAS PRETREATMENT 


David J. Spry 

Summer Apprentice 
Department of the Air Force 


Final Report: 

Summer Research Program 
Propulsion Laboratories WL/POOC-3 
Wright Patterson Air Force Base 
Dayton, OH 45433-6563 


Sponsored by: 

Research and Development Laboratories 
5800 Uplander Way, Culver City, CA 90230-6608 


August 1993 


50-1 




DIAMOND GROWTH BY LOW-PRESSURE 
CHEMICAL VAPOR DEPOSITION WITH BIAS PRETREATMENT 


David J. Spry 

Summer Apprentice 
Department of the Air Force 
Wright Patterson Air Force Base 


Ab stract 

This report discusses the preliminary results of several microwave 
plasma assisted chemical vapor deposition (MPCVD) biasing experiments. 
Substrate biasing was shown to substantially increase nucleation of 
diamond on silicon. Possible reasons for this effect are also proposed. 


50-2 






DIAMOND GROWTH BY LOW-PRESSURE 
CHEMICAL VAPOR DEPOSITION WITH BIAS PRETREATMENT 

David J. Spry 

Introduction 

Diamond is relatively rare in nature, and is in a very difficult form 
to use. For this reason, the only main use for diamond has been in the 
realm of cutting tools. However, diamond has many outstanding physical 
characteristics. One example of these qualities is diamond’s transparency 
to infrared light and other wavelenghts of light that could be used in 
optical applications. If diamond VLSI was possible it would allow 
computer components to be packed closer together enabling them to be 60 
times smaller and operate four times faster’. This is because diamond is 
an outstanding thermoconductor and an electrical insulator permitting 
chips to be stacked on top of each other. There are other interesting 
properties of diamond that are discussed in other papers’. 2 , but this 
report discusses the development of a bias pretreatment to overcome the 
problem of nucleation of diamond on hetergeneous substrates. 

Although production of electronic devices using polycrystalline 
diamond films, homoepitaxial film grown on single-crystal diamond, and 


50-3 




natural IIB diamond has been accomplished3, it is necessary to produce a 
homoepitaxial film grown at an affordable cost to take full advantage of 
the electrical and thermal properties of diamond. Biasing a silicon 
substrate during the initial stages of microwave plasma CVD has been 
shown to cause increase in crystalline alignment and an increase in 
nucleation*. The understanding of the enhancement of diamond nucleation 
by biasing for approximately the first 15 minutes of depostion may be the 
key to understanding diamond nucleation itself and the possible 
development of single-crystal diamond. 

Previously, the only effective way of causing diamond nucleation on 
a silicon wafer was to first scratch it with diamond polish. This caused 
great debate over what caused the nucleation of diamond. Additionaly, 
this method would not be good for VLSI because the physical scratching 
would damage any previous processing. Biasing allows the diamond to be 
grown without modifying the silicon substrate. Also biasing may help 
give a clue to how diamond nucleates and grows. 

Bias experiments and study were investigated as follows. First, the 
bias conditions were duplicateds and observed. Next some hypothesis was 
made. Finally, tests were set up for this hypothesis and was performed. 


50-4 







Experimental Procedure 


All the diamond deposition experiments were performed with an 
Astex high pressure microwave source (H.P.M.S) as shown in figure 1. For 
the first run of biasing (BEN005), a square centimeter piece of 
silicon(IOO) was placed on a two inch diameter clean graphite disk that 
was 3.3 mm high which, in turn, sat on a 4 inch diameter graphite stage. 
The sample was run in an hydrogen atmosphere at 20 torr with a 
microwave power of 1 KW for five minutes at a flow rate of 360 seem in 



FIG. IIIPMS SOURCE 


order to remove the silicon-oxide layer. The sample was also heated to 
850°C as measured by the optical pyrometer. Then 7.2 seem (2%) methane 
was added to the system. A bias of -200 VDC which drew 25 mA current 


50-5 



was applied and maintained for 16 minutes. Longer bias time was not 
used because other researchers observed that after 15 minutes of biasing 
the nucleation decreasess. It was observed that the biasing made a 
region of faintly blue gas discharge directly over the substrate. There 
was a gap between tnis small region and the main plasma ball. When the 
bias was turned off, the plasma became one ball. Then the methane was 
changed to 1.8 seem (0.5%) to produce high quality diamond and was run 
for 18 hours. 

The next run (BEN006) was set up the same as BEN005 except the 
temperature was 775°C according to the optical pyrometer. When the bias 
was run for fifteen minutes it had the same -200 VDC, but it drew 150 mA 
current. The smaller plasma disk that was distinct from the main plasma 
ball was again observed during the time of biasing. The sample was run 
under the same growth conditions for 18 hours and 20 minutes. 

For the third run (BEN007), the hydrogen plasmawas run for ten 
minutes. Then the bias was run at the same 7.2 seem of methane and 360 
seem of hydrogen at a temperature of 845°C for fifteen minutes with a DC 
voltage of -230 drawing a current of 120 mA. The plasma conditions were 
changed to the same as before for diamond growth and was run for 26 
hours. 


50-6 



B&amila 


Using a scanning electron microscope (SEM), the samples were 
analyzed to see how the biasing affected nucleation and growth. The 
BEN005 sample had some good quality diamond crystals very 
intermittently spaced on the silicon with a particle density of 4x103/cm2 
figure 2. The BEN006 sample was very highly nucleated with a complete 
covering of the surface figure 3. It was possible to even see steps on the 
faces of tbe diamond crystals. Sample BEN007 was almost the same 


except the crystals appeared to be slightly bigger figure 4. The particle 



Figw» 4 samptt BEN 007 


50-7 





density of both BEN006 and BEN007 was between 108 and 109/cm2 which 
is slightly higher than diamond scratched silicon that has a particle 
density of 10 7 - 108 /cm 2 6. This phenomenon of the biasing pretreatment 
working only after the firs: run has been noted in other placess. This may 
be due to a film that forms after the first run on the surrounding graphite 
causing the whole graphite stage to act like a cathode. It is also 
important to note that the current changed from 25 mA to 150 mA after 
the first run. 

Conclusions 

It is evident that biasing will increase nucleation without other 
pretreatment. The orientation of diamond with bias did not occur, but that 
is most likely because a carburized layer was not formed as in Wolter’s 
experimentss. One reason for this may be an oriented silicon-carbide 
layer must be formed before an oriented layer of diamond could be formed 
because the lattice parameter of silicon-carbide is closer to diamond than 
is silicon. Cleaning the silicon with hydrofluoric acid will clean off the 
silicon-oxide layer. Also biasing a piece of clean single crystal silicon- 
carbide should improve orientation. A run where cleaning the silicon with 
hydrofluoric acid and then carburizing it before biasing and deposition 


50-8 





was done, but was not yet examined. 

The biasing process is likely causing a larger flux of positive ions 
such as CH3+, CH2+, CH+, H+, and H3+ to the surface of the silicon while not 
effecting the non ionized radicals like CH3, CH2, CH, H, and H3 and the 
stable gasses like CH4 and H2. The increase of CH3+ at the surface is 
possibly responsible for the increase in nucleation thus may be 
responsible for nucleation itsself. CH3+ is possibly responsible for 
nucleation because with its trigonal planer shape it is closer to the same 
spacing of silicon-carbide than CH3 which is trigonal pyramidal. However, 
CH 3 along with CH 4 may be closer to diamond’s spacing and may be 
responsible for diamond growth. A study of the gasses above the 
substrate during biasing would be helpful, but distinguishing between 
ionic radicals and neutral radicals is difficult. 

A c Im Q yyJ e ii^ m en t § 

The author of this report would like to thank Bob Knight, Pat 
Emmert, Sean McGinnis, the workers at Wright Patterson AFB, and RDL 
HSAP Program for supporting this project. 


50-9 


Relgie n&e.s 

iVic Comello, R&D Magazine. 48, (1992). 

2 Aerospace America. 27, (Feb. 1991). 

3Hideo Kiyota, Ken Okano, Tatsuya Iwasaki, Hiroshi Izumiya, Yukio Akiba, 
Tateki Kurosu, and Masamori lida, Japanese J. of Appl. Phys. 30, L2015 
(1991). 

4J. J. Chang and T. D. Mantei, J. Appl. Phys. 11, 72 (1992). 

5S. D. Wolter, B. R. Stoner, and J. T. Glass, Appl. Phys. Lett. 11, 62 (1993). 
sYarborough, W., and Messier, R., Sience. 247, 688, (1990). 


50-10 



