/7/bs a^-y N94 

J 7 ^ How To Handle 6GBvtes a Night and Not Get Swamped 


22 44 4 


R. Allsman, C. Alcock, T. Axelrod, D. Bennett, K. Cook, H.-S. Park (LLNL), K. Griest, 

S. Marshall, S. Perlmutter, C. Stubbs, W. Sutherland (Center for Particle 
Astrophysics, UCB), K. Freeman, B. Peterson, P. Quinn, A. Rodgers (Ml. Stromlo 
and Siding Spring Observatories, ANU) 


The Macho Project has undertaken a 5 year effort to search for dark matter in 
the halo of the Galaxy by scanning the Magellanic Clouds for micro-lensing 
events. Each evening's raw image data will be reduced in real-time into the 

observed stars' photometric measurements. The actual search for micro-lensing 
events will be a post-processing operation. 

The theoretical prediction of the rate of such events necessitates the collection 
of a large number of repeated exposures. The Project designed camera 
subsystem delivers 64 Mbytes per exposure with exposures typically occurring 
every 500 seconds. An ideal evening's observing will provide 6 Gbytes of raw 
image data and 40 Mbytes of reduced photometric measurements. Recognizing 
the difficulty of digging out from a snowballing cascade of raw data, the Project 
requires the real-time reduction of each evening's data. The software team's 
implementation strategy centered on this non-negotiable mandate. 

Accepting the reality that 2 full time people needed to implement the core real- 
time control and data management system within 6 months, off-the-shelf 
vendor components were explored to provide quick solutions to the classic needs 
for file management, data management, and process control. Where vendor 
solutions were lacking, state-of-the-art models were used for hand tailored 
subsystems. In particular, petri nets manage process control, memory mapped 
bulletin boards provide interprocess communication between the multi-tasked 
processes, and C++ class libraries provide memory mapped, disk resident 
databases. 

The differences between the implementation strategy and the final 
implementation reality will be presented. The necessity of validating vendor 
product claims will be explored. Both the successful and hindsight decisions 
enabling the collection and processing of the nightly data barrage will be 
reviewed. 


13 


